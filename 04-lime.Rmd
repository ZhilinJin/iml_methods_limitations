---
output:
  pdf_document: default
  html_document: default
---
# Local Interpretable Model-agnostic Explanations

(# First merge start)

So far all of the interpretation methods presented in this book refer to the global scope of the trained model.
However, none really gives the opportunity to interpret the single predictions of a model on their own.
This is where the method LIME (local interpretable model-agnostic explanations) comes into play.
LIME is a variant of the so-called surrogate models.
This type of models aims to imitate the black box prediction's behaviour of a machine learning model with a specific constraint: they are interpretable.
The intuition is to have an interpretable model that explains the association of the features $X$ and the - by the black box model - \emph{predicted} target $\hat{y}$.
Thus, surrogate models aim to capture the decision boundaries of the original model without attempting to fit the true association itself.
For example, we may use a neural network to solve a classification task, but, while a neural network is anything besides interpretable, we may find that some of the decision boundaries are explained reasonably well by a logistic regression which in fact yields interpretable coefficients.

In general there are two kinds of surrogate models: global and local surrogate models.
In this chapter we will focus on the latter.

The concept of local surrogate models is heavily tied to \cite{LIME}, who propose local interpretable model-agnostic explanations (LIME). 
Different to global surrogate models, local ones aim to rather explain single predictions by interpretable models than the whole black box model.
By using LIME one tries to \emph{locally interpret} this prediction solely by a new surrogate model - a \emph{model-agnostic explanation} so to say. 
This model, also referred to as explainer, needs to be easiliy interpretable (like a linear regression model or a decision tree) and thus may of course not have the adaptability and flexibility of the original black box model.
However, we actually don't care about a global fit in this case.
We only want to have a very local fit of the explainer in the neighbourhood of the instance to be explained. 

So far so good. 
However, the previous outline was not very specific and leaves (at least) three questions.
First, what should the input of the explainer be like?
Second, what does neighbourhood refer to?
Third, what properties should suitable explainers have?

To better assess these open questions it may be helpful to study the mathematical definition of $LIME$.

The explainer function for a datapoint $x$, which we aim to interpret, is of the following form:

\begin{equation}
  explainer\left(x\right) = arg\,min_{g \epsilon G} \,\mathcal{L}\left(f, g, \pi_x \right) + \Omega\left(g\right)
\end{equation}

Let's decompose this compact, yet precise definition:

$x$ can be any arbitrary (new) data point that can be represented in the same way as the original data.
Furthermore, different types of data are possible to be used as an original set. 
Besides the classical tabular data case, we can also interpret models trained on text data or image data. 
However, some data representations (e.g. word embeddings) are not human-interpretable and must be replaced by interpretable variants (e.g. one-hot-encoded word vectors) for LIME to yield interpretable results.
Additionally, it is important to understand that the explainer only aims to explain the black box model behaviour and not the data this model was trained on.
The function fitted by this model operates in the complete feature space and can even yield predictions for instances not seen in the training data.
Hence, we want to create a more complete \emph{grid} of the data and fill the feature space with new observations, so that we can better study the black box model's behaviour.
Still, the data for the explainer should be related to the original data in a way for the explainer to not be ill-located in space and to not lose everything in common with the original problem.
This is why LIME decides to perturb the original data.
The perturbed data set, which is used to train the explainer, is much larger than the original one and supposed to be the better representative of the (possible) feature space.
But perturbation does not work equally well for all kind of data.
For example, in the case of tabular data categorical features are much better dealt with as numerical features.
This is because naturally for categorical data the full grid of possible combinations can be exhausted just by recombining all levels.
LIME's perturbation of numerical data also captures the possible feature space better. 
However, typically numerical data is perturbed by a (gaussian) error or binning the numerical features in categorical/ordinal ones.
One can easily see that this way the full grid is not likely to be explored as fully as for categorical features.
In fact, the way numerical features are handled is one of the main limitations of LIME, which however will not be discussed in detail in this book.
In general one could say a big difficulty for LIME is the perturbation of the data itself.
This topic will be discussed in detail in chapter X and thus not outlined here.

Let's come back to our formula from the beginning.
The range of the function are all possible elements $g$ in our predefined set of interpretable functions $G$. 
$G$ should always be consciously chosen depending on the outcome one desires with heavy emphasis on straight forward interpretability.
All possible decision trees of a certain depth may be a suitable choice for nominal features but unfavourable for numerical ones.

The function returns the model $g$ minimizing two trade-off terms: 
The first term $\mathcal{L}\left(f, g, \pi_x \right)$ is similar to a loss function responsible to deliver the best fit for the model $f$ plus giving a rough idea for the fidelity of the interpretable model $g$ with respect to a proximity measure $\pi_x(z)$.
This measure makes sure that optimal local fit around $x$ or its neighbourhood is preferred over a global fit.
Neighbourhood is a very vague term.
This is for good reason.
A priori it is not clear how to specify the neighbourhood properly.
However, since we want to find a local model, it is important to define a neighbourhood which determines what we understand by local.
Technically, there are many different options to deal with this issue.
Weighting the observations w.r.t. their proximity to the observation that is explained seems like a good idea.
A possible implementation of this weighting is the application of a kernel function.
The kernel can be arbitraryly parametrised.
This leaves in total many scientific degrees of freedom which makes the neighbourhood definition somewhat problematic.
This neighbourhood issue will be discussed in more detail in the next chapter.

Furthermore, the fidelity term  gives a good idea how well our interpretable model fits the black box model in the local neighborhood around our target defined by the proximity measure.
The smaller the fidelity the more we can trust our resulting model to give a good representation. 
On the other hand, if the fidelity is high, we should rather rethink our approach as the result will most likely not be representative for the predictive model - not even locally.

Having discussed the first two open questions, what data the explainer needs and what neighbourhood refers to, we can focus on the third question:
What properties should suitable explainers have?
We mentioned the interpretability property and outlined generalised linear models or decision trees as examples, however, we did not discuss further desired properties of these models.
As these have certain strong assumptions, it is unlikely that they are capable of maintaining optimal fit to the original black box model. 
Recall our formula.
We need a second term working against the first one which constraints the optimal fit to more interpretable models: 
$\Omega\left(g\right)$ is our complexity measure and responsible to choose the model with the lowest complexity - as an example for decision trees tree depth or in the case of linear regression models $L_0$ norm give hard restrictions of how easy interpretation has to be. 
Putting the fidelity and complexity term together and minimizing them gives us a trade-off solution between best local fit and best interpretability. 

The defintion of LIME seems after all very rough and vague.
This leaves us many degrees of freedom when implementing it - for the good and the bad.
For example, we see the model $f$ gives no restrictions on the other choices in the equation, which gives us the opportunity to drastically change the underlying predictive model while keeping the same interpretable model with the same complexity constraints. 
Maybe we already set up a LIME interpretation with a decision tree for a random forest model, but we suddenly get the idea a deep neural network could impress our clients more.
Now, we would only have to rerun the same LIME framework with the new model and recieve a similar interpretable decision tree as before.
Besides that, the complexity term gives us a great opportunity to restrict our final result to a certain amount of parameters. 
If we use $\Omega(g) = \infty \, 1_{||w_g||_0<K}$ we can set exactly the amount of interpretable parameters our final model should have to $K$. 
This means the result will focus on the $K$ most important parameters and fully ignore less significant ones that would give to much clutter to our interpretation.


To give things more substance we end up with the following possible algorithm for LIME:

1. Get instance $x$ out of data space $X$ for which we desire an explanation for its predicted target value.

2. Perturb your dataset $X$ and recieve a perturbed data set $Z$ of increased size. 

3. Retrieve the black box model predictions for $Z$.

4. Weight $Z$ w.r.t. the proximity/neighbourhood to $x$.

5. Train an explainable weighted model $g$ on $Z$ and the associated predictions.

Return: An explanation for the interpretable model $g$.

The following visualisation nicely depicts the described algorithm. 
We start only with our data split into two classes: 1 and 0.
Then, we fit a model that can perfectly distinguish between the two classes - indicated by the sinus shaped function.
We do not perturb the data in this case.
(However, we may argue that our pertubation function is the identity function.)
Now, the data point, which we want an explanation for, is coloured yellow.
With respect to this point we weight our data by giving close observations higher weights.
Afterwards, we fit a classifaction model based on these weights.
This yields an interpretable linear decision boundary. 
As we can see, this is indeed locally very similar to the black box decision boundary and seems to be a reasonable result.

```{r, echo = FALSE}
output <- knitr::opts_knit$get("rmarkdown.pandoc.to")
is.html = !is.null(output) && output == "html"
```

```{r, eval = is.html, echo = FALSE}
knitr::include_graphics("images/lime.gif")
```

```{r, eval = !is.html, echo = FALSE}
knitr::include_graphics("images/lime.png")
```

For specific examples of LIME please refer to the next two chapters or \cite{Molnar}.

Certainly LIME has great potential, however, as a very young (as of 2019) method, the actual implementations in software packages have short-comings.
With LIME being a very generic approach also comes the consequence that many hyperparameters, like the neighbourhood definition or the sampling strategy, are arbitrarily chosen with no opportunity to the user to set them manually.
Hence, it is likely that in many use cases LIME explanations can hardly be trusted and should be treated with great care.

The following two chapters will focus on two very significant "hyperparameters": the neighbourhood definition and the sampling strategy, and investigate how these affect the results and their interpretability.
For this, we will emphasise the coefficient stability of LIME explainers in order to illustrate the trustworthiness of the results. 

(# First merge end) 