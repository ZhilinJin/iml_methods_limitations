# Local Interpretable Model-Agnostic Explanations

When doing machine learning we always build models.
Models are a simplifications of the reality.
Even if the predictive power of a model may be very strong, it will still only be a model.
However, models with high predictive capacity are most of the time not interpretable as seen thoughout this book.
In this case the model seems as complex as the problem itself to a human.
The scientific approach suggests to simplify reality by a __model__ - a model explaining another more complex model.
These explaining models are refered to as surrogate models.
They imitate the black box prediction behaviour of a machine learning model subject to a specific and important constraint: 
surrogate models are interpretable.
More technically, a simple (interpretable) model describes the association of the features $X$ and the - by the black box model - _predicted_ target $\hat{y}$.
For example, we may use a neural network to solve a classification task.
While a neural network is anything but interpretable, we may find that some of the decision boundaries are explained reasonably well by a logistic regression which in fact yields interpretable coefficients.

In general there are two kinds of surrogate models: global and local surrogate models.
In this chapter we will focus on the latter ones.

The concept of local surrogate models is heavily tied to @ribeiro2016should, who propose local interpretable model-agnostic explanations (LIME). 
Different to global surrogate models, local ones aim to rather explain single predictions by interpretable models than the whole black box model.
By using LIME one tries to _locally interpret_ this prediction solely by a new surrogate model - a _model-agnostic explanation_ so to say. 
This model, also referred to as explainer, needs to be easiliy interpretable (like a linear regression model or a decision tree) and thus may of course not have the adaptability and flexibility of the original black box model.
However, we actually don't care about a global fit in this case.
We only want to have a very local fit of the explainer in the neighbourhood of the instance to be explained. 
A single explanation can only help to understand and validate this one prediction.
However, the model itself needs to be examined and validated by multiple (representative) LIME explanations.

A LIME explanation can be retrieved by the following algorithm:

1. Get instance $x$ out of data space $X$ for which we desire an explanation for its predicted target value.

2. _Perturb_ your dataset $X$ and recieve a perturbed data set $Z$ of increased size. 

3. Retrieve the black box model predictions for $Z$ using the black box model $f$.

4. Weight $Z$ w.r.t. the proximity/neighbourhood to $x$.

5. Train an explainable weighted model $g$ on $Z$ and the associated predictions.

Return: An explanation for the interpretable model $g$.

The following visualisation nicely depicts the described algorithm for a two-dimensional classification problem. 
We start only with our data split into two classes: 1 and 0.
Then, we fit a model that can perfectly distinguish between the two classes - indicated by the sinus shaped function drawn as a black curve.
We do not perturb the data in this case.
(However, we may argue that our pertubation function is the identity function.
We will more formally discuss pertubation later on.)
Now, we choose the data point, which we want an explanation for.
It is coloured in yellow.
With respect to this point we weight our data by giving close observations higher weights.
We indicate this by the size of data points.
Afterwards, we fit a classifaction model based on these weighted instances.
This yields an interpretable linear decision boundary - depicted by the purple line. 
As we can see, this is indeed locally very similar to the black box decision boundary and seems to be a reasonable result.

```{r, echo = FALSE}
output <- knitr::opts_knit$get("rmarkdown.pandoc.to")
is.html = !is.null(output) && output == "html"
```

```{r, eval = is.html, echo = FALSE, fig.align = 'center', fig.cap = "Simplified GIF representation of the LIME algorithm. The GIF is in line with the description presented here."}
knitr::include_graphics("images/lime.gif")
```

```{r, eval = !is.html, echo = FALSE, fig.align = 'center', out.width = '99%', fig.cap = "Simplified graphical representaion of the LIME algorithm. Each single panel represents one step of the described algorithm. It reads from left to right."}
knitr::include_graphics("images/lime.png")
```


So far so good. 
However, the previous outline was not very specific and leaves (at least) three questions.
First, what does neighbourhood refer to?
Second, what properties should suitable explainers have?
Third, what data do we use why and how do we perturb?

To better assess these open questions it may be helpful to study the mathematical definition of $LIME$.
The explanation for a datapoint $x$, which we aim to interpret, can be represented by the following formula:

$$explanation\left(x\right) = arg\,min_{g \epsilon G} \,\mathcal{L}\left(f, g, \pi_x \right) + \Omega\left(g\right)$$

Let's decompose this compact, yet precise definition:

$x$ is the instance which we aim to receive an explanation for.
It can be an instance which is entirely new to us as long as it can be represented in the same way as the training data of the black box model.
The explanation results from the maximisation of the loss-like fidelity term $\mathcal{L}\left(f, g, \pi_x \right)$ and a complexity term $\Omega\left(g\right)$.
$f$ refers to the black box model we want to explain and $g$ to the explainer. 
$G$ represents the complete set of prespecified explainers.
$G$ should always be consciously chosen depending on the outcome one desires with heavy emphasis on straight forward interpretability.
All possible decision trees of a certain depth may be a suitable choice for nominal features but unfavourable for numerical ones.
The explanation has to deal with two trade-off terms when minimising: 
The first term $\mathcal{L}\left(f, g, \pi_x \right)$ is responsible to deliver the optimal fit of $g$ to the model $f$ while a low _loss_ is desirable indicating high (local) fidelity.
The optimal fit is only found with respect to a proximity measure $\pi_x(z)$.
This measure makes sure that optimal local fit around $x$ or its neighbourhood is preferred over a global fit.

This leads us to the first open question:
What does neighbourhood refer to?
Neighbourhood is a very vague term.
This is for good reason, though.
A priori it is not clear how to specify a neighbourhood properly.
On the other hand, since we want to find a local model, it is important to define a neighbourhood which determines what we understand by local.
Technically, there are many different options to deal with this issue.
Weighting the observations w.r.t. their proximity to the observation that is explained seems like a good idea.
A possible implementation of this weighting is the application of a kernel function.
The kernel can be arbitrarily parametrised.
This leaves in total many scientific degrees of freedom which makes the neighbourhood definition somewhat problematic.
This neighbourhood issue will be discussed in more detail in the next chapter.

We answered the second open question - what properties suitable explainers should have - in parts already.
We mentioned the interpretability property and outlined generalised linear models or decision trees as examples, however, we did not discuss further desired properties of these models.
As these have certain strong assumptions, it is unlikely that they are capable of maintaining optimal fit to the original black box model. 
Recall our formula.
As we want local optimal fit subjected to a certain (low) degree of model complexity, which allows interpretation, our formula needs to facilitate this aspect.
Recondisering our formula, $\Omega\left(g\right)$ is our complexity measure and responsible to choose the model with the lowest complexity - as an example for decision trees tree depth or in the case of linear regression models $L_0$ norm give hard restrictions of how easy interpretation has to be. 
If we use $\Omega(g) = \infty \, 1_{||w_g||_0<K}$ we can set exactly the amount of interpretable parameters that our final model should have to $K$.
This means that the result will focus on the $K$ most important parameters and fully ignore less significant ones that would give too much clutter to our interpretation.

Having answered the first two open question we still have the last question related to the data and pertubation open.
Besides the tabular data case, we can also interpret models trained on text data or image data. 
However, some data representations (e.g. word embeddings) are not human-interpretable and must be replaced by interpretable variants (e.g. one-hot-encoded word vectors) for LIME to yield interpretable results.
Additionally, it is important to understand that the explainer only aims to explain the black box model behaviour and not the data this model was trained on itself.
The function, that is modeled, operates in the complete feature space and can even yield predictions for instances not seen in the training data.
Hence, we want to create a more complete _grid_ of the data and fill the feature space with new observations, so that we can better study the behaviour of the black box model.
Still, the data for the explainer should be related to the original data.
This is because we don't want the explainer to be ill-located in space having nothing in common with the original problem anymore.
This is why we perturb the original data.
The perturbed data set, which is used to train the explainer, is much larger than the original one and supposed to better represent the (possible) feature space.
But perturbation does not work equally well for all kinds of data.
Details on problems associated with pertubation will be studied in chapter X.

For specific examples of LIME please refer to the next two chapters or @molnar2019.

The defintion of LIME still seems after all very rough and vague.
This leaves us many scientific degrees of freedom when implementing it - for the good and for the bad.
For example, we see that the model $f$ can be any machine learning model that exists.
This gives us the opportunity to drastically change the underlying predictive model while keeping the same interpretable model with the same complexity constraints. 
Assume that we already set up a LIME interpretation with a decision tree for a random forest model, but then we replace the random forest by a deep neural network could.
Now, we would only have to rerun the same LIME framework with the new model and receive a similar interpretable decision tree as before.
Besides that, the complexity term gives us a great opportunity to restrict our final result to a certain amount of parameters. 

On the other hand, LIME being a very generic approach also means that many hyperparameters, like the neighbourhood definition or the sampling strategy, are arbitrary.
In software implementations (R and Phyton) the user has no opportunity to set them manually.
Hence, it is likely that in many use cases LIME explanations do not fit the given context.
In these cases they can hardly be trusted and should be treated with great care.

The following two chapters will focus on two very significant "hyperparameters": 
the neighbourhood definition and the sampling strategy.
They will investigate how these affect the results of the method and their interpretability.
We will emphasise the coefficient stability of LIME explainers in order to illustrate the trustworthiness of the results. 
