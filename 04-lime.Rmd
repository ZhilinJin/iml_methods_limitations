# Local Interpretable Model-agnostic Explanations

(# Sebastian start)

So far all the interpretation methods are at the global scope of the trained model with respect to its underlying dataset, but none really gives the opportunity to interpret a single prediction on its own in a quick and easy way. This is where the method LIME (Local Interpretable Model-agnostic Explanations) comes into play. While usually the full scope of a feature is tried to be explained, LIME doesn't even try to give an overview of how a feature acts in the whole model. Instead, we only have a single observation with its given prediction and try to "interpret locally" this prediction alone through a second, surrogate model - a "model-agnostic explanation" so to say. This model has to be easiliy interpretable for this to make sense (like a linear regression model or a decision tree) and thus may of course not have the adaptability and flexibility of the original model, but we actually don't care about a global fit here - we only want to have a very local fit of the new model to the old model because we only care about a single data point - making the idea of the procedure plausible for a reasonable outcome. Later on we will discover why a reasonable outcome really shouldn't be taken granted, but for now we are going to dive a bit deeper into how and why LIME works.

The mathematical definition of the $LIME$ function for a datapoint $x$ whose interpretation we desire is of the following form:

\begin{equation}
  LIME\left(x\right) = arg\,min_{g \epsilon G} \,\mathcal{L}\left(f, g, \pi_x \right) + \Omega\left(g\right)
\end{equation}

Let's dissect this compact, yet precise definition:
First of all any $x$ as input is allowed here as long as it is conform with the original data structure. But as a side note one should mention, depending on the implementation, data points far away of the original distribution may lead to unreliable results. The range of the function are all possible elements $g$ in our predefined set of interpretable functions $G$. $G$ should always be consciously chosen depending on the outcome one desires with heavy emphasis on straight forward interpretability - all possible decision trees of a certain depth may be a great choice for nominal features but unfavorable for numerical ones.
Furthermore the functions returns the model $g$ minimizing two trade-off terms: The first term $\mathcal{L}\left(f, g, \pi_x \right)$ is similar to a loss function responsible to deliver the best fit for the model $f$ plus giving a rough idea for the fidelity of the interpretable model $g$ with respect to a proximity measure $\pi_x(z)$ - this measure does nothing more than ensuring a local best fit around $x$ on datapoints $z$ instead of a global one. Of course returning the model with the best fit to the original model undermines our idea of an easy interpretation thus we need a second term working against the first one: $\Omega\left(g\right)$ is our complexity measure and responsible to chose the model with the lowest complexity - for decision trees tree depth or in the case of linear regression models $L_0$ Norm give hard restrictions of how easy interpretation has to be.
Putting these two terms together and minimizing them gives us a trade-off between best local fit and best interpretability of our solution. Of course this may sound great in theory, but for a practical application a lot of non-trivial choices for each component has to be made to deliver desirable results. Such choices may concern the set $G$, the hyperparameters of the complexity function or the calcuation of the proximity measure $\pi_x(z)$ - not only $\pi_x(z)$ but also different datapoints $z$ in the domain of the model have to be chosen (in general by sampling and/or perturbating the original data) for a local best fit.
Although this rough definition of LIME certainly makes it a challenge to implement for the first time, but it also gives us great freedom and flexibility of how we tackle our problem. For example we see the model $f$ gives no restrictions on the other choices in the equation (as long as the model range stays the same), which gives us the opportunity to drastically change the underlying predictive model while keeping the same interpretable model with the same complexity constraints. Maybe you already put up a LIME interpretation with a decision tree for a random forest model, but suddenly got the idea a deep neural network could impress your clients more? Just rerun the same LIME framework with the new model and recieve a similar interpretable decision tree as before.
Besides that, the complexity term gives us a great way to restrict our final result to a certain amount of parameters. If we use $\Omega(g) = \infty \, 1_{||w_g||_0<K}$ we can set exactly the amount of interpretable parameters our final model should have to $K$. This means we can focus on the $K$ most important parameters and fully ignore smaller ones that would give to much clutter to our interpretation. Additionally, the fidelity term gives a good idea how well our interpretable model fits the preditive model in the local neighborhood around our target defined by the proximity measure. The smaller the fidelity the more we can trust our resulting model to give a good representation. If the fidelity is high on the other hand, we should rather rethink our approach as the result will most likely not be representative for the predictive model - not even locally.
This flexibility isn't only constrained to the predictive model, but also for the data space. Besides the classical tabular data case, we can also interprete models trained on text data or image data. On the flipside if one wants to use LIME for such differing cases it is important to set the data perturbations/sampling and the proximity measure for training the interpretable model with great care since it highly influences the picked model. For example in the case of image data, perturbations of single pixels won't lead to great results due to their high correlation with each other. Especially the perturbation and the proximity measure are open questions in the field of research with rather semi-random choices in current implementations. The result may vary hugely with the size of the neighboorhood for predictive models with a jagged prediction surface - this may not be a big blow at first sight, but implementations may not allow to adjust the size of the neighboorhood which does make this to a big blow. Furthermore in this case, the sampled data points $z$ can also have a big influence on the outcome, which gives concern that resampling may change the values unacceptably large. As this wasn't already enough the sampling of data points currently destroys the correlations in most implementations, which means for some scenarios highly unlikely samples are substantially more likely to be drawn, influencing the interpretable model in an unrealistic way.

It is certain to say LIME has great potential, but the problems of current implementations may suggest to act with great care and to not use it for critical and time-restricting tasks as results can not be trusted for fixed neighboorhood and sampling strategy. Unfortunately it is often not possible to adjust these, which simply forces the user to time consumingly create his own implementation - unfeasable for most people.

(# Sebastian end)