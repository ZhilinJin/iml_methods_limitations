---
output:
  pdf_document: default
  html_document: default
---
# Local Interpretable Model-agnostic Explanations

(# First merge start)

So far all of the interpretation methods presented in this book are refer to the global scope of the trained model.
However, none really gives the opportunity to interpret a model via single predictions on their own.
This is where the method LIME (local interpretable model-agnostic explanations) comes into play.
LIME is a variant of so-called surrogate models.
Surrogate models aim to imitate the black box prediction behaviour of a machine learning model. 
Surrogate models themselves have a certain feature: they are interpretable.
The intuition is to have an interpretable model that explains the association of the features $X$ and the - by the black box model - \emph{predicted} target $\hat{y}$.
Thus, surrogate models aim to capture the decision boundaries of the black box model without attempting to model the true association itself.
For example, we may use a neural network to solve a classification task.
While a neural network is anything but interpretable, we may find that some of the decision boundaries are reasonably well explained by a logistic regression which in fact yields interpretable coefficients.

In general there are two kinds of surrogate models: global and local surrogate models.
In this chapter we will focus on the latter ones.

The concept of local surrogate models is heavily tied to \cite{LIME} who propose local interpretable model-agnostic explanations (LIME). 
Different to global surrogate models, local surrogate models such as LIME aim to rather explain single predictions by interpretable models than the whole black box model.
Using LIME one tries to \emph{locally interpret} this prediction solely by a new surrogate model - a \emph{model-agnostic explanation} so to say. 
This model, also referred to as explainer, needs to be easiliy interpretable (like a linear regression model or a decision tree) and thus may of course not have the adaptability and flexibility of the original black box model.
However, we actually don't care about a global fit in this case.
We only want to have a very local fit of the explainer in the neighbourhood of the instance to be explained. 

So far so good. 
However, the previous outline was not very specific and leaves (at least) three questions.
First, what should the input of the explainer be like?
Second, what does neighbourhood refer to?
Third, what properties should suitable explainers have?

To better assess these open questions it may be helpful to study the mathematical defintion of $LIME$.

The explainer function for a datapoint $x$ which we aim to interpret is of the following form:

\begin{equation}
  explainer\left(x\right) = arg\,min_{g \epsilon G} \,\mathcal{L}\left(f, g, \pi_x \right) + \Omega\left(g\right)
\end{equation}

Let's decompose this compact, yet precise definition:
$x$ can be any arbitrary (new) data that can be represented in the same way as the original data.
Besides the classical tabular data case, we can also interpret models trained on text data or image data. 
However, some data representations (e.g. word embeddings for text data) are not human-interpretable and must be replaced by interpretable variants (e.g. one-hot-encoded word vectors) for LIME to yield interpretable results.
Additionally, it is important to understand that the explainer only aims to explain the black box model behaviour and not the data this model was fit on.
The function modelled by the black box model operates in the complete feature space and can even yield predictions for instances not seen in the training data.
Hence, we want to create more complete \emph{grid} of the data and fill the feature space with new observations, so that we can better study the black box model behaviour.
Still, the data for the explainer should be related to the original data so that the data for the explainer is not ill-located in space and does not anything in common with the original problem anymore.
This is why LIME perturbs the original data.
The perturbed data set, which is used to train the explainer, is much larger than the original one and supposed the better represent the (possible) feature space.
Perturbation does not work equally well for all kind of data.
In the case of tabular data categorical features are much better dealt with as numerical features.
This is because naturally for categorical data the full grid of possible combinations can be exhausted just by recombining all levels.
LIME's perturbation of numerical data also better captures the possible feature space. 
However, typically numerical data is perturbed by a (gaussian) error or binning the numerical features in categorical/ordinal ones.
One can easily see that this way the full grid is not likely to be explored as fully as for categorical features.
In fact, the way numerical features are handled in LIME is one of the main limitations of LIME which however will not be discussed in detail in this book.
As you may have observed in this section another difficulty of LIME is the perturbation of the data itself.
This topic will be discussed in detail in chapter X and thus not  outlined within this chapter.

Let's come back to our formula from the beginning.
The function returns the model $g$ minimizing two trade-off terms: 
The first term $\mathcal{L}\left(f, g, \pi_x \right)$ is similar to a loss function responsible to deliver the best fit for the model $f$ plus giving a rough idea for the fidelity of the interpretable model $g$ with respect to a proximity measure $\pi_x(z)$.
This measure makes sure that optimal local fit around $x$ or its neighbourhood is preferred over a global fit.
Neighbourhood is a very vague term.
This is for good reason.
A priori it is not clear how to specify the neighbourhood properly.
However, as we want to find a local model, it is important to define a neighbourhood which determines what we understand by local.
Technically, there are many different options to deal with this issue.
Weighting the observations w.r.t. their proximity to the observation that is explained seems like a good idea.
A possible implementation of this weighting is the application of a kernel function.
The kernel can be arbitraryly parametrised.
This leaves in total many scientific degrees of freedom which makes the neighbourhood definition somewhat problematic.
This neighbourhood issue will be discussed in more detail in the next chapter.

Having discussed the first two open questions, what data the explainer needs and what neighbourhood refers to, we can focus on the third question:
What properties should suitable explainers have?
We mentioned the interpretability property and outlined generalised linear models or decision trees as examples.
However, we did not discuss further desired properties of these models.
As these models have certain strong assumptions, it is unlikely that these models are capable of maintaining optimal fit to the original black box model. 
Recall our formula.
We need a second term working against the first one which constraints the optimal fit to more interpretable models: 
$\Omega\left(g\right)$ is our complexity measure and responsible to choose the model with the lowest complexity - for decision trees tree depth or in the case of linear regression models $L_0$ norm give hard restrictions of how easy interpretation has to be. 
Putting these two terms together and minimizing them gives us a trade-off solution between best local fit and best interpretability. 
The range of the function are all possible elements $g$ in our predefined set of interpretable functions $G$. 
$G$ should always be consciously chosen depending on the outcome one desires with heavy emphasis on straight forward interpretability.
All possible decision trees of a certain depth may be a suitable choice for nominal features but unfavourable for numerical ones.

The defintion of LIME seems after all very rough and vague.
This leaves us many scientific degrees of freedom when implementing it - for the good and the bad.
For example, we see the model $f$ gives no restrictions on the other choices in the equation, which gives us the opportunity to drastically change the underlying predictive model while keeping the same interpretable model with the same complexity constraints. 
Maybe you already set up a LIME interpretation with a decision tree for a random forest model.
Then, you suddenly got the idea that a deep neural network could impress your clients more.
Now, you would only have to rerun the same LIME framework with the new model and recieve a similar interpretable decision tree as before.
Besides that, the complexity term gives us a great opportunity to restrict our final result to a certain amount of parameters. 
If we use $\Omega(g) = \infty \, 1_{||w_g||_0<K}$ we can set exactly the amount of interpretable parameters our final model should have to $K$. 
This means we can focus on the $K$ most important parameters and fully ignore less significant ones that would give to much clutter to our interpretation.

[//]: # <> Meiner Meinung nach: der Absatz ist verwirrend. Die Idee ist super aber es kommt nicht so ganz r√ºber. 

Additionally, the fidelity term gives a good idea how well our interpretable model fits the black box model in the local neighborhood around our target defined by the proximity measure. 
The smaller the fidelity the more we can trust our resulting model to give a good representation. 
If the fidelity is high on the other hand, we should rather rethink our approach as the result will most likely not be representative for the predictive model - not even locally.

Summing it up we end up with the following algorithm for LIME:

1. Select instance $x$ out of the original data $X$ for which we want an explanation for its prediction.

2. Perturb your dataset $X$ and achieve a perturbed data set $Z$. 

3. Retrieve the black box model predictions for $Z$.

4. Weight $Z$ w.r.t. the proximity/neighbourhood to $x$.

5. Train an explainable weighted model $g$ on $Z$ and the associated predictions.

Return: An explanation for the interpretable model $g$.

The following GIF nicely visualises the described algorithm. 
We start with only our data that is split into two classes: 1 and 0.
Then, we fit a model that can perfectly distinguish between the two classes - indicating by the sinus shaped function.
We do not perturb the data (however, we may argue that our pertubation function is the identity function).
Now we select an observation (yellow point) for which we want an explanation.
With respect to this observation we weight our data.
Close observations receive higher weights.
Then, we fit a weighted classifaction model which yields an interpretable linear decision boundary which is actually locally very similar to the black box decision boundary.

```{r echo = FALSE}
knitr::include_graphics("images/LIME.gif")
```

For specific examples of LIME please refer to the next two chapters or \cite{Molnar}.

Certainly LIME has great potential.
However, as LIME is a very young (as of 2019) approach the actual implementation of LIME in software packages (e.g. R or Python) have short-comings.
LIME being a very generic approach also means that many hyperparameters, like the neighbourhood definition or the sampling strategy, are arbitriraly chosen with no opportunity to the user to change them manually.
Hence, it is likely that in some or many use cases LIME explanations can hardly be trusted.

The following two chapters will focus on two very significant "hyperparameters": the neighbourhood definition and the sampling strategy and investigate how these affect the results and interpretability of these.
We will emphasise the coefficient stability of LIME explainers in order to illustrate the trustworthiness of the results.

(# First merge end)