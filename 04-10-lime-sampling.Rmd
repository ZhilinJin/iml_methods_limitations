# LIME and Sampling

*Author: Sebastian Gruber*


## Introduction

The idea behind LIME reduced to its most basic scheme is basically fitting a simpler model on a more complex model with weights definining an area of higher relevance for the loss function.
Let's remember how an arbitrary machine learning model is fitted:
We search the function (= model) $f^*$ out of our hypothesis space $\mathcal{H}$ offering the best minimization of a loss function $L$ plus a regularization term $J(f)$ for counteracting overfitting as can be seen by

$$ f^* = \arg\min_{f \in \mathcal{H}} \sum_{i=1}^{n} L\Big(y^{(i)}, f\big(x^{(i)}\big) \Big) + \lambda \times J(f) $$

A more specific description of this is given in [C. Bishop, Pattern Recognition and Machine Learning] and is out of scope in this book.
The interesting part is now, how this is related to the LIME equation.
Let's assume we just fitted successfully a very complex (and highly impressive) model on a not less intimidating dataset.
This means we identified our $f^*$, that is -- of course -- still a function of $x$.
So we have our black box model and predict $ŷ'$ of a new point $x'$, but we are possibly not satisfied with the received output and want to have an explanation of how the features in $x'$ are weighted to receive $ŷ'$.
This is the point, where LIME is applied to deliver an explainer model $g^*$, whose parameters we can interpret.
Recall from the introduction how this looks as a minimization problem:
$$ explanation\left(x'\right) := g^* = arg\,min_{g \epsilon G} \,\mathcal{L}\left(f^*, g, \pi_{x'} \right) + \Omega\left(g\right) $$
If we now do some small changes of notations, the task of calculating the LIME explainer can be seen as a very similar problem as the minimization problem of any original machine learning task:

$$ g^* = \arg\min_{g \epsilon G} \sum_{i=1}^{n'} \pi_{x'}(x^{(i)}) \times L\Big( f^*\big(x^{(i)}\big), \, g\big(x^{(i)}\big) \Big) + \lambda \times J(g) $$

with $\Omega\left(g\right) := \lambda \times J(g)$ and $\mathcal{L}\left(f, g, \pi_{x'} \right) := \sum_{i=1}^{n'} \pi_{x'}(x^{(i)}) \times L\Big( f\big(x^{(i)}\big), \, g\big(x^{(i)}\big) \Big)$.
Besides the fact of $G$ being supposed to only hold easily interpretable functions compared to $\mathcal{H}$ offering a wider range of models, the difference between fitting a LIME model and a normal supervised model can be precisely indentified to three parts.
The first is the replacement of the original target variable $y$ with the response $ŷ := f^*\big(x^{(i)} \big)$ of the black box model.
This means nothing more besides the fact that we do not care about our original target variable anymore.
We just don't, it's gone.
We only want to explain the black box model $f^*$ and thus completely ignore everything of distraction for a best fit of $g^*$ on $f^*$ -- even the original target.
The second difference is the factor $\pi_{x'}(x^{(i)})$ scaling the loss value of each observation.
As we have already discussed in the introduction, $\pi_{x'}$ is simply a proximity measure giving observations $x^{(i)}$ more weight the closer they are to $x'$, ensuring significantly lower loss on more distant points and thus being responsible for a local fit around $x'$ instead of a global fit of the whole available feature space.
When you read the last few words, there may just have popped up a small question mark in the back of your head.
Why 'feature space'?
Isn't it simply the original dataset instead?
The feature space is basically infinitely large for numerical cases, and almost always by more than a few factors bigger than the dataset for categorical cases.
So how does it make sense talking about the feature space instead of the real, given samples of this space?
Long story short, the answer is the solely reason why this Chapter 10 even exists.
And the answer also leads to another set of gigantic problems in practice -- it's the third difference between the equations: $n'$ is not the same as $n$.
Usually one would assume $n' < n$ because you can't simulate the data generation process, but in our case $n' >> n$.
The reason why this works is due to our target.
The normal target $y$ is a vector of values of fixed size.
Full stop.
But because our new target is not a fixed vector, but rather a function $f^*$, we can basically generate an infinite amount of new target observations (assuming the codomain is also infinite) and these observations represent exactly the ground truth $f^*(x)$ without any stochastic noise or whatsoever -- afterall functions are deterministic.
But why should someone burden this computational effort?
Isn't our real dataset enough?
The quick answer is 'no'.
The long answer is that we want a local fit of a new observation $x'$, but what if $x'$ lies in a very sparse area, or on the edge, or even far away of the space consumed by $\{x^{(i)}: i = 1, ..., n\}$?
A general advice in machine learning is to never ever assume you have a reliable fit in these cases due to local overfitting or -- the worst nightmare -- extrapolation.
These problems occure because $\{x^{(i)}: i = 1, ..., n\}$ may not offer enough anchor points for $g*$ to deliver a good fit on $\{(x, f^*(x)): x \in X\}$ for a new $x' \in X$.
So, generating more data sounds suddenly like a solid plan.
The first (and certainly not the last) issue here is the definition of the feature space.
We need a new set $\{x: x \sim X\}$ with feature space $X$ to receive the responses of $f^*$, but nowhere is written how $X$ looks like, and our original dataset is a finite sample of an infinite space (numerical features) or 'only' an exponentially large space with respect to $dim(X)$.
As a consequence, we can't assume producing a dataset equal to the size of our feature space -- we need strategies to receive the best possible representation with respect to our task.


## Sampling strategies

This chapter is about different approaches of generating a sample representing the original feature space as exactly as possible.
To see how absolutely non-trivial this is, just keep in mind that we possibly don't even need an approximation of the probability density in the feature space, but just areas where the assumed possibility of data is higher than zero, due to the deterministic nature of $f^*(x)$, and uniformly sample there.
The word 'possibly' was used here because we could also decide to use the density and draw more samples in certain areas, this would give the explanation $g^*$ of our $x'$ more anchor points in it's surrounding for a more precise fit, assuming $x'$ is located in areas of higher probability.
But what if $x'$ is located in an area of lower probability?
Unlikely, but absolutely possible with potentially disastrous results due to a missing locality around $x'$.
The next best idea to reduce the likelihood of this happening is to increase the amount of samples of our feature space.
But thanks to our good old friend 'Curse of Dimensionality' (who absolutely deserves that name), this is not feasable for an arbitrarely large feature dimension.
The Curse of Dimensionality means the feature space grows exponentially large with each added dimension, thus we would need an exponentially large sample size under the assumption of linear independence between the features.
A way to circumvent this would be to not sample in the traditional way, but rather pick a data point of our original dataset and randomly change a random amount of its feature values.
The results are so called perturbations, because they are 'perturbed' in some of their features.
It happens (like in the R implementation) that these are even called permutations, assuming this comes from the case of text data, where the words are randomly unordered, removed or inserted, thus resulting in different permutations of the whole one-hot encoded vector of the word dictionary.
To not confuse the reader 'perturbation' and 'permutation' are both not used anymore in the following, but instead the more general 'sample' describing the same thing in our case.
As a rule of thumb, sampling takes place as a form of the following trade-off:
The one extreme is to completely ignore any distribution in the data and uniformly explore all areas in the feature space, providing roughly the same saturated locality for any $x'$ to explain.
The other extreme would be to not change the original data $x \sim \{x^{(i)}: i = 1, ..., n\}$ at all, reasoning the current data points are a perfect and complete representation of all realistic feature values.
Both extremes are not a smart choice, because the former ignores the Curse of Dimensionality, while the latter assumes perfect knowledge.
The following strategies take up a position somewhere between, but something like a perfect middle has still to be found for the general case.


### Categorical features

Categorical features are handled a bit more straight forward then numerical ones due to finite space.
But this doesn't mean there's a trivial strategy.
The easiest way would be to simply sample each feature independently of the others and with probabilities of the frequency of each category appearing in the original dataset.
The cases when this goes wrong is if one category is very unlikely and then simply not drawn, giving the fitting process not enough information of how this category would influence our prediction.
If we throw away the original data after sampling, no information is left over about this category.
On the other side, by ignoring feature combinations, we may sample points that are impossible in the real world and add no value to our fit, or may even distort it.
For example imagine a dataset about eating habits of persons, with one feature called 'is_vegetarian' and the other 'favorite_food' and getting the sample combination of 'Yes' and 'Roast beef'.
Of course this is a very specific example easy to spot, but if you have high dimensional datasets, a lot of drawn samples may be wasted on non-sense combinations, filling the feature space at areas not accessible for real data on the cost of areas with plausible combinations.

Another way to draw samples would be to randomly draw a data point of our given dataset $\{x^{(i)}: i = 1, ..., n\}$, then change a random amount of features randomly to different categories.
At first, this may sound confusing, but by adjusting the expected value of the amount of features changed, we can control the trade-off stated in the end of last chapter.
By always changing all features, we basically end up with the same strategy as before.
If the expected amount of changed features is small, we lower the risk of unrealistic feature combinations.
This strategy is especially used for text data, when the underlying feature space is the size of the word dictionary with binary variables if a word is appearing in a sentence or not.
The dimension of this feature space is easily in the thousands, making it impossible to sample most word combinations.
On the other hand due to the way language works, most of the possible word combinations make no sense at all, so staying close to the original combinations is a sound plan.


### Numerical features

Numerical features rise the challenge even further.
While categorical features make it possible for at least very low dimensions to gather a dataset with all possible values, numerical features are theoretically of infinite size.
Fortunately in practice one can at least give a good approximation of the lower and upper bounds, making sure a certain denseness of the samples is possible.
 - bins
 - normal distribution
 - kernel approximated distribution
 - danger of extrapolating areas


## Side-effects of sampling
- Curse of Dimensionality OR $x'$ far off from used density leading to sparseness in local surrounding
- instability of picked features
- instability of coefficients


## Numerical features and learner instability


## Proposing removal of proximity measure with new sampling strategy


## Datasets used in experiments
- mnist as high dimensional classification with numerical features
- boston as low dimensional regression with numerical features
- transformed mnist as high dimensional classification with binary features
- mental health data as mid dimensional classification/regression with categorical features
https://www.kaggle.com/osmi/mental-health-in-tech-survey/downloads/mental-health-in-tech-survey.zip/3


## Influence of feature dimension
- Curse of dimensionality

### Categorical features
- finite feature space
- only binary distance measure

### Numerical features
- infinte feature space
- euclidian distance measure
- gower not working

## Influence of sample size
- counter acting curse of dimensionality
### Categorical features

### Numerical features


## Influence of black box stability

(stability in terms of learning theory)

### Categorical features

### Numerical features

