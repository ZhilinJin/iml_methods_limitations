# LIME and Sampling

*Author: Sebastian Gruber*


## Understanding Sampling in LIME

In this chapter we will discuss the fundamentals of LIME from a slightly different angle, compared to the introductional chapter, to receive further understanding of what enables sampling, the different implementations in the current R package (called ```LIME```) and how a basic result may look like.

### Formula

Recall the minimzation formula of LIME in the introductional chapter.
If we may do some small changes of notations, the task of calculating the LIME explainer can be seen as

$$ g^* = \arg\min_{g \epsilon G} \sum_{i=1}^{n'} \pi_{\tilde x}(x^{(i)}) L\Big( f\big(x^{(i)}\big), \, g\big(x^{(i)}\big) \Big) + \Omega\left(g\right) $$

with $\mathcal{L}\left(f, g, \pi_{\tilde x} \right) := \sum_{i=1}^{n'} \pi_{\tilde x}(x^{(i)}) \times L\Big( f\big(x^{(i)}\big), \, g\big(x^{(i)}\big) \Big)$ further expressed more in detail as in the introduction and $\tilde x$ as our desired point to explain.
This change of notation allows us to spot the enabling property for sampling.
As one may have already noticed, the original target variable $y$ is replaced by the response $Å· := f\big(x^{(i)} \big)$ of the black box model.
This means nothing more besides the fact that we can minimize this problem without accessing the original target at all.
We just don't care about it anymore, it's gone.
The great thing about this is, that $f$ can be evaluated for any value in the feature space, giving us -- theoretically -- an arbitrarily amount $n'$ of non-stochastic observations compared to before.
This may sound absolutely great at first, but we still need values of our feature space we can evaluate.
And this is where problems arise in the horizon.
At first, one may ask why even try to receive new values of the feature space?
Is our real dataset not enough?
The short answer is 'no'.
The long answer is that we want a local fit of a new observation $\tilde x$ to receive an explanation, but what if $\tilde x$ lies in a very sparse area, or on the edge, or even far away of the space consumed by $\{x^{(i)}: i = 1, ..., n\}$?
A general advice in machine learning is to never ever assume you have a reliable fit of the ground truth in these cases due to local overfitting or -- the worst nightmare -- extrapolation.
These problems occure because $\{x^{(i)}: i = 1, ..., n\}$ may not offer enough anchor points for $g*$ to deliver a good fit on $\{(x, f(x)): x \in X\}$ for a new $\tilde x \in X$.
So, generating more data sounds suddenly like a solid plan.
The first (and certainly not the last) issue here is the definition of the feature space.
We need a new set $\{x: x \sim X\}$ with feature space $X$ to receive the responses of $f$.
However, a priori it is not clear what $X$ looks like, and our original dataset is a finite sample of an infinite space (numerical features) or 'only' an exponentially large space with respect to $dim(X)$.
As a consequence, we can't assume producing a dataset equal to the size of our feature space -- we need strategies to receive the best possible representation with respect to our task.


### Sampling strategies

This chapter is about different approaches of generating a sample representing the original feature space.
The word 'possibly' was used here because we could also decide to use the density and draw more samples in certain areas, this would give the explanation $g^*$ of our $x'$ more anchor points in it's surrounding for a more precise fit, assuming $x'$ is located in areas of higher probability.
But what if $x'$ is located in an area of lower probability?
Unlikely, but absolutely possible with potentially disastrous results due to a missing locality around $x'$.
The next best idea to reduce the likelihood of this happening is to increase the amount of samples of our feature space.
But thanks to our good old friend 'Curse of Dimensionality' (who absolutely deserves that name), this is not feasable for a large feature dimension.
The Curse of Dimensionality means the feature space grows exponentially large with each added dimension, thus we would need an exponentially large sample size under the assumption of linear independence between the features.
A way to circumvent this would be to not sample in the traditional way, but rather pick a data point of our original dataset and randomly change a random amount of its feature values.
The results are so called perturbations, because they are 'perturbed' in some of their features.
It happens (like in the R implementation) that these are even called permutations, assuming this comes from the case of text data, where the words are randomly unordered, removed or inserted, thus resulting in different permutations of the whole one-hot encoded vector of the word dictionary.
To not confuse the reader 'perturbation' and 'permutation' are both not used anymore in the following, but instead the more general 'sample' describing the same thing in our case.
As a rule of thumb, sampling takes place as a form of the following trade-off:
The one extreme is to completely ignore any distribution in the data and uniformly explore all areas in the feature space, providing roughly the same saturated locality for any $x'$ to explain.
The other extreme would be to not change the original data $x \sim \{x^{(i)}: i = 1, ..., n\}$ at all, reasoning the current data points are a perfect and complete representation of all realistic feature values.
Both extremes are not a smart choice, because the former ignores the Curse of Dimensionality, while the latter assumes perfect knowledge.
The following strategies take up a position somewhere between, but something like a perfect middle has still to be found for the general case.


#### Categorical features

<!-- It would be helpful if you outlined where these ideas come from. -->
Categorical features are handled a bit more straight forward then numerical ones due to finite space.
But this doesn't mean there's a trivial strategy.
The easiest way would be to simply sample each feature independently of the others and with probabilities of the frequency of each category appearing in the original dataset.
The cases when this goes wrong is if one category is very unlikely and then simply not drawn, giving the fitting process not enough information of how this category would influence our prediction.
If we throw away the original data after sampling, no information is left over about this category.
On the other hand, by ignoring feature combinations, we may sample points that are impossible in the real world and add no value to our fit, or may even distort it. 
For example imagine a dataset about eating habits of persons, with one feature called 'is_vegetarian' and the other 'favorite_food' and getting the sample combination of 'Yes' and 'Roast beef'.
Of course, this is a very specific example easy to spot, but if you have high dimensional datasets, a lot of drawn samples may be wasted on non-sense combinations, filling the feature space at areas not accessible for real data on the cost of areas with plausible combinations.
<!-- Well, but how does non-sense then finally affect our local model? -->

Another way to draw samples would be to randomly draw a data point of our given dataset $\{x^{(i)}: i = 1, ..., n\}$, then change a random amount of features randomly to different categories.
At first, this may sound confusing, but by adjusting the expected value of the amount of features changed, we can control the trade-off stated in the end of last section.
By always changing all features, we basically end up with the same strategy as before.
If the expected amount of changed features is small, we lower the risk of unrealistic feature combinations.
This strategy is especially applicable to text data, when the underlying feature space is the size of the word dictionary with binary variables if a word is appearing in a sentence or not.
The dimension of this feature space is easily in the thousands, making it impossible to sample most word combinations.
On the other hand, due to the way language works, most of the possible word combinations make no sense at all, so staying close to the original combinations is a sound plan.


#### Numerical features

Numerical features rise the challenge even further.
While categorical features make it possible for at least very low dimensions to gather a dataset with all possible values, numerical features are theoretically of infinite size.
Fortunately in practice one can at least give a good approximation of the lower and upper bounds, making sure a certain denseness of the samples is possible.

There are currently three different options implemented in the LIME package for sampling numerical features.
The first -- and default -- one uses a fixed amount of bins.
The limits of these bins are picked so the original data is approximately equally distributed to each bin.
In the sampling step, one of these bins will be randomly picked and after that a value is uniformly sampled between the lower and upper limit of that bin.
The benefit here is being allowed to fine tune the amount of bins, leading to a rougher or more detailed sample representation of the original feature.
The downside is having a lower and upper limit of all bins, i.e. it is possible the new point for explanation lies outside of all bins.
The current implementation handles this by discretizing the explanation with each bin as a category class, making it possible to assign values to the lowest (or highest) bin even if it lies below (or above) that bin.
But this has the highly undesired consequence that the order of the bins is lost, thus risking the loss of a global fit as each bin receives its own weight.
Another option would be to approximate the original feature through a normal distribution and then sample out of that one.
This is relatively straight forward, but one may ask if the asumption of normally distributed features is correct.
Namely, it is not possible to change options for each feature, so by chosing this distribution, all your features will be handled as normally distributed -- but they do get their own mean and variance.
The consequence of a violated assumption is that the explained data point lies more likely in an area of less denseness, for example if the real distribution is very skewed to one side.
The danger of this happening is lower for the third and last option for numerical features: Approximating the real feature distribution through a kernel density estimation.
Any downsides besides slightly increased computational effort have not been encountered with this option.
Thus -- and after gathering empirical evidence supporting this -- we choose to not use bins, but rather kernel density estimation for most of our trials following down.


### Visualization of an easy example


```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "LIME implementation preliminaries", warning = FALSE}

# default input parameters
model_smoothness = 270
kernel_width     = 900
sample_seed      = 2
sample_size      = 10

# create ground truth of simulated black box model
black_box = function(x) sin(x / model_smoothness)
x = 1:1000
y = black_box(x)

set.seed(1)
# pick data point to explain (by random in this case)
x_ex = runif(1, 1, 1000)
y_ex = black_box(x_ex)
```


```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "LIME implementation", warning = FALSE}

set.seed(sample_seed)
# sample new data points uniformly
x_samp  = runif(sample_size, 1, 1000)
y_samp  = black_box(x_samp)
samples = data.frame(x = x_samp, y = y_samp)
  
# apply gaussian kernel to receive proximity weights
proximity = exp( - (x_samp - x_ex)^2 / kernel_width )
  
# fit interpretable model
model = lm(y ~ x, data = samples, weights = proximity)

```


The next figure shows LIME results of a simple numerical example.
Black box model in blue is tried to be explained by the surrogate model as the red line.
The black dots are the sampled values dealing as train data set for the surrogate model.

```{r, eval = TRUE, echo = FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3, fig.show='hold', fig.align = 'center', fig.cap = "Visulization of LIME applied on a non-linear function - the right plot uses the same settings but is resampled", warning=FALSE}

knitr::include_graphics(c("images/convex_samples_1.png", "images/convex_samples_2.png"))
#plot_lime(sample_seed = 2)
#plot_lime(sample_seed = 1)  
```

As we can see, the surrogate models depend only on randomly generated samples, that lie closer or further spread across the feature space, giving raise to the following questions.
How much influences a new sample the explanation?
What is the average confidence of certain weights?
Do settings influence these and is there a tendency?


## Sketching Problems of Sampling

A sinus shaped black box model makes two LIME explanations of the same scenario and with the same settings hold totally different results.
This indicates how untrusty single explanations could be.
What can we do here?


```{r, eval = TRUE, echo = FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3, fig.show='hold', fig.align = 'center', fig.cap = "LIME applied on a non-convex function - again, the right plot uses the same settings but is resampled", warning=FALSE}

knitr::include_graphics(c("images/nonconvex_samples_1.png", "images/nonconvex_samples_2.png"))
#plot_lime(sample_seed = 1, model_smoothness = 50)
#plot_lime(sample_seed = 2, model_smoothness = 50)

```

The most obvious solution is increasing the sample size.
This indeed shrinks the problem to almost non-existence.
But this is computationally very intensive, so it would be good to know when the additional computational effort is necessary.

```{r, eval = TRUE, echo = FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3, fig.show='hold', fig.align = 'center', fig.cap = "LIME applied on a non-convex function with increased sample size - resampled plot on the right", warning=FALSE}

knitr::include_graphics(c("images/nonconvex_samples_1_size100.png", "images/nonconvex_samples_2_size100.png"))
#plot_lime(sample_seed = 1, model_smoothness = 50, sample_size = 100)
#plot_lime(sample_seed = 2, model_smoothness = 50, sample_size = 100)
  
```

A little quickcheck what an increase of the kernel width would do in this toy example.
As we can see, the explanations are more similar, but the locality is lost.
We won't deal with this further in this chapter, because chapter 9 gave an extensive look into this subtopic, plus we don't want to lose locality and thus rather investigate further options possibly influencing weight stability of the explanation.

```{r, eval = TRUE, echo = FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3, fig.show='hold', fig.align = 'center', fig.cap = "LIME applied on a non-convex function with increased kernel width - resampled plot on the right", warning=FALSE}

knitr::include_graphics(c("images/nonconvex_samples_1_width81000.png", "images/nonconvex_samples_2_width81000.png"))
#plot_lime(sample_seed = 2, model_smoothness = 50, kernel_width = 81000)
#plot_lime(sample_seed = 1, model_smoothness = 50, kernel_width = 81000)
  
```


## Real World Problems with LIME

So far, only artificial problems have been shown, but how does LIME act applied on real world problems?
We are using real datasets in the following to show weight stability associated with different samples.

### Boston Housing Data

Boston housing dataset is a well-known data set, so a deeper description of its properties are skipped here.
It's offering a good amount of numerical features (p = 12) and can be seen as a typical case of a numerical regression task.

```{r, eval = TRUE, echo = FALSE, fig.align = 'center', fig.cap = "Overview of the normalized numerical features compared to target 'medv' in the boston housing dataset"}

knitr::include_graphics("images/boston_prezi.png")

```


#### Mean point versus outlying point
In the following weight stability is explored by resampling an explanation 100 times of specific settings.
Of the each weight the mean and the empirical quantiles of the 100 results are calculated and depicted in the figures.
Out of the quantiles we calculate and then plot the 95% confidence interval (by assuming the weights are approximately normally distributed due to the central limit theorem).
As black box model, a random forest model with untuned default parameters is used.
In the first case, the mean data point in the feature space is compared with an extreme outlier (having the maximum value of each feature in the original data set).
As we can see, the outlier has clearly larger confidence intervals as the mean point.
This suggests that either the model is behaving really roughly in its area, or, more likely, the sample size in the neighborhood has big influence on our stability.

```{r, eval = TRUE, echo = FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3, fig.show='hold', fig.align = 'center', fig.cap = "Weight coefficients of LIME applied to the mean data point with errorbars indicating the standard deviation across repeated runs"}

knitr::include_graphics(c("images/boston_standard_presi.png", "images/boston_outlier_presi.png"))

```


#### Decision tree versus linear regression model
Here, the same settings as above have been used, but both plots use the mean data point and the left plot shows the weights explaining a decision tree as black box model, while the right figure shows the case of a linear regression model.
As we can see, the differences are striking, suggesting the black box model may have huge influence on the weight stability.

```{r, eval = TRUE, echo = FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3, fig.show='hold', fig.align = 'center', fig.cap = "LIME weights of an extreme outlying data point"}

knitr::include_graphics(c("images/boston_tree_presi.png", "images/boston_lm_presi.png"))

```


#### Kernel density estimation versus binning
Again, the same settings as above, but using different sampling options.
And again, the differences are clearly visible, leading to the question if their are strict ranks in weight stability of the sampling options.

```{r, eval = TRUE, echo = FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3, fig.show='hold', fig.align = 'center', fig.cap = "LIME weights of mean data point with binning as sampling strategy -- standard deviation highly increased compared to previous results"}

knitr::include_graphics(c("images/boston_standard_presi.png", "images/boston_bins_presi.png"))

```


### Rental Bikes Data

We only used numerical features so far.
We are using a dataset with only categorical features now (rental bikes).
This means, we are forced to use gower distance (binary distance here).
Do we get similar results as for numerical features?

```{r, eval = TRUE, echo = FALSE, fig.align = 'center', fig.cap = "Overview of the normalized numerical features compared to target 'medv' in the boston housing dataset"}

knitr::include_graphics("images/bikes_prezi.png")

```


#### Mean point versus outlying point
Same settings as in the boston case.
Comparing the majority data point versus the minority data point.
The differences are a lot more subtle, almost not visible.

```{r, eval = TRUE, echo = FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3, fig.show='hold', fig.align = 'center', fig.cap = "Weight coefficients of LIME applied to the mean data point with errorbars indicating the standard deviation across repeated runs"}

knitr::include_graphics(c("images/bikes_standard_presi.png", "images/bikes_outlier_presi.png"))

```


#### Decision tree versus linear regression model
Again, same settings as before.
Comparing a decision tree with a linar regression model as black box model.
The differences are visible, but by far not as much as in the numerical case.
This suggests we include this categorical data set in our experiments further down, but expect the results won't be as clear cut as in the numerical case.

```{r, eval = TRUE, echo = FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3, fig.show='hold', fig.align = 'center', fig.cap = "LIME weights of an extreme outlying data point"}

knitr::include_graphics(c("images/bikes_tree_presi.png", "images/bikes_lm_presi.png"))

```



## Experiments about Sampling stability

We saw several evaluations of explanations of single points under different settings.
How is the general trend over certain settings?

### Influence of feature dimension

Is Curse of dimensionality real?
How is the general trend over amount of feature dimensions in the train data set?

#### Feature dimension setup

Experiment design: 10 randomly sampled data points of the original data set are explained 10 times under increasing feature dimension.
Each iteration adds a new feature to the existing feature set with a new black box model being trained on that set.
The results of the experiment in terms of stability (= standard deviation of each weight) are averaged over all features.
The plot of feature dimension vs stability is in the following.

#### Feature dimension results

```{r, eval = TRUE, echo = FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3, fig.show='hold', fig.align = 'center', fig.cap = "Average standard deviation of 10 randomly sampled points for explanation repeated 100 times. Each feature is sequentially added with a new black box model being calculated each time. Increased feature amount has no monotonous effect on coefficient stability. Curse of dimensionality is not applying in this case."}

knitr::include_graphics(c("images/sd_p_presi.png", "images/sd_pc_presi.png"))

```

No clear tendency can be found.
This suggests a general curse of dimensionality can not be confirmed.
As a further thought, the weight stability may depend on learned interactions of the black box model, but not on the actual train data.

#### Amount of features selected setup

But what about the option of selecting a small subset of the weights?
Can this option deliver higher stability in the results?

#### Amount of features selected results

Full data set used.
Averaged standard deviation of all features plotted versus the amount of features selected for explanation.
Again, no tendency can be found.

```{r, eval = TRUE, echo = FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3, fig.show='hold', fig.align = 'center', fig.cap = "Average standard deviation with the same settings as before but with full feature size. The x-axis plots the amount of features selected for the explainer. As can be seen, the weight stability is remarkably constant for low amount of features and suddenly becomes very jumpy for higher amount of selected feature. If the experiment was only evaluated for small amounts of selected features, a clear recommendation of sticking to less explained features could be given, but unfortunately no real rule of thumb can be suggested in this case. The shape of the graph may result due to globally linear predictions for the lesser important features -- assuming the features are picked by weight size."}

knitr::include_graphics(c("images/sd_nfeat_presi.png", "images/sd_nfeatc_presi.png"))

```

We can say overall no curse of dimensionality is visible.


### Influence of sample size

How does increased denseness in the feature space (by increasing sample size) increase stability of the surrogate model?
Results of single trials above suggest a lot.

#### Sample size setup
Experiment design: 10 randomly picked datapoints of the original data set explained 10 times with iteration over amount of permutations/size of sample.
Again standard deviation of each weight calculated and averaged over all weights.

#### Sample size results


```{r, eval = TRUE, echo = FALSE, fig.align = 'center', fig.cap = "Average standard deviation under the same settings as before. A clear trend can be seen here: Increasing the amount of samples acting as train data for our surrogate model has incredible influence on weight stability."}
knitr::include_graphics(c("images/sd_npermutations_presi.png"))
```

Clear and monotonous trend of more permutations giving better stability can be seen.
Feature dimension is unrelated to weight stability, but sample size highly related - does this make sense?
We should move focus from train data to black box model.


### Influence of black box fit
The simulation suggests more overfitting and less smoothness of the prediction surface influences weight stability.
Interim explanation of the random forest hyperparameters:
Higher tree amount gives the prediction surface more smoothness (by reducing average step size of each step in the prediction function), while higher minimum node size reduces overfitting (by making predictions dependent on more train data points).

#### Motivation
We are demonstrating the problem by a small semi simulation.
We are using sampled boston housing data of sample size 20, modelling only medv ~ lstat.
Because LIME doesn't know the original data, the resulting black box fit seems like an impossible task to approximate linearly in an appropriate manner with only samples.

```{r, eval = TRUE, echo = FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3, fig.show='hold', fig.align = 'center', fig.cap = "Average standard deviation under the same settings as before. A clear trend can be seen here: Increasing the amount of samples acting as train data for our surrogate model has incredible influence on weight stability."}
knitr::include_graphics(c("images/boston_sampled_tree_presi.png", "images/boston_sampled_tree_1_presi.png"))
```


#### Black box fit setup
We expected: Better stability of smoother models (random forest and linear model).
Experiment: 10 randomly picked data points of the original data set explained 10 times are used for iteration over random forest with different hyperparameter settings (each iteration more trees are added and minimum node size increased, leading to less overfitting and more smoothness)
Results again used to get average standard deviation (= average weight stability).


#### Black box fit results

```{r, eval = TRUE, echo = FALSE, fig.align = 'center', fig.cap = "Average standard deviation of the same settings as before but with black box model smoothness/complexity iterated over. As a base model a random forest with only one tree and minimum node size of one was used and each iteration the amount of trees was increased by 10 and the minimum node size by one. This means the last tick in this graph is corresponding to a random forest with 91 trees and minimum node size of 10. It's nicely shown how important the smoothness of the model is for the weight stability. Keep in mind the model was fitted on some training data, which means if we would pick more complex data, the line may take much longer to flatten out, and vice versa for less complex data."}
knitr::include_graphics("images/sd_smoothness_presi.png")
```

As can be seen, a big influence of the smoothness is present.
Although the influence flattens quickly, but this may be different for more complex data sets requiring more "overfitting" (meant is less smooth fitting).
This gives raise to the last experiment.

#### Black box overfit setup

Before, we started with a really unsmooth model and gradually added more regularisation (more trees and higher minimum node size).
But now, we are doing the opposite with a model class being able to fit an arbitrarily complex data structure with increase of only a single hyperparameter: extreme gradient boosting.
We start with only two trees (often known as 'M' or 'nrounds' or as 'the stopping parameter') and double the amount with each iteration.
All the other settings and the procedure for receiving the results are kept the same.

#### Black box overfit results

```{r, eval = TRUE, echo = FALSE, fig.align = 'center', fig.cap = "Uniform sampling strategy -- black dots are sampled data points, red line is the LIME explanation, yellow dot the explained data point and the horizontal lines the square root of the kernel width, approximating size of neighborhood"}

knitr::include_graphics("images/sd_overfitting_presi.png")

```

As we can see, as long as the xgboost learner is able to reduce the train error, the weight stability gets consistently worse.
Let's try to get an idea of why this is happening in such a clear manner.
What makes the train error get smaller? 
Reducing the residuals.
What consequence has reducing the residuals in this case on the prediction surface? 
It becomes more volatile, assuming a certain level of noise.
And this volatility kills our weight stability.

## Conclusion
Categorical data and numerical data have weight stability issues, but categorical data offers less settings, making it easier to handle.
But, numerical data can be improved a lot more by tweaking some settings.
On of these is turning of binning, which is the default option in the LIME package.
This results in a huge benefit in the weight stability in all encountered cases.
What makes us have trust in a single explanation?
Weight stability is almost independent of the weight size, so high weights are very trustworthy.
Also picking a very high sample size increases the stability.
Do this whenever possible.
Furthermore, what makes us have no trust in the result?
When we know the data set is very complex with a curvy/wavy fit almost surely going to happen.
And if the model we are using is capable of extreme overfitting.
In this case the less regularisation we put onto it, the less trustworthy our LIME explanations are going to be.

```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "LIME output", warning = FALSE}
req_packages  = c("lime", "mlr", "ggplot2", "ranger", "mlbench")
install_these = req_packages[!(req_packages %in% installed.packages())]
# Wo bleibt meine Zitation ;-)? Better work with the Description file, though!

if (length(install_these) > 0) install.packages(install_these)
library(ggplot2)

plot_lime = function(model_smoothness = 270, sample_seed, kernel_width = 900, sample_size = 10) {
  
  # create ground truth
  black_box = function(x) sin(x / model_smoothness)
  x = 1:1000
  y = black_box(x)

  set.seed(1)
  # randomly pick data point to explain
  x_ex = runif(1, 1, 1000)
  y_ex = black_box(x_ex)
  
  set.seed(sample_seed)
  # sample new data points
  x_samp = runif(sample_size, 1, 1000)
  y_samp = black_box(x_samp)
  data   = data.frame(x = x_samp, y = y_samp)
  
  # apply gaussian kernel to receive weights
  weights = exp( - (x_samp - x_ex)^2 / kernel_width )
  
  # fit surrogate model and get predictions
  model  = lm(y ~ x, data = data, weights = weights)
  y_pred = predict(model, newdata = data.frame(x = x))
  
  # visualize everything
  ggplot(data = NULL, aes(y = y, x = x)) +
    geom_line(color = "#00C5CD", size = 1.5) +
    geom_point(data = NULL, aes(x = x_samp, y = y_samp)) +
    geom_line(data = NULL, aes(x = x, y = y_pred), color = "#e04d2e", size = 1) +
    geom_point(data = NULL, aes(x = x_ex, y = y_ex), color = "#c1c10d", size = 3) +
    geom_vline(aes(xintercept = x_ex - sqrt(kernel_width))) +
    geom_vline(aes(xintercept = x_ex + sqrt(kernel_width))) +
    theme_minimal() +
    ylim(c(-1.5, 1.5)) +
    ylab("target") +
    xlab("feature")

}
  
```

```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "LIME output", warning=FALSE}

library(ggplot2)
library(mlbench)
library(mlr)


book_plot = function(means, sds, color1 = rgb(135/255, 150/255, 40/255), color2 = rgb(70/255, 95/255, 25/255), xlab = "Feature", ylab = "Weight", angle = 0, hjust = 0) {

  ggplot(data = NULL, aes(x = names(means), y = means)) +
    geom_bar(stat = "identity", fill = color1) +
    geom_errorbar(aes(ymin = means - sds, ymax = means + sds), color = color2, width = 0.4, size = 1.2, alpha = 0.7) +
    theme_minimal() +
    theme(
      text = element_text(size = 15),
      axis.title.x = element_text(vjust = -4),
      axis.text.x = element_text(angle = angle, hjust = hjust),
      plot.margin = ggplot2::margin(20,20,30,20)
    ) +
    xlab(xlab) +
    ylab(ylab)
}

presi_plot = function(means, lower, upper, color1 = rgb(135/255, 150/255, 40/255), color2 = rgb(70/255, 95/255, 25/255), xlab = "Feature", ylab = "Weight", angle = 0, hjust = 0, ylim = NULL) {

  ggplot(data = NULL, aes(x = names(means), y = means, ymin = lower, ymax = upper)) +
    geom_errorbar(color = color1, width = 1L, size = 1.6) +
    geom_point(color = color2, size = 2L) +
    geom_hline(yintercept = 0L) +
    theme_minimal() +
    theme(
      text = element_text(size = 25),
      axis.title.x = element_text(vjust = -4),
      axis.text.x = element_text(angle = angle, hjust = hjust),
      plot.margin = ggplot2::margin(20,20,30,20)
    ) +
    xlab(xlab) +
    ylab(ylab) +
    ylim(ylim)
}


data("BostonHousing", package = "mlbench")
# removing categorical feature
boston     = BostonHousing[, -4]
# normalizing standard deviation to make coefficients comparable
boston     = as.data.frame(lapply(boston, function(x) x/sd(x)))

plots = lapply(
  names(boston[-ncol(boston)]), function(feat) qplot(get(feat), medv, data = boston, ylab = "", xlab = feat) + theme(
  axis.text.x = element_blank(),
  axis.text.y = element_blank(),
  axis.ticks = element_blank(),
  text = element_text(size = 20))
)
args = paste0("plots[[", 1:12, "]]", collapse = ", ")
grid_plot_func = paste0("gridExtra::grid.arrange(", args, ", nrow = 3)")

filename = paste0("images/boston_prezi.png")
png(filename, width = 700L, height = 500L)
eval(parse(text = grid_plot_func))
dev.off()

btask      = makeRegrTask(data = boston, target = "medv")

# normal non-continuous model
regr_model = makeLearner("regr.ranger")
black_box  = train(regr_model, btask)
explainer  = lime(boston[, -ncol(boston)], black_box, bin_continuous = FALSE, use_density = FALSE)

# pick mean of each feature as data point
data_point = as.data.frame(lapply(boston[, -ncol(boston)], mean))
# set index of datapoint to use
limes = sapply(1:100, function(k) {
  interim = explain(data_point, explainer, n_features = ncol(boston)-1, dist_fun = "euclidian")
  weights_l = interim$feature_weight
  names(weights_l) = interim$feature
  weights_l
})

means = apply(limes, MARGIN = 1, mean)
sds   = apply(limes, MARGIN = 1, sd)
ci_upper = apply(limes, MARGIN = 1, quantile, 0.975)
ci_lower = apply(limes, MARGIN = 1, quantile, 0.025)

filename = paste0("images/boston_standard_presi.png")
png(filename, width = 700L, height = 500L)
presi_plot(means, lower = ci_lower, ci_upper, angle = 45L, hjust = 1L, ylim = c(-0.4, 0.3))
dev.off()

filename = paste0("images/boston_100iter_standard.png")
png(filename, width = 700L, height = 500L)
book_plot(means, sds)
dev.off()

means_scaled = means / means
sds_scaled   = sds / abs(means)

filename = paste0("images/boston_100iter_scaled.png")
png(filename, width = 700L, height = 500L)
book_plot(means_scaled, sds_scaled, ylab = "Weight / weight")
dev.off()


# pick max of each feature as data point
data_point = as.data.frame(lapply(boston[, -ncol(boston)], max))
# set index of datapoint to use
limes = sapply(1:100, function(k) {
  interim = explain(data_point, explainer, n_features = ncol(boston)-1, dist_fun = "euclidian")
  weights_l = interim$feature_weight
  names(weights_l) = interim$feature
  weights_l
})


means = apply(limes, MARGIN = 1, mean)
sds   = apply(limes, MARGIN = 1, sd)
ci_upper = apply(limes, MARGIN = 1, quantile, 0.975)
ci_lower = apply(limes, MARGIN = 1, quantile, 0.025)

filename = paste0("images/boston_outlier_presi.png")
png(filename, width = 700L, height = 500L)
presi_plot(means, lower = ci_lower, ci_upper, angle = 45L, hjust = 1L, ylim = c(-0.4, 0.3))
dev.off()


filename = paste0("images/boston_100iter_outlier.png")
png(filename, width = 700L, height = 500L)
book_plot(means, sds)
dev.off()

means_scaled = means / means
sds_scaled   = sds / abs(means)

filename = paste0("images/boston_100iter_outlier_scaled.png")
png(filename, width = 700L, height = 500L)
book_plot(means_scaled, sds_scaled, ylab = "Weight / weight")
dev.off()

# switch to bins
explainer  = lime(boston[, -ncol(boston)], black_box)

# pick mean of each feature as data point
data_point = as.data.frame(lapply(boston[, -ncol(boston)], mean))
# set index of datapoint to use
limes = sapply(1:100, function(k) {
  interim = explain(data_point, explainer, n_features = ncol(boston)-1)
  weights_l = interim$feature_weight
  names(weights_l) = interim$feature
  weights_l
})


means = apply(limes, MARGIN = 1, mean)
sds   = apply(limes, MARGIN = 1, sd)
ci_upper = apply(limes, MARGIN = 1, quantile, 0.975)
ci_lower = apply(limes, MARGIN = 1, quantile, 0.025)

filename = paste0("images/boston_bins_presi.png")
png(filename, width = 700L, height = 500L)
presi_plot(means, lower = ci_lower, ci_upper, angle = 45L, hjust = 1L, ylim = c(-0.4, 0.3))
dev.off()


filename = paste0("images/boston_100iter_bins.png")
png(filename, width = 700L, height = 500L)
book_plot(means, sds)
dev.off()

means_scaled = means / means
sds_scaled   = sds / abs(means)

filename = paste0("images/boston_100iter_bins_scaled.png")
png(filename, width = 700L, height = 500L)
book_plot(means_scaled, sds_scaled, ylab = "Weight / weight")
dev.off()


# pick max of each feature as data point
data_point = as.data.frame(lapply(boston[, -ncol(boston)], max))
# set index of datapoint to use
limes = sapply(1:100, function(k) {
  interim = explain(data_point, explainer, n_features = ncol(boston)-1)
  weights_l = interim$feature_weight
  names(weights_l) = interim$feature
  weights_l
})


means = apply(limes, MARGIN = 1, mean)
sds   = apply(limes, MARGIN = 1, sd)


filename = paste0("images/boston_100iter_bins_outlier.png")
png(filename, width = 700L, height = 500L)
book_plot(means, sds)
dev.off()

means_scaled = means / means
sds_scaled   = sds / abs(means)

filename = paste0("images/boston_100iter_bins_outlier_scaled.png")
png(filename, width = 700L, height = 500L)
book_plot(means_scaled, sds_scaled, ylab = "Weight / weight")
dev.off()


### LINEAR MODEL
regr_model = makeLearner("regr.lm")
black_box  = train(regr_model, btask)
explainer  = lime(boston[, -ncol(boston)], black_box, bin_continuous = FALSE, use_density = FALSE)

# pick mean of each feature as data point
data_point = as.data.frame(lapply(boston[, -ncol(boston)], mean))
# set index of datapoint to use
limes = sapply(1:100, function(k) {
  interim = explain(data_point, explainer, n_features = ncol(boston)-1, dist_fun = "euclidian")
  weights_l = interim$feature_weight
  names(weights_l) = interim$feature
  weights_l
})


means = apply(limes, MARGIN = 1, mean)
sds   = apply(limes, MARGIN = 1, sd)
ci_upper = apply(limes, MARGIN = 1, quantile, 0.975)
ci_lower = apply(limes, MARGIN = 1, quantile, 0.025)

filename = paste0("images/boston_lm_presi.png")
png(filename, width = 700L, height = 500L)
presi_plot(means, lower = ci_lower, ci_upper, angle = 45L, hjust = 1L, ylim = c(-0.45, 0.35))
dev.off()


filename = paste0("images/boston_lm.png")
png(filename, width = 700L, height = 500L)
book_plot(means, sds)
dev.off()

means_scaled = means / means
sds_scaled   = sds / abs(means)

filename = paste0("images/boston_lm_scaled.png")
png(filename, width = 700L, height = 500L)
book_plot(means_scaled, sds_scaled, ylab = "Weight / weight")
dev.off()


# pick max of each feature as data point
data_point = as.data.frame(lapply(boston[, -ncol(boston)], max))
# set index of datapoint to use
limes = sapply(1:100, function(k) {
  interim = explain(data_point, explainer, n_features = ncol(boston)-1, dist_fun = "euclidian")
  weights_l = interim$feature_weight
  names(weights_l) = interim$feature
  weights_l
})


means = apply(limes, MARGIN = 1, mean)
sds   = apply(limes, MARGIN = 1, sd)


filename = paste0("images/boston_lm_outlier.png")
png(filename, width = 700L, height = 500L)
book_plot(means, sds)
dev.off()

means_scaled = means / means
sds_scaled   = sds / abs(means)

filename = paste0("images/boston_lm_outlier_scaled.png")
png(filename, width = 700L, height = 500L)
book_plot(means_scaled, sds_scaled, ylab = "Weight / weight")
dev.off()

# switch to bins
explainer  = lime(boston[, -ncol(boston)], black_box)

# pick mean of each feature as data point
data_point = as.data.frame(lapply(boston[, -ncol(boston)], mean))
# set index of datapoint to use
limes = sapply(1:100, function(k) {
  interim = explain(data_point, explainer, n_features = ncol(boston)-1)
  weights_l = interim$feature_weight
  names(weights_l) = interim$feature
  weights_l
})


means = apply(limes, MARGIN = 1, mean)
sds   = apply(limes, MARGIN = 1, sd)


filename = paste0("images/boston_lm_bins.png")
png(filename, width = 700L, height = 500L)
book_plot(means, sds)
dev.off()

means_scaled = means / means
sds_scaled   = sds / abs(means)

filename = paste0("images/boston_lm_bins_scaled.png")
png(filename, width = 700L, height = 500L)
book_plot(means_scaled, sds_scaled, ylab = "Weight / weight")
dev.off()


# pick max of each feature as data point
data_point = as.data.frame(lapply(boston[, -ncol(boston)], max))
# set index of datapoint to use
limes = sapply(1:100, function(k) {
  interim = explain(data_point, explainer, n_features = ncol(boston)-1)
  weights_l = interim$feature_weight
  names(weights_l) = interim$feature
  weights_l
})


means = apply(limes, MARGIN = 1, mean)
sds   = apply(limes, MARGIN = 1, sd)


filename = paste0("images/boston_lm_bins_outlier.png")
png(filename, width = 700L, height = 500L)
book_plot(means, sds)
dev.off()

means_scaled = means / means
sds_scaled   = sds / abs(means)

filename = paste0("images/boston_lm_bins_outlier_scaled.png")
png(filename, width = 700L, height = 500L)
book_plot(means_scaled, sds_scaled, ylab = "Weight / weight")
dev.off()


### OVERFITTING MODEL
regr_model = makeLearner("regr.ranger", num.trees = 1, min.node.size = 1)
black_box  = train(regr_model, btask)
explainer  = lime(boston[, -ncol(boston)], black_box, bin_continuous = FALSE, use_density = FALSE)

# pick mean of each feature as data point
data_point = as.data.frame(lapply(boston[, -ncol(boston)], mean))
# set index of datapoint to use
limes = sapply(1:100, function(k) {
  interim = explain(data_point, explainer, n_features = ncol(boston)-1, dist_fun = "euclidian")
  weights_l = interim$feature_weight
  names(weights_l) = interim$feature
  weights_l
})


means = apply(limes, MARGIN = 1, mean)
sds   = apply(limes, MARGIN = 1, sd)
ci_upper = apply(limes, MARGIN = 1, quantile, 0.975)
ci_lower = apply(limes, MARGIN = 1, quantile, 0.025)

filename = paste0("images/boston_tree_presi.png")
png(filename, width = 700L, height = 500L)
presi_plot(means, lower = ci_lower, ci_upper, angle = 45L, hjust = 1L, ylim = c(-0.45, 0.35))
dev.off()


filename = paste0("images/boston_tree.png")
png(filename, width = 700L, height = 500L)
book_plot(means, sds)
dev.off()

means_scaled = means / means
sds_scaled   = sds / abs(means)

filename = paste0("images/boston_tree_scaled.png")
png(filename, width = 700L, height = 500L)
book_plot(means_scaled, sds_scaled, ylab = "Weight / weight")
dev.off()


# pick max of each feature as data point
data_point = as.data.frame(lapply(boston[, -ncol(boston)], max))
# set index of datapoint to use
limes = sapply(1:100, function(k) {
  interim = explain(data_point, explainer, n_features = ncol(boston)-1, dist_fun = "euclidian")
  weights_l = interim$feature_weight
  names(weights_l) = interim$feature
  weights_l
})


means = apply(limes, MARGIN = 1, mean)
sds   = apply(limes, MARGIN = 1, sd)


filename = paste0("images/boston_tree_outlier.png")
png(filename, width = 700L, height = 500L)
book_plot(means, sds)
dev.off()

means_scaled = means / means
sds_scaled   = sds / abs(means)

filename = paste0("images/boston_tree_outlier_scaled.png")
png(filename, width = 700L, height = 500L)
book_plot(means_scaled, sds_scaled, ylab = "Weight / weight")
dev.off()

```


```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "LIME output", warning=FALSE}
#################################
# AMOUNT OF FEATURES VS STABILITY
#################################

library(mlbench)
library(mlr)
library(lime)
library(ggplot2)


#' @description function used in this chunk for general stability measurement in dependence
#' of feature dimensions and amount of selected features (n_features parameter)
#' @return dataframe of feature weights
#' @example result = feature_growth(iris, "Species", dim_increment = 1)
feature_growth = function(
  data,
  target,
  pts_to_predict,
  type,
  repeats = 10L,
  dim_increment = 10L,
  bin_continuous = FALSE,
  use_density = FALSE
  ) {
  
  # dimension of feature space
  p_max = ncol(data) - 1L
  feature_names = names(data[names(data) != target])

  # move target variable to the end
  data_sort           = data[names(data) != target]
  data_sort[[target]] = data[[target]]
  
  # remove the target variable from points for interpretation
  pts_to_predict = pts_to_predict[names(pts_to_predict) != target]

  # iterate over amount of feature dimensions
  outer_return = lapply(#parallel::mclapply(
    #mc.cores = 4,
    seq(2L, p_max, by = dim_increment),
    function(p) {
      
      # define train data based on iterated dimension
      train_data           = data_sort[, 1L:p]
      train_data[[target]] = data_sort[[target]]
      
      # define task and learner based on data type
      if (type == "classif") {
        task = makeClassifTask(data = train_data, target = target)
        learner = makeLearner("classif.randomForest", ntree = 20L, predict.type = "prob")
        
      } else if (type == "regr") {
        task = makeRegrTask(data = train_data, target = target)
        learner = makeLearner("regr.randomForest", ntree = 20L)
        
      } else {
        stop("Wrong type, buddy")
      }
      
      black_box = train(learner, task)
      explainer = lime(train_data[1L:p], black_box, bin_continuous = bin_continuous, use_density = use_density)

      if (!bin_continuous && use_density) {
        lapply(1:length(explainer$feature_distribution), function(i) {
          explainer$feature_distribution[[i]]["mean"] <<- mean(explainer$feature_distribution[[i]]$x, na.rm = TRUE)
          explainer$feature_distribution[[i]]["sd"]   <<- sd(  explainer$feature_distribution[[i]]$x, na.rm = TRUE)
        })
      }
     
      # create sequence of "n_feature" arguments
      n_feat_seq = seq(1L, p, by = dim_increment)
      n_feat_seq = rep(n_feat_seq, each = repeats)
      
      # iterate over sequence of "n_feature" arguments
      inner_return = lapply(
        n_feat_seq,
        function(n_features) {
          
          feat_return        = rep(NA, p_max)
          names(feat_return) = feature_names
          
          # iterate over all points for interpretation
          inner_inner = apply(
            pts_to_predict,
            MARGIN = 1,
            function(target_pt) {
              
              explanation = explain(
                as.data.frame(t(target_pt[1:p])),
                explainer,
                n_labels = 1L,
                n_features = n_features,
                dist_fun = "euclidian",
                kernel_width = 100
              )
              
              to_update = names(feat_return) %in% explanation$feature
              feat_return[to_update] = explanation$feature_weight
              names(target_pt) = paste0("data_", feature_names)
              
              c(
                p = p,
                n_features = n_features,
                target_pt,
                feat_return
              )
            }
          )
          # transform from matrix to dataframe
          as.data.frame(t(inner_inner))
        }
      )

      # output progress
      log = sprintf("%2.2f/1.00 done", (p-1)/(p_max-1))
      print(log)
      # concatenate dataframes
      data.table::rbindlist(inner_return)
    }
  )
  # concatenate dataframes
  data.table::rbindlist(outer_return)
}


data("BostonHousing", package = "mlbench")
# removing categorical feature
boston     = BostonHousing[, -4]
# normalizing standard deviation to make coefficients comparable
boston     = as.data.frame(lapply(boston, function(x) x/sd(x)))
# forgot seed, pls dont kill me
set.seed(123)
pts_to_predict = boston[sample(1:nrow(boston), 10), -ncol(boston)]

# this may take a while
#results = feature_growth(boston, "medv", pts_to_predict, type = "regr", dim_increment = 1L, repeats = 10)
#saveRDS(results, "LIME_experiment_results/feature_growth_boston_repeats10")
results  = readRDS("LIME_experiment_results/feature_growth_boston_repeats10")
results$type = "kernel"
#resultsb = feature_growth(
#  boston, "medv", pts_to_predict,
#  type = "regr",
#  dim_increment = 1L,
#  repeats = 10,
#  bin_continuous = TRUE,
#  use_density = TRUE
#)
#saveRDS(resultsb, "LIME_experiment_results/feature_growth_boston_repeats10_bins")
resultsb = readRDS("LIME_experiment_results/feature_growth_boston_repeats10_bins")
resultsb$type = "bins"
#resultsn = feature_growth(
#  boston, "medv", pts_to_predict,
#  type = "regr",
#  dim_increment = 1L,
#  repeats = 10,
#  use_density = TRUE
#)
#saveRDS(resultsn, "LIME_experiment_results/feature_growth_boston_repeats10_nd")
resultsn = readRDS("LIME_experiment_results/feature_growth_boston_repeats10_nd")
resultsn$type = "normal"

### categorical data
bikes = read.csv("datasets/day.csv")
# remove undesired variables
bikes = bikes[-which(names(bikes) %in% c("casual", "registered", "instant", "dteday"))]

# quantile binning with 4 bins
bikes[c("temp", "atemp", "hum", "windspeed")] = lapply(
  bikes[c("temp", "atemp", "hum", "windspeed")],
  function(vec) {
    quantiles = quantile(vec)
    quantiles[5] = Inf
    sapply(vec, function(x) sum(x >= quantiles))
  }
)
# normalize
bikes$cnt = bikes$cnt / sd(bikes$cnt)

bikes[-ncol(bikes)] = lapply(bikes[-ncol(bikes)], as.factor)
set.seed(123)
pts_to_predict = bikes[sample(10, 1:nrow(bikes)), -ncol(bikes)]

#resultsc = feature_growth(
#  bikes, "cnt", pts_to_predict,
#  type = "regr", dim_increment = 1L, repeats = 10,
#  bin_continuous = TRUE,
#  use_density = TRUE
#)
#saveRDS(resultsc, "LIME_experiment_results/feature_growth_bikes_repeats10")
resultsc = readRDS("LIME_experiment_results/feature_growth_bikes_repeats10")
resultsc$type = "categorical"


results = rbind(results, resultsb, resultsn)
# boston
results_gr = dplyr::group_by(
  results,
  data_crim,
  data_zn,
  data_indus,
  data_nox,
  data_rm,
  data_age,
  data_dis,
  data_rad,
  data_tax,
  data_ptratio,
  data_b,
  data_lstat,
  p,
  type
)

# bikes
resultsc_gr = dplyr::group_by(
  resultsc,
  data_season,
  data_yr,
  data_mnth,
  data_holiday,
  data_weekday,
  data_workingday,
  data_weathersit,
  data_temp,
  data_atemp,
  data_hum,
  data_windspeed,
  p,
  type
)

# boston
results_sd = as.data.frame(dplyr::summarize(
  results_gr,
  sd_crim  = sd(crim, na.rm = TRUE),
  sd_zn    = sd(zn, na.rm = TRUE),
  sd_indus = sd(indus, na.rm = TRUE),
  sd_nox   = sd(nox, na.rm = TRUE),
  sd_rm    = sd(rm, na.rm = TRUE),
  sd_age   = sd(age, na.rm = TRUE),
  sd_dis   = sd(dis, na.rm = TRUE),
  sd_rad   = sd(rad, na.rm = TRUE),
  sd_tax   = sd(tax, na.rm = TRUE),
  sd_ptratio = sd(ptratio, na.rm = TRUE),
  sd_b     = sd(b, na.rm = TRUE),
  sd_lstat = sd(lstat, na.rm = TRUE)#,
))
# the following shows that the standard deviation is the same across all features
# thus standardizing would do more harm than good
#summary(results_sd)
# bikes
resultsc_sd = as.data.frame(dplyr::summarize(
  resultsc_gr,
  sd_season     = sd(as.numeric(as.character(season)), na.rm = TRUE),
  sd_yr         = sd(as.numeric(as.character(yr)), na.rm = TRUE),
  sd_mnth       = sd(as.numeric(as.character(mnth)), na.rm = TRUE),
  sd_holiday    = sd(as.numeric(as.character(holiday)), na.rm = TRUE),
  sd_weekday    = sd(as.numeric(as.character(weekday)), na.rm = TRUE),
  sd_workingday = sd(as.numeric(as.character(workingday)), na.rm = TRUE),
  sd_weathersit = sd(as.numeric(as.character(weathersit)), na.rm = TRUE),
  sd_temp       = sd(as.numeric(as.character(temp)), na.rm = TRUE),
  sd_atemp      = sd(as.numeric(as.character(atemp)), na.rm = TRUE),
  sd_hum        = sd(as.numeric(as.character(hum)), na.rm = TRUE),
  sd_windspeed  = sd(as.numeric(as.character(windspeed)), na.rm = TRUE)
))
# the following shows that the standard deviation is the same across (almost) all features
# thus standardizing would do more harm than good
#summary(resultsc_sd)

results_gr = dplyr::group_by(
  results_sd,
  p,
  type
)

# bikes
resultsc_gr = dplyr::group_by(
  resultsc_sd,
  p,
  type
)

# boston
results_sd = as.data.frame(dplyr::summarize(
  results_gr,
  sd_crim  = mean(sd_crim, na.rm = TRUE),
  sd_zn    = mean(sd_zn, na.rm = TRUE),
  sd_indus = mean(sd_indus, na.rm = TRUE),
  sd_nox   = mean(sd_nox, na.rm = TRUE),
  sd_rm    = mean(sd_rm, na.rm = TRUE),
  sd_age   = mean(sd_age, na.rm = TRUE),
  sd_dis   = mean(sd_dis, na.rm = TRUE),
  sd_rad   = mean(sd_rad, na.rm = TRUE),
  sd_tax   = mean(sd_tax, na.rm = TRUE),
  sd_ptratio = mean(sd_ptratio, na.rm = TRUE),
  sd_b     = mean(sd_b, na.rm = TRUE),
  sd_lstat = mean(sd_lstat, na.rm = TRUE)#,
))
# the following shows that the standard deviation is the same across all features
# thus standardizing would do more harm than good
#summary(results_sd)
# bikes
resultsc_sd = as.data.frame(dplyr::summarize(
  resultsc_gr,
  sd_season     = mean(as.numeric(as.character(sd_season)), na.rm = TRUE),
  sd_yr         = mean(as.numeric(as.character(sd_yr)), na.rm = TRUE),
  sd_mnth       = mean(as.numeric(as.character(sd_mnth)), na.rm = TRUE),
  sd_holiday    = mean(as.numeric(as.character(sd_holiday)), na.rm = TRUE),
  sd_weekday    = mean(as.numeric(as.character(sd_weekday)), na.rm = TRUE),
  sd_workingday = mean(as.numeric(as.character(sd_workingday)), na.rm = TRUE),
  sd_weathersit = mean(as.numeric(as.character(sd_weathersit)), na.rm = TRUE),
  sd_temp       = mean(as.numeric(as.character(sd_temp)), na.rm = TRUE),
  sd_atemp      = mean(as.numeric(as.character(sd_atemp)), na.rm = TRUE),
  sd_hum        = mean(as.numeric(as.character(sd_hum)), na.rm = TRUE),
  sd_windspeed  = mean(as.numeric(as.character(sd_windspeed)), na.rm = TRUE)
))

plot_data = data.frame(sd = c(
  apply(results_sd[-1:-2], MARGIN = 1, function(row) mean(row[!is.nan(row)]))#,
#  apply(resultsc_sd[-1:-2], MARGIN = 1, function(row) mean(row[!is.nan(row)]))
))
plot_data$p = c(results_sd$p)#, as.numeric(as.character(resultsc_sd$p)))
plot_data$type = c(results_sd$type)#, resultsc_sd$type)


plot = ggplot(plot_data, aes(y = sd, x = p, color = type)) +
  geom_line(size = 2) +
  theme_minimal() +
  theme(
    text = element_text(size = 25L),
    axis.title.x = element_text(vjust = -4L),
    axis.text.x = element_text(angle = 45L, hjust = 1L),
    plot.margin = ggplot2::margin(20L, 20L, 30L, 20L),
    axis.ticks.x = element_blank()
  ) +
  ylab("Average Standard Deviation") +
  xlab("Added feature") +
  scale_x_continuous(
    breaks = 1L:12L,
    labels = c("", "crim & zn", names(boston[c(-1, -2, -13)]))
  ) +
  ylim(c(0, 0.2))

filename = paste0("images/sd_p_presi2.png")
png(filename, width = 700L, height = 500L)
plot
dev.off()

plot_datac = data.frame(
  sd = apply(resultsc_sd[-1:-2], MARGIN = 1, function(row) mean(row[!is.nan(row)], na.rm = TRUE)),
  p = as.numeric(as.character(resultsc_sd$p)),
  type = resultsc_sd$type
)

plotc = ggplot(plot_datac, aes(y = sd, x = p, color = type)) +
  geom_line(size = 2L) +
  theme_minimal() +
  theme(
    text = element_text(size = 25L),
    axis.title.x = element_text(vjust = -4L),
    axis.text.x = element_text(angle = 45L, hjust = 1L),
    plot.margin = ggplot2::margin(20L, 20L, 30L, 20L),
    axis.ticks.x = element_blank()
  ) +
  ylab("Average Standard Deviation") +
  xlab("Added feature") +
  scale_x_continuous(
    breaks = 1L:11L,
    labels = c("", "season & yr", names(bikes[c(-1, -2, -12)]))
  ) +
  ylim(c(0, 0.2))


filename = paste0("images/sd_pc_presi2.png")
png(filename, width = 700L, height = 500L)
plotc
dev.off()


####################
### plotting n_features
# boston
results_12 = results[results$p == 12, ]
# bikes
resultsc_11 = resultsc[resultsc$p == 11, ]

results_gr = dplyr::group_by(
  results_12,
  data_crim,
  data_zn,
  data_indus,
  data_nox,
  data_rm,
  data_age,
  data_dis,
  data_rad,
  data_tax,
  data_ptratio,
  data_b,
  data_lstat,
  n_features,
  type
)

# bikes
resultsc_gr = dplyr::group_by(
  resultsc_11,
  data_season,
  data_yr,
  data_mnth,
  data_holiday,
  data_weekday,
  data_workingday,
  data_weathersit,
  data_temp,
  data_atemp,
  data_hum,
  data_windspeed,
  n_features,
  type
)

# boston
results_sd = as.data.frame(dplyr::summarize(
  results_gr,
  sd_crim  = sd(crim, na.rm = TRUE),
  sd_zn    = sd(zn, na.rm = TRUE),
  sd_indus = sd(indus, na.rm = TRUE),
  sd_nox   = sd(nox, na.rm = TRUE),
  sd_rm    = sd(rm, na.rm = TRUE),
  sd_age   = sd(age, na.rm = TRUE),
  sd_dis   = sd(dis, na.rm = TRUE),
  sd_rad   = sd(rad, na.rm = TRUE),
  sd_tax   = sd(tax, na.rm = TRUE),
  sd_ptratio = sd(ptratio, na.rm = TRUE),
  sd_b     = sd(b, na.rm = TRUE),
  sd_lstat = sd(lstat, na.rm = TRUE)#,
))
# the following shows that the standard deviation is the same across all features
# thus standardizing would do more harm than good
#summary(results_sd)
# bikes
resultsc_sd = as.data.frame(dplyr::summarize(
  resultsc_gr,
  sd_season     = sd(as.numeric(as.character(season)), na.rm = TRUE),
  sd_yr         = sd(as.numeric(as.character(yr)), na.rm = TRUE),
  sd_mnth       = sd(as.numeric(as.character(mnth)), na.rm = TRUE),
  sd_holiday    = sd(as.numeric(as.character(holiday)), na.rm = TRUE),
  sd_weekday    = sd(as.numeric(as.character(weekday)), na.rm = TRUE),
  sd_workingday = sd(as.numeric(as.character(workingday)), na.rm = TRUE),
  sd_weathersit = sd(as.numeric(as.character(weathersit)), na.rm = TRUE),
  sd_temp       = sd(as.numeric(as.character(temp)), na.rm = TRUE),
  sd_atemp      = sd(as.numeric(as.character(atemp)), na.rm = TRUE),
  sd_hum        = sd(as.numeric(as.character(hum)), na.rm = TRUE),
  sd_windspeed  = sd(as.numeric(as.character(windspeed)), na.rm = TRUE)
))
# the following shows that the standard deviation is the same across (almost) all features
# thus standardizing would do more harm than good
#summary(resultsc_sd)

results_gr = dplyr::group_by(
  results_sd,
  n_features,
  type
)

# bikes
resultsc_gr = dplyr::group_by(
  resultsc_sd,
  n_features,
  type
)

# boston
results_sd = as.data.frame(dplyr::summarize(
  results_gr,
  sd_crim  = mean(sd_crim, na.rm = TRUE),
  sd_zn    = mean(sd_zn, na.rm = TRUE),
  sd_indus = mean(sd_indus, na.rm = TRUE),
  sd_nox   = mean(sd_nox, na.rm = TRUE),
  sd_rm    = mean(sd_rm, na.rm = TRUE),
  sd_age   = mean(sd_age, na.rm = TRUE),
  sd_dis   = mean(sd_dis, na.rm = TRUE),
  sd_rad   = mean(sd_rad, na.rm = TRUE),
  sd_tax   = mean(sd_tax, na.rm = TRUE),
  sd_ptratio = mean(sd_ptratio, na.rm = TRUE),
  sd_b     = mean(sd_b, na.rm = TRUE),
  sd_lstat = mean(sd_lstat, na.rm = TRUE)#,
))
# the following shows that the standard deviation is the same across all features
# thus standardizing would do more harm than good
#summary(results_sd)
# bikes
resultsc_sd = as.data.frame(dplyr::summarize(
  resultsc_gr,
  sd_season     = mean(as.numeric(as.character(sd_season)), na.rm = TRUE),
  sd_yr         = mean(as.numeric(as.character(sd_yr)), na.rm = TRUE),
  sd_mnth       = mean(as.numeric(as.character(sd_mnth)), na.rm = TRUE),
  sd_holiday    = mean(as.numeric(as.character(sd_holiday)), na.rm = TRUE),
  sd_weekday    = mean(as.numeric(as.character(sd_weekday)), na.rm = TRUE),
  sd_workingday = mean(as.numeric(as.character(sd_workingday)), na.rm = TRUE),
  sd_weathersit = mean(as.numeric(as.character(sd_weathersit)), na.rm = TRUE),
  sd_temp       = mean(as.numeric(as.character(sd_temp)), na.rm = TRUE),
  sd_atemp      = mean(as.numeric(as.character(sd_atemp)), na.rm = TRUE),
  sd_hum        = mean(as.numeric(as.character(sd_hum)), na.rm = TRUE),
  sd_windspeed  = mean(as.numeric(as.character(sd_windspeed)), na.rm = TRUE)
))

plot_data = data.frame(
  sd = apply(results_sd[-1:-2], MARGIN = 1, function(row) mean(row[!is.nan(row) & !is.na(row)])),
  n_features = results_sd$n_features,
  type = results_sd$type
)

plot = ggplot(plot_data, aes(y = sd, x = n_features, color = type)) +
  geom_line(size = 2L) +
  theme_minimal() +
  theme(
    text = element_text(size = 25L),
    axis.title.x = element_text(vjust = -4L),
    axis.text.x = element_text(angle = 45L, hjust = 1L),
    plot.margin = ggplot2::margin(20L, 20L, 30L, 20L),
    axis.ticks.x = element_blank()
  ) +
  ylab("Average Standard Deviation") +
  xlab("Amount selected features") +
  scale_x_continuous(
    breaks = 1:12L,
    labels = 1:12L
  ) +
  ylim(c(0, 0.3))

filename = paste0("images/sd_nfeat_presi2.png")
png(filename, width = 700L, height = 500L)
plot
dev.off()

# bikes
plot_datac = data.frame(
  sd = apply(resultsc_sd[-1:-2], MARGIN = 1, function(row) mean(row[!is.nan(row)], na.rm = TRUE)),
  n_features = as.numeric(as.character(resultsc_sd$n_features)),
  type = resultsc_sd$type
)

plotc = ggplot(plot_datac, aes(y = sd, x = n_features, color = type)) +
  geom_line(size = 2L) +
  theme_minimal() +
  theme(
    text = element_text(size = 25L),
    axis.title.x = element_text(vjust = -4L),
    axis.text.x = element_text(angle = 45L, hjust = 1L),
    plot.margin = ggplot2::margin(20L, 20L, 30L, 20L),
    axis.ticks.x = element_blank()
  ) +
  ylab("Average Standard Deviation") +
  xlab("Amount selected features") +
  scale_x_continuous(
    breaks = 1L:11L,
    labels = 1L:11L
  ) +
  ylim(c(0, 0.3))

filename = paste0("images/sd_nfeatc_presi2.png")
png(filename, width = 700L, height = 500L)
plotc
dev.off()


```


```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "LIME output", warning=FALSE}
#################################
# AMOUNT OF SAMPLES/PERMUTATIONS VS STABILITY
#################################

library(mlbench)
library(mlr)
#library(dplyr)
library(lime)
library(ggplot2)


permutation_growth = function(
  data,
  target,
  pts_to_predict,
  type,
  repeats = 10L,
  permutation_seq = c(2500L, 5000L, 10000L),
  dim_increment = 10L,
  bin_continuous = FALSE,
  use_density = FALSE
  ) {
  
  # dimension of feature space
  p_max = ncol(data) - 1L
  feature_names = names(data[names(data) != target])
  
  # create sequence of "n_feature" arguments
  n_feat_seq = seq(1L, p_max, by = dim_increment)
  n_feat_seq = rep(n_feat_seq, each = repeats)
  
  # move target variable to the end
  train_data           = data[names(data) != target]
  train_data[[target]] = data[[target]]
  
  # define task and learner based on data type
  if (type == "classif") {
    task = makeClassifTask(data = train_data, target = target)
    learner = makeLearner("classif.randomForest", ntree = 20L, predict.type = "prob")
    
  } else if (type == "regr") {
    task = makeRegrTask(data = train_data, target = target)
    learner = makeLearner("regr.randomForest", ntree = 20L)
    
  } else {
    stop("Wrong type, buddy")
  }
  
  black_box = train(learner, task)
  explainer = lime(train_data[1L:p_max], black_box, bin_continuous = bin_continuous, use_density = use_density)

  if (!bin_continuous && use_density) {
    lapply(1:length(explainer$feature_distribution), function(i) {
      explainer$feature_distribution[[i]]["mean"] <<- mean(explainer$feature_distribution[[i]]$x, na.rm = TRUE)
      explainer$feature_distribution[[i]]["sd"]   <<- sd(  explainer$feature_distribution[[i]]$x, na.rm = TRUE)
    })
  }
  
  # iterate over sequence of permutation amount
  outer_return = parallel::mclapply(
    mc.cores = 4,
    permutation_seq,
    function(n_permutations) {
      
      # iterate over sequence of "n_feature" arguments
      inner_return = lapply(
        n_feat_seq,
        function(n_features) {
          
          feat_return        = rep(NA, p_max)
          names(feat_return) = feature_names
          
          # iterate over all points for interpretation
          inner_inner = apply(
            pts_to_predict,
            MARGIN = 1,
            function(target_pt) {
              
              explanation = explain(
                as.data.frame(t(target_pt[1:p_max])),
                explainer,
                n_labels = 1L,
                n_features = p_max,
                n_permutations = n_permutations,
                dist_fun = "euclidian"
              )
              
              to_update = names(feat_return) %in% explanation$feature
              feat_return[to_update] = explanation$feature_weight
              names(target_pt) = paste0("data_", feature_names)
              
              c(
                n_features = n_features,
                n_permutations = n_permutations,
                target_pt,
                feat_return
              )
            }
          )
          # transform from matrix to dataframe
          as.data.frame(t(inner_inner))
        }
      )
      # output progress
      frac = which(n_permutations == permutation_seq) / length(permutation_seq)
      log = sprintf("%2.2f/1.00 done", frac)
      print(log)
      # transpose matrix and transform to dataframe
      data.table::rbindlist(inner_return)
    }
  )
  # concatenate dataframes
  data.table::rbindlist(outer_return)
}

data("BostonHousing", package = "mlbench")
# removing categorical feature
boston     = BostonHousing[, -4]
# normalizing standard deviation to make coefficients comparable
boston     = as.data.frame(lapply(boston, function(x) x/sd(x)))
set.seed(123)
pts_to_predict = boston[sample(1:nrow(boston), 10), -ncol(boston)]

# this may take a while
#results = permutation_growth(
#  boston, "medv", pts_to_predict,
#  type = "regr",
#  dim_increment = 1L,
#  repeats = 10,
#  permutation_seq = c(1000, 2000, 4000, 8000, 16000, 32000)
#)
#saveRDS(results, "LIME_experiment_results/permutation_growth_boston_repeats10_perm1kx2kx4kx8kx16kx32k")
results = readRDS("LIME_experiment_results/permutation_growth_boston_repeats10_perm1kx2kx4kx8kx16kx32k")
results$type = "kernel (boston)"

#resultsb = permutation_growth(
#  boston, "medv", pts_to_predict,
#  type = "regr",
#  dim_increment = 1L,
#  repeats = 10,
#  permutation_seq = c(16000, 32000, 1000, 2000, 4000, 8000),
#  bin_continuous = TRUE,
#  use_density = TRUE
#)
#saveRDS(resultsb, "LIME_experiment_results/permutation_growth_boston_repeats10_perm1kx2kx4kx8kx16kx32k_bins")
resultsb = readRDS("LIME_experiment_results/permutation_growth_boston_repeats10_perm1kx2kx4kx8kx16kx32k_bins")
resultsb$type = "bins (boston)"

#resultsn = permutation_growth(
#  boston, "medv", pts_to_predict,
#  type = "regr",
#  dim_increment = 1L,
#  repeats = 10,
#  permutation_seq = c(16000, 32000, 1000, 2000, 4000, 8000),
#  use_density = TRUE
#)
#saveRDS(resultsn, "permutation_growth_boston_repeats10_perm1kx2kx4kx8kx16kx32k_nd")
resultsn = readRDS("LIME_experiment_results/permutation_growth_boston_repeats10_perm1kx2kx4kx8kx16kx32k_nd")
resultsn$type = "normal (boston)"

### categorical data
bikes = read.csv("datasets/day.csv")
# remove undesired variables
bikes = bikes[-which(names(bikes) %in% c("casual", "registered", "instant", "dteday"))]

# quantile binning with 4 bins
bikes[c("temp", "atemp", "hum", "windspeed")] = lapply(
  bikes[c("temp", "atemp", "hum", "windspeed")],
  function(vec) {
    quantiles = quantile(vec)
    quantiles[5] = Inf
    sapply(vec, function(x) sum(x >= quantiles))
  }
)
# normalize
bikes$cnt = bikes$cnt / sd(bikes$cnt)

bikes[-ncol(bikes)] = lapply(bikes[-ncol(bikes)], as.factor)
set.seed(123)
pts_to_predict = bikes[sample(10, 1:nrow(bikes)), -ncol(bikes)]

#resultsc = permutation_growth(
#  bikes, "cnt", pts_to_predict,
#  type = "regr",
#  dim_increment = 1L,
#  repeats = 10,
#  permutation_seq = c(16000, 32000, 1000, 2000, 4000, 8000),
#  bin_continuous = TRUE,
#  use_density = TRUE
#)
#saveRDS(resultsc, "permutation_growth_bikes_repeats10_perm1kx2kx4kx8kx16kx32k")
resultsc = readRDS("LIME_experiment_results/permutation_growth_bikes_repeats10_perm1kx2kx4kx8kx16kx32k")
resultsc$type = "categorical (bikes)"

results = rbind(results, resultsb, resultsn)
# boston
results_gr = dplyr::group_by(
  results,
  data_crim,
  data_zn,
  data_indus,
  data_nox,
  data_rm,
  data_age,
  data_dis,
  data_rad,
  data_tax,
  data_ptratio,
  data_b,
  data_lstat,
  n_permutations,
  type
)

# bikes
resultsc_gr = dplyr::group_by(
  resultsc,
  data_season,
  data_yr,
  data_mnth,
  data_holiday,
  data_weekday,
  data_workingday,
  data_weathersit,
  data_temp,
  data_atemp,
  data_hum,
  data_windspeed,
  n_permutations,
  type
)

# boston
results_sd = as.data.frame(dplyr::summarize(
  results_gr,
  sd_crim  = sd(crim, na.rm = TRUE),
  sd_zn    = sd(zn, na.rm = TRUE),
  sd_indus = sd(indus, na.rm = TRUE),
  sd_nox   = sd(nox, na.rm = TRUE),
  sd_rm    = sd(rm, na.rm = TRUE),
  sd_age   = sd(age, na.rm = TRUE),
  sd_dis   = sd(dis, na.rm = TRUE),
  sd_rad   = sd(rad, na.rm = TRUE),
  sd_tax   = sd(tax, na.rm = TRUE),
  sd_ptratio = sd(ptratio, na.rm = TRUE),
  sd_b     = sd(b, na.rm = TRUE),
  sd_lstat = sd(lstat, na.rm = TRUE)#,
))
# the following shows that the standard deviation is the same across all features
# thus standardizing would do more harm than good
#summary(results_sd)
# bikes
resultsc_sd = as.data.frame(dplyr::summarize(
  resultsc_gr,
  sd_season     = sd(as.numeric(as.character(season)), na.rm = TRUE),
  sd_yr         = sd(as.numeric(as.character(yr)), na.rm = TRUE),
  sd_mnth       = sd(as.numeric(as.character(mnth)), na.rm = TRUE),
  sd_holiday    = sd(as.numeric(as.character(holiday)), na.rm = TRUE),
  sd_weekday    = sd(as.numeric(as.character(weekday)), na.rm = TRUE),
  sd_workingday = sd(as.numeric(as.character(workingday)), na.rm = TRUE),
  sd_weathersit = sd(as.numeric(as.character(weathersit)), na.rm = TRUE),
  sd_temp       = sd(as.numeric(as.character(temp)), na.rm = TRUE),
  sd_atemp      = sd(as.numeric(as.character(atemp)), na.rm = TRUE),
  sd_hum        = sd(as.numeric(as.character(hum)), na.rm = TRUE),
  sd_windspeed  = sd(as.numeric(as.character(windspeed)), na.rm = TRUE)
))
# the following shows that the standard deviation is the same across (almost) all features
# thus standardizing would do more harm than good
#summary(resultsc_sd)

results_gr = dplyr::group_by(
  results_sd,
  n_permutations,
  type
)

# bikes
resultsc_gr = dplyr::group_by(
  resultsc_sd,
  n_permutations,
  type
)

# boston
results_sd = as.data.frame(dplyr::summarize(
  results_gr,
  sd_crim  = mean(sd_crim, na.rm = TRUE),
  sd_zn    = mean(sd_zn, na.rm = TRUE),
  sd_indus = mean(sd_indus, na.rm = TRUE),
  sd_nox   = mean(sd_nox, na.rm = TRUE),
  sd_rm    = mean(sd_rm, na.rm = TRUE),
  sd_age   = mean(sd_age, na.rm = TRUE),
  sd_dis   = mean(sd_dis, na.rm = TRUE),
  sd_rad   = mean(sd_rad, na.rm = TRUE),
  sd_tax   = mean(sd_tax, na.rm = TRUE),
  sd_ptratio = mean(sd_ptratio, na.rm = TRUE),
  sd_b     = mean(sd_b, na.rm = TRUE),
  sd_lstat = mean(sd_lstat, na.rm = TRUE)#,
))
# the following shows that the standard deviation is the same across all features
# thus standardizing would do more harm than good
#summary(results_sd)
# bikes
resultsc_sd = as.data.frame(dplyr::summarize(
  resultsc_gr,
  sd_season     = mean(as.numeric(as.character(sd_season)), na.rm = TRUE),
  sd_yr         = mean(as.numeric(as.character(sd_yr)), na.rm = TRUE),
  sd_mnth       = mean(as.numeric(as.character(sd_mnth)), na.rm = TRUE),
  sd_holiday    = mean(as.numeric(as.character(sd_holiday)), na.rm = TRUE),
  sd_weekday    = mean(as.numeric(as.character(sd_weekday)), na.rm = TRUE),
  sd_workingday = mean(as.numeric(as.character(sd_workingday)), na.rm = TRUE),
  sd_weathersit = mean(as.numeric(as.character(sd_weathersit)), na.rm = TRUE),
  sd_temp       = mean(as.numeric(as.character(sd_temp)), na.rm = TRUE),
  sd_atemp      = mean(as.numeric(as.character(sd_atemp)), na.rm = TRUE),
  sd_hum        = mean(as.numeric(as.character(sd_hum)), na.rm = TRUE),
  sd_windspeed  = mean(as.numeric(as.character(sd_windspeed)), na.rm = TRUE)
))

# the following shows that the standard deviation is the same across (almost) all features
# thus standardizing would do more harm than good
#summary(resultsc_sd)

plot_data = data.frame(sd = c(
  apply(results_sd[-1:-2], MARGIN = 1, function(row) mean(row[!is.nan(row)])),
  apply(resultsc_sd[-1:-2], MARGIN = 1, function(row) mean(row[!is.nan(row)]))
))
plot_data$n_permutations = c(results_sd$n_permutations, as.numeric(as.character(resultsc_sd$n_permutations)))
plot_data$type = c(results_sd$type, resultsc_sd$type)

plot = ggplot(plot_data, aes(y = sd, x = n_permutations, color = type)) +
  geom_line(size = 2L) +
  geom_vline(xintercept = 5000L) +
  theme_minimal() +
  theme(
    text = element_text(size = 25L),
    axis.title.x = element_text(vjust = -4L),
    axis.text.x = element_text(angle = 45L, hjust = 1L),
    plot.margin = ggplot2::margin(20L, 20L, 30L, 20L)
  ) +
  ylab("Average Standard Deviation") +
  xlab("Sample size")

filename = paste0("images/sd_npermutations_presi2.png")
png(filename, width = 1000L, height = 500L)
plot
dev.off()

```


```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "Random forest with 'node size = 1' and 'trees = 1'", warning=FALSE}

### EVALUATE THIS CELL TO MAKE FUNCTIONS BELOW WORK

library(mlbench)
library(mlr)

data(BostonHousing)
set.seed(5)
boston = BostonHousing[sample(nrow(BostonHousing), 20), ]

# create task and leaner
btask      = makeRegrTask(data = boston[c("lstat", "medv")], target = "medv")
regr_model = makeLearner("regr.xgboost", nrounds = 50)#, min.node.size = 1, num.trees = 1)#makeLearner("regr.ranger", min.node.size = 1, num.trees = 1)
# run model and get prediction surface
black_box  = train(regr_model, btask)
x_grid     = 1:4000 / 100
y_pred     = predict(black_box, newdata = data.frame(lstat=x_grid))

# visualize results
plot = ggplot() +
  geom_line(
    data = data.frame(x_grid, y_pred=y_pred$data$response),
    aes(x = x_grid, y = y_pred),
    color = "#00C5CD",
    size  = 2L
  ) +
  geom_point(data = boston, aes(y = medv, x = lstat), size = 3L) +
  ylim(c(0, 50)) +
  theme_minimal() +
  theme(
    text = element_text(size = 25L),
    axis.title.x = element_text(vjust = -4),
    plot.margin = ggplot2::margin(20,20,30,20)
  ) +
  ylab("target (medv)") +
  xlab("feature (lstat)")

filename = paste0("images/boston_sampled_tree_presi.png")
png(filename, width = 700L, height = 500L)
plot
dev.off()
```


```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "LIME output", warning=FALSE}

# EVALUATE ABOVE CELL FIRST
plot_lime_boston = function(model_smoothness = 50, sample_seed, kernel_width = 25, sample_size = 10) {
  
  # create grid
  x_grid     = 1:4000 / 100
  y_grid     = predict(black_box, newdata = data.frame(lstat=x_grid))

  set.seed(1)
  # randomly pick data point to explain
  x_ex = runif(1, 1, 40)
  yret = predict(black_box, newdata = data.frame(lstat = x_ex))
  y_ex = yret$data$response
  
  set.seed(sample_seed)
  # sample new data points
  x_samp = runif(sample_size, 1, 40)
  y_samp = predict(black_box, newdata = data.frame(lstat = x_samp))
  data   = data.frame(x = x_samp, y = y_samp$data$response)
  
  # apply gaussian kernel to receive weights
  weights = exp( - (x_samp - x_ex)^2 / kernel_width )
  
  # fit surrogate model and get predictions
  model  = lm(y ~ x, data = data, weights = weights)
  y_pred = predict(model, newdata = data.frame(x = x_grid))
  
  # visualize everything
  ggplot(data = NULL, aes(x = x_grid, y = y_grid$data$response)) +
    geom_line(color = "#00C5CD", size = 2) +
    geom_point(data = data, aes(x = x, y = y), size = 3) +
    geom_line( data = NULL, aes(x = x_grid, y = y_pred), color = "#e04d2e", size = 2) +
    geom_point(data = NULL, aes(x = x_ex,   y = y_ex  ), color = "#c1c10d", size = 5) +
    geom_vline(aes(xintercept = x_ex - sqrt(kernel_width))) +
    geom_vline(aes(xintercept = x_ex + sqrt(kernel_width))) +
    theme_minimal() +
    theme(
      text = element_text(size = 25),
      axis.title.x = element_text(vjust = -4),
      plot.margin = ggplot2::margin(20,20,30,20)
    ) +
    ylim(c(0, 50)) +
    ylab("target") +
    xlab("feature")

}

```

```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "LIME output", warning=FALSE}

filename = paste0("images/boston_sampled_tree_1_presi.png")
png(filename, width = 700L, height = 500L)
plot_lime_boston(sample_seed = 1, kernel_width = 1)
dev.off()
  
```


```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "LIME output", warning=FALSE}

filename = paste0("images/boston_sampled_tree_snd.png")
png(filename, width = 700L, height = 500L)
plot_lime_boston(sample_seed = 5, kernel_width = 1)
dev.off()
  
```


```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "LIME output", warning=FALSE}
#################################
# BLACK BOX COMPLEXITY/SMOOTHNESS VS STABILITY
#################################

library(mlbench)
library(mlr)
#library(dplyr)
library(lime)
library(ggplot2)


complexity_growth = function(
  data,
  target,
  pts_to_predict,
  type,
  repeats = 10L,
  max_degree = 10L,
  bin_continuous = FALSE,
  use_density = FALSE
) {
  

  # dimension of feature space
  p_max = ncol(data) - 1L
  feature_names = names(data[names(data) != target])
  
  # move target variable to the end
  train_data           = data[names(data) != target]
  train_data[[target]] = data[[target]]

  # set exponential size for nrounds hyperparameter
  degree_seq = rep(1:max_degree, each = repeats)
  # iterate over sequence of exponential degrees
  outer_return = parallel::mclapply(
    mc.cores = 4,
    degree_seq,
    function(degree) {

      # make model smoother with each iteration
      num.trees = 1L + (degree - 1L) * 10L
      # less overfitting
      min.node.size = degree
      
      # more overfitting
      nrounds = 2^degree
      
      # define task and learner based on data type
      if (type == "classif") {
        task = makeClassifTask(data = train_data, target = target)
        learner = makeLearner("classif.ranger", num.trees = num.trees, min.node.size = min.node.size, predict.type = "prob")
        #learner = makeLearner("classif.xgboost", nrounds = nrounds, predict.type = "prob")
        
      } else if (type == "regr") {
        task = makeRegrTask(data = train_data, target = target)
        #learner = makeLearner("regr.xgboost", nrounds = nrounds)
        learner = makeLearner("regr.ranger", num.trees = num.trees, min.node.size = min.node.size)
        
      } else {
        stop("Wrong type, buddy")
      }
      
      black_box = train(learner, task)
      explainer = lime(train_data[1L:p_max], black_box, bin_continuous = bin_continuous, use_density = use_density)
    
      if (!bin_continuous && use_density) {
        lapply(1:length(explainer$feature_distribution), function(i) {
          explainer$feature_distribution[[i]]["mean"] <<- mean(explainer$feature_distribution[[i]]$x, na.rm = TRUE)
          explainer$feature_distribution[[i]]["sd"]   <<- sd(  explainer$feature_distribution[[i]]$x, na.rm = TRUE)
        })
      }
      
      inner_return = apply(
        pts_to_predict,
        MARGIN = 1,
        function(target_pt) {
          
          feat_return        = rep(NA, p_max)
          names(feat_return) = feature_names
          
          explanation = explain(
            as.data.frame(t(target_pt[1:p_max])),
            explainer,
            n_labels = 1L,
            n_features = p_max,
            dist_fun = "euclidian"
          )
          
          to_update = names(feat_return) %in% explanation$feature
          feat_return[to_update] = explanation$feature_weight
          names(target_pt) = paste0("data_", feature_names)
          
          c(
            smoothness = degree,
            target_pt,
            feat_return
          )
        }
      )
      # output progress
      frac = degree / max_degree
      log = sprintf("%2.2f/1.00 done", frac)
      print(log)
      # transpose matrix and transform to dataframe
      as.data.frame(t(inner_return))
    }
  )
  # concatenate dataframes
  data.table::rbindlist(outer_return)
}

data("BostonHousing", package = "mlbench")
# removing categorical feature
boston     = BostonHousing[, -4]
# normalizing standard deviation to make coefficients comparable
boston     = as.data.frame(lapply(boston, function(x) x/sd(x)))
set.seed(123)
pts_to_predict = boston[sample(1:nrow(boston), 10), -ncol(boston)]

# this may take a while
#results = complexity_growth(boston, "medv", pts_to_predict, type = "regr", repeats = 10L)
#saveRDS(results, "LIME_experiment_results/complexity_growth_boston_repeats10_xgboost10")
results = readRDS("LIME_experiment_results/complexity_growth_boston_repeats10_xgboost10")
results$type = "kernel"

# this may take a while
#resultsb = complexity_growth(boston, "medv", pts_to_predict, type = "regr", repeats = 10L, bin_continuous = TRUE, use_density = TRUE)
#saveRDS(resultsb, "LIME_experiment_results/complexity_growth_boston_repeats10_xgboost10_bins")
resultsb = readRDS("LIME_experiment_results/complexity_growth_boston_repeats10_xgboost10_bins")
resultsb$type = "bins"

# this may take a while
#resultsn = complexity_growth(boston, "medv", pts_to_predict, type = "regr", repeats = 10L, use_density = TRUE)
#saveRDS(resultsn, "LIME_experiment_results/complexity_growth_boston_repeats10_xgboost10_nd")
resultsn = readRDS("LIME_experiment_results/complexity_growth_boston_repeats10_xgboost10_nd")
resultsn$type = "normal"

### categorical data
bikes = read.csv("datasets/day.csv")
# remove undesired variables
bikes = bikes[-which(names(bikes) %in% c("casual", "registered", "instant", "dteday"))]

# quantile binning with 4 bins
bikes[c("temp", "atemp", "hum", "windspeed")] = lapply(
  bikes[c("temp", "atemp", "hum", "windspeed")],
  function(vec) {
    quantiles = quantile(vec)
    quantiles[5] = Inf
    sapply(vec, function(x) sum(x >= quantiles))
  }
)
# normalize
bikes$cnt = bikes$cnt / sd(bikes$cnt)

bikes[-ncol(bikes)] = lapply(bikes[-ncol(bikes)], as.factor)
# dummy encoding
bikes = createDummyFeatures(
  bikes, target = "cnt",
  cols = names(bikes[-ncol(bikes)])
)
set.seed(123)
pts_to_predict = bikes[sample(10, 1:nrow(bikes)), -1] # "cnt" switched position

#resultsc = complexity_growth(bikes, "cnt", pts_to_predict, type = "regr", repeats = 10L, bin_continuous = TRUE, use_density = TRUE)
#saveRDS(resultsc, "LIME_experiment_results/complexity_growth_bikes_xgboost10")
resultsc = readRDS("LIME_experiment_results/complexity_growth_bikes_xgboost10")
resultsc$type = "categorical"

# merge results
results = rbind(results, resultsb, resultsn)

# bikes...
resultsc$season = resultsc$season.1 + resultsc$season.2 + resultsc$season.3 + resultsc$season.4
resultsc$yr = resultsc$yr.0 + resultsc$yr.1
resultsc$mnth = eval(parse(text = paste0("resultsc$mnth.", 1:12, collapse = " + ")))
resultsc$holiday = eval(parse(text = paste0("resultsc$holiday.", 0:1, collapse = " + ")))
resultsc$weekday = eval(parse(text = paste0("resultsc$weekday.", 0:6, collapse = " + ")))
resultsc$workingday = eval(parse(text = paste0("resultsc$workingday.", 0:1, collapse = " + ")))
resultsc$weathersit = eval(parse(text = paste0("resultsc$weathersit.", 1:3, collapse = " + ")))
resultsc$temp = eval(parse(text = paste0("resultsc$temp.", 1:4, collapse = " + ")))
resultsc$atemp = eval(parse(text = paste0("resultsc$atemp.", 1:4, collapse = " + ")))
resultsc$hum = eval(parse(text = paste0("resultsc$hum.", 1:4, collapse = " + ")))
resultsc$windspeed = eval(parse(text = paste0("resultsc$windspeed.", 1:4, collapse = " + ")))

resultsc$data_season = resultsc$season.1 + resultsc$season.2*2 + resultsc$season.3*3 + resultsc$season.4*4
resultsc$data_yr = resultsc$yr.0 + resultsc$yr.1 * 2
resultsc$data_mnth = eval(parse(text = paste0("resultsc$data_mnth.", 1:12, "*", 1:12, collapse = " + ")))
resultsc$data_holiday = eval(parse(text = paste0("resultsc$data_holiday.", 0:1, "*", 1:2, collapse = " + ")))
resultsc$data_weekday = eval(parse(text = paste0("resultsc$data_weekday.", 0:6, "*", 1:7, collapse = " + ")))
resultsc$data_workingday = eval(parse(text = paste0("resultsc$data_workingday.", 0:1, "*", 1:2, collapse = " + ")))
resultsc$data_weathersit = eval(parse(text = paste0("resultsc$data_weathersit.", 1:3, "*", 1:3, collapse = " + ")))
resultsc$data_temp = eval(parse(text = paste0("resultsc$data_temp.", 1:4, "*", 1:4, collapse = " + ")))
resultsc$data_atemp = eval(parse(text = paste0("resultsc$data_atemp.", 1:4, "*", 1:4, collapse = " + ")))
resultsc$data_hum = eval(parse(text = paste0("resultsc$data_hum.", 1:4, "*", 1:4, collapse = " + ")))
resultsc$data_windspeed = eval(parse(text = paste0("resultsc$data_windspeed.", 1:4, "*", 1:4, collapse = " + ")))


# boston
results_gr = dplyr::group_by(
  results,
  data_crim,
  data_zn,
  data_indus,
  data_nox,
  data_rm,
  data_age,
  data_dis,
  data_rad,
  data_tax,
  data_ptratio,
  data_b,
  data_lstat,
  smoothness,
  type
)

# bikes
resultsc_gr = dplyr::group_by(
  resultsc,
  data_season,
  data_yr,
  data_mnth,
  data_holiday,
  data_weekday,
  data_workingday,
  data_weathersit,
  data_temp,
  data_atemp,
  data_hum,
  data_windspeed,
  smoothness,
  type
)

# boston
results_sd = as.data.frame(dplyr::summarize(
  results_gr,
  sd_crim  = sd(crim, na.rm = TRUE),
  sd_zn    = sd(zn, na.rm = TRUE),
  sd_indus = sd(indus, na.rm = TRUE),
  sd_nox   = sd(nox, na.rm = TRUE),
  sd_rm    = sd(rm, na.rm = TRUE),
  sd_age   = sd(age, na.rm = TRUE),
  sd_dis   = sd(dis, na.rm = TRUE),
  sd_rad   = sd(rad, na.rm = TRUE),
  sd_tax   = sd(tax, na.rm = TRUE),
  sd_ptratio = sd(ptratio, na.rm = TRUE),
  sd_b     = sd(b, na.rm = TRUE),
  sd_lstat = sd(lstat, na.rm = TRUE)#,
))
# the following shows that the standard deviation is the same across all features
# thus standardizing would do more harm than good
#summary(results_sd)
# bikes
resultsc_sd = as.data.frame(dplyr::summarize(
  resultsc_gr,
  sd_season     = sd(as.numeric(as.character(season)), na.rm = TRUE),
  sd_yr         = sd(as.numeric(as.character(yr)), na.rm = TRUE),
  sd_mnth       = sd(as.numeric(as.character(mnth)), na.rm = TRUE),
  sd_holiday    = sd(as.numeric(as.character(holiday)), na.rm = TRUE),
  sd_weekday    = sd(as.numeric(as.character(weekday)), na.rm = TRUE),
  sd_workingday = sd(as.numeric(as.character(workingday)), na.rm = TRUE),
  sd_weathersit = sd(as.numeric(as.character(weathersit)), na.rm = TRUE),
  sd_temp       = sd(as.numeric(as.character(temp)), na.rm = TRUE),
  sd_atemp      = sd(as.numeric(as.character(atemp)), na.rm = TRUE),
  sd_hum        = sd(as.numeric(as.character(hum)), na.rm = TRUE),
  sd_windspeed  = sd(as.numeric(as.character(windspeed)), na.rm = TRUE)
))
# the following shows that the standard deviation is the same across (almost) all features
# thus standardizing would do more harm than good
#summary(resultsc_sd)

results_gr = dplyr::group_by(
  results_sd,
  smoothness,
  type
)

# bikes
resultsc_gr = dplyr::group_by(
  resultsc_sd,
  smoothness,
  type
)

# boston
results_sd = as.data.frame(dplyr::summarize(
  results_gr,
  sd_crim  = mean(sd_crim, na.rm = TRUE),
  sd_zn    = mean(sd_zn, na.rm = TRUE),
  sd_indus = mean(sd_indus, na.rm = TRUE),
  sd_nox   = mean(sd_nox, na.rm = TRUE),
  sd_rm    = mean(sd_rm, na.rm = TRUE),
  sd_age   = mean(sd_age, na.rm = TRUE),
  sd_dis   = mean(sd_dis, na.rm = TRUE),
  sd_rad   = mean(sd_rad, na.rm = TRUE),
  sd_tax   = mean(sd_tax, na.rm = TRUE),
  sd_ptratio = mean(sd_ptratio, na.rm = TRUE),
  sd_b     = mean(sd_b, na.rm = TRUE),
  sd_lstat = mean(sd_lstat, na.rm = TRUE)#,
))
# the following shows that the standard deviation is the same across all features
# thus standardizing would do more harm than good
#summary(results_sd)
# bikes
resultsc_sd = as.data.frame(dplyr::summarize(
  resultsc_gr,
  sd_season     = mean(as.numeric(as.character(sd_season)), na.rm = TRUE),
  sd_yr         = mean(as.numeric(as.character(sd_yr)), na.rm = TRUE),
  sd_mnth       = mean(as.numeric(as.character(sd_mnth)), na.rm = TRUE),
  sd_holiday    = mean(as.numeric(as.character(sd_holiday)), na.rm = TRUE),
  sd_weekday    = mean(as.numeric(as.character(sd_weekday)), na.rm = TRUE),
  sd_workingday = mean(as.numeric(as.character(sd_workingday)), na.rm = TRUE),
  sd_weathersit = mean(as.numeric(as.character(sd_weathersit)), na.rm = TRUE),
  sd_temp       = mean(as.numeric(as.character(sd_temp)), na.rm = TRUE),
  sd_atemp      = mean(as.numeric(as.character(sd_atemp)), na.rm = TRUE),
  sd_hum        = mean(as.numeric(as.character(sd_hum)), na.rm = TRUE),
  sd_windspeed  = mean(as.numeric(as.character(sd_windspeed)), na.rm = TRUE)
))

# the following shows that the standard deviation is the same across (almost) all features
# thus standardizing would do more harm than good
#summary(resultsc_sd)

plot_data = data.frame(
  sd = c(
    apply(results_sd[-1:-2], MARGIN = 1, function(row) mean(row[!is.nan(row)]))#,
    #apply(resultsc_sd[-1:-2], MARGIN = 1, function(row) mean(row[!is.nan(row)]))
  ),
  smoothness = c(
    results_sd$smoothness#,
    #as.numeric(as.character(resultsc_sd$smoothness))
  ),
  type = c(results_sd$type)#, resultsc_sd$type)
)

############################
### calculate train error

# set exponential size for nrounds hyperparameter
degree_seq = rep(1:10)
# iterate over sequence of exponential degrees
errors_data = lapply(#parallel::mclapply(
  #mc.cores = 4,
  degree_seq,
  function(degree) {

    # more overfitting
    nrounds = 2^degree
    
    task = makeRegrTask(data = boston, target = "medv")
    learner = makeLearner("regr.xgboost", nrounds = nrounds)
      
    # let's assume 10 times 99,9% of the training data have almost equal train error as 100%
    black_box = train(learner, task)
    
    preds = predict(black_box, task)

    data.frame(
      smoothness = degree,
      mse = mean((preds$data$response - preds$data$truth)^2, na.rm = TRUE)
    )
  }
)
# concatenate dataframes
errors_data = data.table::rbindlist(errors_data)
      

plot = ggplot(plot_data, aes(y = sd, x = smoothness, color = type)) +
  geom_line(size = 2L) +
  geom_line(data = errors_data, aes(y = mse/max(mse) * 0.02, x = smoothness, color = NULL), size = 2L) +
  theme_minimal() +
  theme(
    text = element_text(size = 25L),
    axis.title.x = element_text(vjust = -4),
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.margin = ggplot2::margin(20,20,30,20)
  ) +
  ylab("Average Standard Deviation") +
  xlab("Amount of trees") +
  scale_x_continuous(
    breaks = 1:10,
    labels = 2^(1:10)
  )

filename = paste0("images/sd_overfitting_presi2.png")
png(filename, width = 1000L, height = 500L)
plot
dev.off()

```

```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "Average standard deviation"}
#results = readRDS("complexity_growth_boston_100repeats")
#results$type = "kernel (boston)"

#resultsb = readRDS("complexity_growth_boston_100repeats_bins")
#resultsb$type = "bins (boston)"

#resultsn = readRDS("complexity_growth_boston_100repeats_nd")
#resultsn$type = "normal (boston)"

### categorical data
#resultsc = readRDS("complexity_growth_bikes_100repeats")
#resultsc$type = "categorical (bikes)"

data("BostonHousing", package = "mlbench")
# removing categorical feature
boston     = BostonHousing[, -4]
# normalizing standard deviation to make coefficients comparable
boston     = as.data.frame(lapply(boston, function(x) x/sd(x)))
set.seed(123)
pts_to_predict = boston[sample(1:nrow(boston), 10), -ncol(boston)]

# this may take a while
results = readRDS("LIME_experiment_results/complexity_growth_boston_repeats10")
#complexity_growth(boston, "medv", pts_to_predict, type = "regr", repeats = 10L)
#saveRDS(results, "LIME_experiment_results/complexity_growth_boston_repeats10")
results$type = "kernel"

# this may take a while
resultsb = readRDS("LIME_experiment_results/complexity_growth_boston_repeats10_bins")
#complexity_growth(boston, "medv", pts_to_predict, type = "regr", repeats = 10L, bin_continuous = TRUE, use_density = TRUE)
#saveRDS(resultsb, "LIME_experiment_results/complexity_growth_boston_repeats10_bins")
resultsb$type = "bins"

# this may take a while
resultsn = readRDS("LIME_experiment_results/complexity_growth_boston_repeats10_nd")
#complexity_growth(boston, "medv", pts_to_predict, type = "regr", repeats = 10L, use_density = TRUE)
#saveRDS(resultsn, "LIME_experiment_results/complexity_growth_boston_repeats10_nd")
resultsn$type = "normal"

### categorical data
bikes = read.csv("datasets/day.csv")
# remove undesired variables
bikes = bikes[-which(names(bikes) %in% c("casual", "registered", "instant", "dteday"))]

# quantile binning with 4 bins
bikes[c("temp", "atemp", "hum", "windspeed")] = lapply(
  bikes[c("temp", "atemp", "hum", "windspeed")],
  function(vec) {
    quantiles = quantile(vec)
    quantiles[5] = Inf
    sapply(vec, function(x) sum(x >= quantiles))
  }
)
# normalize
bikes$cnt = bikes$cnt / sd(bikes$cnt)

bikes[-ncol(bikes)] = lapply(bikes[-ncol(bikes)], as.factor)

set.seed(123)
pts_to_predict = bikes[sample(1:nrow(bikes), 10), -ncol(bikes)]

#resultsc = complexity_growth(bikes, "cnt", pts_to_predict, type = "regr", repeats = 10L, bin_continuous = TRUE, use_density = TRUE)
#saveRDS(resultsc, "LIME_experiment_results/complexity_growth_bikes_repeats10")
resultsc = readRDS("LIME_experiment_results/complexity_growth_bikes_repeats10")
resultsc$type = "categorical"


# merge results
results = rbind(results, resultsb, resultsn)
# boston
results_gr = dplyr::group_by(
  results,
  data_crim,
  data_zn,
  data_indus,
  data_nox,
  data_rm,
  data_age,
  data_dis,
  data_rad,
  data_tax,
  data_ptratio,
  data_b,
  data_lstat,
  smoothness,
  type
)

# bikes
resultsc_gr = dplyr::group_by(
  resultsc,
  data_season,
  data_yr,
  data_mnth,
  data_holiday,
  data_weekday,
  data_workingday,
  data_weathersit,
  data_temp,
  data_atemp,
  data_hum,
  data_windspeed,
  smoothness,
  type
)

# boston
results_sd = as.data.frame(dplyr::summarize(
  results_gr,
  sd_crim  = sd(crim, na.rm = TRUE),
  sd_zn    = sd(zn, na.rm = TRUE),
  sd_indus = sd(indus, na.rm = TRUE),
  sd_nox   = sd(nox, na.rm = TRUE),
  sd_rm    = sd(rm, na.rm = TRUE),
  sd_age   = sd(age, na.rm = TRUE),
  sd_dis   = sd(dis, na.rm = TRUE),
  sd_rad   = sd(rad, na.rm = TRUE),
  sd_tax   = sd(tax, na.rm = TRUE),
  sd_ptratio = sd(ptratio, na.rm = TRUE),
  sd_b     = sd(b, na.rm = TRUE),
  sd_lstat = sd(lstat, na.rm = TRUE)#,
))
# the following shows that the standard deviation is the same across all features
# thus standardizing would do more harm than good
#summary(results_sd)
# bikes
resultsc_sd = as.data.frame(dplyr::summarize(
  resultsc_gr,
  sd_season     = sd(as.numeric(as.character(season)), na.rm = TRUE),
  sd_yr         = sd(as.numeric(as.character(yr)), na.rm = TRUE),
  sd_mnth       = sd(as.numeric(as.character(mnth)), na.rm = TRUE),
  sd_holiday    = sd(as.numeric(as.character(holiday)), na.rm = TRUE),
  sd_weekday    = sd(as.numeric(as.character(weekday)), na.rm = TRUE),
  sd_workingday = sd(as.numeric(as.character(workingday)), na.rm = TRUE),
  sd_weathersit = sd(as.numeric(as.character(weathersit)), na.rm = TRUE),
  sd_temp       = sd(as.numeric(as.character(temp)), na.rm = TRUE),
  sd_atemp      = sd(as.numeric(as.character(atemp)), na.rm = TRUE),
  sd_hum        = sd(as.numeric(as.character(hum)), na.rm = TRUE),
  sd_windspeed  = sd(as.numeric(as.character(windspeed)), na.rm = TRUE)
))
# the following shows that the standard deviation is the same across (almost) all features
# thus standardizing would do more harm than good
#summary(resultsc_sd)

results_gr = dplyr::group_by(
  results_sd,
  smoothness,
  type
)

# bikes
resultsc_gr = dplyr::group_by(
  resultsc_sd,
  smoothness,
  type
)

# boston
results_sd = as.data.frame(dplyr::summarize(
  results_gr,
  sd_crim  = mean(sd_crim, na.rm = TRUE),
  sd_zn    = mean(sd_zn, na.rm = TRUE),
  sd_indus = mean(sd_indus, na.rm = TRUE),
  sd_nox   = mean(sd_nox, na.rm = TRUE),
  sd_rm    = mean(sd_rm, na.rm = TRUE),
  sd_age   = mean(sd_age, na.rm = TRUE),
  sd_dis   = mean(sd_dis, na.rm = TRUE),
  sd_rad   = mean(sd_rad, na.rm = TRUE),
  sd_tax   = mean(sd_tax, na.rm = TRUE),
  sd_ptratio = mean(sd_ptratio, na.rm = TRUE),
  sd_b     = mean(sd_b, na.rm = TRUE),
  sd_lstat = mean(sd_lstat, na.rm = TRUE)#,
))
# the following shows that the standard deviation is the same across all features
# thus standardizing would do more harm than good
#summary(results_sd)
# bikes
resultsc_sd = as.data.frame(dplyr::summarize(
  resultsc_gr,
  sd_season     = mean(as.numeric(as.character(sd_season)), na.rm = TRUE),
  sd_yr         = mean(as.numeric(as.character(sd_yr)), na.rm = TRUE),
  sd_mnth       = mean(as.numeric(as.character(sd_mnth)), na.rm = TRUE),
  sd_holiday    = mean(as.numeric(as.character(sd_holiday)), na.rm = TRUE),
  sd_weekday    = mean(as.numeric(as.character(sd_weekday)), na.rm = TRUE),
  sd_workingday = mean(as.numeric(as.character(sd_workingday)), na.rm = TRUE),
  sd_weathersit = mean(as.numeric(as.character(sd_weathersit)), na.rm = TRUE),
  sd_temp       = mean(as.numeric(as.character(sd_temp)), na.rm = TRUE),
  sd_atemp      = mean(as.numeric(as.character(sd_atemp)), na.rm = TRUE),
  sd_hum        = mean(as.numeric(as.character(sd_hum)), na.rm = TRUE),
  sd_windspeed  = mean(as.numeric(as.character(sd_windspeed)), na.rm = TRUE)
))

plot_data = data.frame(
  sd = c(
    apply(results_sd[-1:-2], MARGIN = 1, function(row) mean(row[!is.nan(row)])),
    apply(resultsc_sd[-1:-2], MARGIN = 1, function(row) mean(row[!is.nan(row)]))
  ),
  smoothness = c(
    results_sd$smoothness,
    as.numeric(as.character(resultsc_sd$smoothness))
  ),
  type = c(results_sd$type, resultsc_sd$type)
)

plot = ggplot(plot_data, aes(y = sd, x = smoothness, color = type)) +
  geom_line(size = 2L) +
  theme_minimal() +
  theme(
    text = element_text(size = 25),
    axis.title.x = element_text(vjust = -4),
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.margin = ggplot2::margin(20,20,30,20)#,
    #axis.ticks.x = element_blank()
  ) +
  ylab("Average Standard Deviation") +
  xlab("Min. node size and tree amount [x10]") +
  scale_x_continuous(
    breaks = 1:10,
    labels = 1:10
  )

filename = paste0("images/sd_smoothness_presi2.png")
png(filename, width = 1000L, height = 500L)
plot
dev.off()

```


```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "LIME output", warning=FALSE}

library(ggplot2)
library(mlr)
library(lime)

bikes = read.csv("datasets/day.csv")
# remove undesired variables
bikes = bikes[-which(names(bikes) %in% c("casual", "registered", "instant", "dteday"))]

# quantile binning with 4 bins
bikes[c("temp", "atemp", "hum", "windspeed")] = lapply(
  bikes[c("temp", "atemp", "hum", "windspeed")],
  function(vec) {
    quantiles = quantile(vec)
    quantiles[5] = Inf
    sapply(vec, function(x) sum(x >= quantiles))
  }
)
# normalize
bikes$cnt = bikes$cnt / sd(bikes$cnt)
bikes[-ncol(bikes)] = lapply(bikes[-ncol(bikes)], as.factor)

plots = lapply(names(bikes[-ncol(bikes)]), function(feat) qplot(get(feat), cnt, data = bikes, ylab = "", xlab = feat, geom = "boxplot") + theme(
  axis.text.x = element_blank(),
  axis.text.y = element_blank(),
  axis.ticks = element_blank(),
  text = element_text(size = 20))
)

args = paste0("plots[[", 1:11, "]]", collapse = ", ")
grid_plot_func = paste0("gridExtra::grid.arrange(", args, ", nrow = 3)")

filename = paste0("images/bikes_prezi.png")
png(filename, width = 700L, height = 500L)
eval(parse(text = grid_plot_func))
dev.off()

btask      = makeRegrTask(data = bikes, target = "cnt")

# normal non-continuous model
regr_model = makeLearner("regr.ranger")
black_box  = train(regr_model, btask)
explainer  = lime(bikes[, -ncol(bikes)], black_box)

# pick majority of each feature as data point
data_point = as.data.frame(lapply(
  bikes[, -ncol(bikes)],
  function(vec) names(table(vec))[which.max(table(vec))]
))

set.seed(123)
# repeat explanation 100 times
limes = sapply(1:100, function(k) {
  interim = explain(data_point, explainer, n_features = ncol(bikes)-1)
  weights_l = interim$feature_weight
  names(weights_l) = interim$feature
  weights_l
})


means = apply(limes, MARGIN = 1, mean)
sds   = apply(limes, MARGIN = 1, sd)
ci_upper = apply(limes, MARGIN = 1, quantile, 0.975)
ci_lower = apply(limes, MARGIN = 1, quantile, 0.025)

filename = paste0("images/bikes_standard_presi.png")
png(filename, width = 700L, height = 500L)
presi_plot(means, lower = ci_lower, ci_upper, color1 = "#62b1e7", color2 = "#6274e7", angle = 45L, hjust = 1L, ylim = c(-1.0, 1.0))
dev.off()


filename = paste0("images/bikes_standard.png")
png(filename, width = 700L, height = 500L)
book_plot(means, sds, color1 = "#62b1e7", color2 = "#6274e7", angle = 45, hjust = 1)
dev.off()

means_scaled = means / means
sds_scaled   = sds / abs(means)

filename = paste0("images/bikes_scaled.png")
png(filename, width = 700L, height = 500L)
book_plot(means_scaled, sds_scaled, ylab = "Weight / weight", color1 = "#62b1e7", color2 = "#6274e7", angle = 45, hjust = 1)
dev.off()


# pick minority of each feature as data point
data_point = as.data.frame(lapply(
  bikes[, -ncol(bikes)],
  function(vec) names(table(vec))[which.min(table(vec))]
))

set.seed(123)
# set index of datapoint to use
limes = sapply(1:100, function(k) {
  interim = explain(data_point, explainer, n_features = ncol(bikes)-1)
  weights_l = interim$feature_weight
  names(weights_l) = interim$feature
  weights_l
})


means = apply(limes, MARGIN = 1, mean)
sds   = apply(limes, MARGIN = 1, sd)
ci_upper = apply(limes, MARGIN = 1, quantile, 0.975)
ci_lower = apply(limes, MARGIN = 1, quantile, 0.025)

filename = paste0("images/bikes_outlier_presi.png")
png(filename, width = 700L, height = 500L)
presi_plot(means, lower = ci_lower, ci_upper, color1 = "#62b1e7", color2 = "#6274e7", angle = 45L, hjust = 1L, ylim = c(-1.0, 1.0))
dev.off()


filename = paste0("images/bikes_outlier.png")
png(filename, width = 700L, height = 500L)
book_plot(means, sds, color1 = "#62b1e7", color2 = "#6274e7", angle = 45, hjust = 1)
dev.off()

means_scaled = means / means
sds_scaled   = sds / abs(means)

filename = paste0("images/bikes_outlier_scaled.png")
png(filename, width = 700L, height = 500L)
book_plot(means_scaled, sds_scaled, ylab = "Weight / weight", color1 = "#62b1e7", color2 = "#6274e7", angle = 45, hjust = 1)
dev.off()


# EASY MODEL
regr_model = makeLearner("regr.lm")
black_box  = train(regr_model, btask)
explainer  = lime(bikes[, -ncol(bikes)], black_box)

# pick majority of each feature as data point
data_point = as.data.frame(lapply(
  bikes[, -ncol(bikes)],
  function(vec) names(table(vec))[which.max(table(vec))]
))

set.seed(123)
# repeat explanation 100 times
limes = sapply(1:100, function(k) {
  interim = explain(data_point, explainer, n_features = ncol(bikes)-1)
  weights_l = interim$feature_weight
  names(weights_l) = interim$feature
  weights_l
})


means = apply(limes, MARGIN = 1, mean)
sds   = apply(limes, MARGIN = 1, sd)
ci_upper = apply(limes, MARGIN = 1, quantile, 0.975)
ci_lower = apply(limes, MARGIN = 1, quantile, 0.025)

filename = paste0("images/bikes_lm_presi.png")
png(filename, width = 700L, height = 500L)
presi_plot(means, lower = ci_lower, ci_upper, color1 = "#62b1e7", color2 = "#6274e7", angle = 45L, hjust = 1L, ylim = c(-0.6, 1.1))
dev.off()


filename = paste0("images/bikes_lm.png")
png(filename, width = 700L, height = 500L)
book_plot(means, sds, color1 = "#62b1e7", color2 = "#6274e7", angle = 45, hjust = 1)
dev.off()

means_scaled = means / means
sds_scaled   = sds / abs(means)

filename = paste0("images/bikes_lm_scaled.png")
png(filename, width = 700L, height = 500L)
book_plot(means_scaled, sds_scaled, ylab = "Weight / weight", color1 = "#62b1e7", color2 = "#6274e7", angle = 45, hjust = 1)
dev.off()


# pick minority of each feature as data point
data_point = as.data.frame(lapply(
  bikes[, -ncol(bikes)],
  function(vec) names(table(vec))[which.min(table(vec))]
))
# set index of datapoint to use
limes = sapply(1:100, function(k) {
  interim = explain(data_point, explainer, n_features = ncol(bikes)-1)
  weights_l = interim$feature_weight
  names(weights_l) = interim$feature
  weights_l
})


means = apply(limes, MARGIN = 1, mean)
sds   = apply(limes, MARGIN = 1, sd)


filename = paste0("images/bikes_lm_outlier.png")
png(filename, width = 700L, height = 500L)
book_plot(means, sds, color1 = "#62b1e7", color2 = "#6274e7", angle = 45, hjust = 1)
dev.off()

means_scaled = means / means
sds_scaled   = sds / abs(means)

filename = paste0("images/bikes_lm_outlier_scaled.png")
png(filename, width = 700L, height = 500L)
book_plot(means_scaled, sds_scaled, ylab = "Weight / weight", color1 = "#62b1e7", color2 = "#6274e7", angle = 45, hjust = 1)
dev.off()

####

# OVERFITTING MODEL
regr_model = makeLearner("regr.ranger", num.trees = 1, min.node.size = 1)
black_box  = train(regr_model, btask)
explainer  = lime(bikes[, -ncol(bikes)], black_box)

# pick majority of each feature as data point
data_point = as.data.frame(lapply(
  bikes[, -ncol(bikes)],
  function(vec) names(table(vec))[which.max(table(vec))]
))

set.seed(123)
# repeat explanation 100 times
limes = sapply(1:100, function(k) {
  interim = explain(data_point, explainer, n_features = ncol(bikes)-1)
  weights_l = interim$feature_weight
  names(weights_l) = interim$feature
  weights_l
})


means = apply(limes, MARGIN = 1, mean)
sds   = apply(limes, MARGIN = 1, sd)
ci_upper = apply(limes, MARGIN = 1, quantile, 0.975)
ci_lower = apply(limes, MARGIN = 1, quantile, 0.025)

filename = paste0("images/bikes_tree_presi.png")
png(filename, width = 700L, height = 500L)
presi_plot(means, lower = ci_lower, ci_upper, color1 = "#62b1e7", color2 = "#6274e7", angle = 45L, hjust = 1L, ylim = c(-0.6, 1.1))
dev.off()


filename = paste0("images/bikes_tree.png")
png(filename, width = 700L, height = 500L)
book_plot(means, sds, color1 = "#62b1e7", color2 = "#6274e7", angle = 45, hjust = 1)
dev.off()

means_scaled = means / means
sds_scaled   = sds / abs(means)

filename = paste0("images/bikes_tree_scaled.png")
png(filename, width = 700L, height = 500L)
book_plot(means_scaled, sds_scaled, ylab = "Weight / weight", color1 = "#62b1e7", color2 = "#6274e7", angle = 45, hjust = 1)
dev.off()


# pick minority of each feature as data point
data_point = as.data.frame(lapply(
  bikes[, -ncol(bikes)],
  function(vec) names(table(vec))[which.min(table(vec))]
))
# set index of datapoint to use
limes = sapply(1:100, function(k) {
  interim = explain(data_point, explainer, n_features = ncol(bikes)-1)
  weights_l = interim$feature_weight
  names(weights_l) = interim$feature
  weights_l
})


means = apply(limes, MARGIN = 1, mean)
sds   = apply(limes, MARGIN = 1, sd)


filename = paste0("images/bikes_tree_outlier.png")
png(filename, width = 700L, height = 500L)
book_plot(means, sds, color1 = "#62b1e7", color2 = "#6274e7", angle = 45, hjust = 1)
dev.off()

means_scaled = means / means
sds_scaled   = sds / abs(means)

filename = paste0("images/bikes_tree_outlier_scaled.png")
png(filename, width = 700L, height = 500L)
book_plot(means_scaled, sds_scaled, ylab = "Weight / weight", color1 = "#62b1e7", color2 = "#6274e7", angle = 45, hjust = 1)
dev.off()

```


```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "LIME output", warning=FALSE}

library(ggplot2)
library(mlr)
library(lime)

bikes = read.csv("datasets/day.csv")
# remove undesired variables
bikes = bikes[-which(names(bikes) %in% c("casual", "registered", "instant", "dteday"))]
bikes[c("temp", "atemp", "hum", "windspeed")] = as.data.frame(lapply(
  bikes[c("temp", "atemp", "hum", "windspeed")],
  function(x) x/sd(x)
))

# normalize
bikes$cnt = bikes$cnt / sd(bikes$cnt)

bikes_num = as.data.frame(bikes[c("temp", "atemp", "hum", "windspeed")])
bikes_num$cnt = bikes$cnt

b_numtask = makeRegrTask(data = bikes_num, target = "cnt")

# normal non-continuous model
regr_model = makeLearner("regr.ranger")
black_box  = train(regr_model, b_numtask)
explainer  = lime(bikes_num[, -ncol(bikes_num)], black_box)

# pick majority of each feature as data point
data_point = as.data.frame(lapply(
  bikes_num[, -ncol(bikes_num)],
  mean
))
# set index of datapoint to use
limes = sapply(1:100, function(k) {
  interim = explain(data_point, explainer, n_features = ncol(bikes_num)-1)
  weights_l = interim$feature_weight
  names(weights_l) = interim$feature
  weights_l
})


means = apply(limes, MARGIN = 1, mean)
sds   = apply(limes, MARGIN = 1, sd)


filename = paste0("images/bikes_bins_lime.png")
png(filename, width = 700L, height = 500L)
book_plot(means, sds, color1 = "#df8a4f", color2 = "#c64b00", angle = 45, hjust = 1)
dev.off()

# quantile binning with 4 bins
bikes_bins = as.data.frame(lapply(
  bikes[c("temp", "atemp", "hum", "windspeed")],
  function(vec) {
    quantiles = quantile(vec)
    quantiles[5] = Inf
    sapply(vec, function(x) sum(x >= quantiles))
  }
))

bikes_bins$cnt = bikes$cnt
bikes_bins[-ncol(bikes_bins)] = lapply(bikes_bins[-ncol(bikes_bins)], as.factor)


b_binstask = makeRegrTask(data = bikes_bins, target = "cnt")

# normal non-continuous model
regr_model = makeLearner("regr.ranger")
black_box  = train(regr_model, b_binstask)
explainer  = lime(bikes_bins[, -ncol(bikes_bins)], black_box)

# put mean point into bins
data_point = as.data.frame(lapply(
  names(data_point),
  function(name) {
    quantiles = quantile(bikes_num[[name]])
    quantiles[5] = Inf
    as.factor(sum(data_point[[name]] >= quantiles))
  }
))
names(data_point) = names(bikes_bins[-ncol(bikes_bins)])

# set index of datapoint to use
limes = sapply(1:100, function(k) {
  interim = explain(data_point, explainer, n_features = ncol(bikes_bins)-1)
  weights_l = interim$feature_weight
  names(weights_l) = interim$feature
  weights_l
})


means = apply(limes, MARGIN = 1, mean)
sds   = apply(limes, MARGIN = 1, sd)

filename = paste0("images/bikes_manual_bins.png")
png(filename, width = 700L, height = 500L)
book_plot(means, sds, color1 = "#df8a4f", color2 = "#c64b00", angle = 45, hjust = 1)
dev.off()

##########
### non bins, but kernel density estimation

# normal non-continuous model
regr_model = makeLearner("regr.ranger")
black_box  = train(regr_model, b_numtask)
explainer  = lime(bikes_num[, -ncol(bikes_num)], black_box, bin_continuous = FALSE, use_density = FALSE)

# pick majority of each feature as data point
data_point = as.data.frame(lapply(
  bikes_num[, -ncol(bikes_num)],
  mean
))
# set index of datapoint to use
limes = sapply(1:100, function(k) {
  interim = explain(data_point, explainer, n_features = ncol(bikes_num)-1, dist_fun = "euclidian")
  weights_l = interim$feature_weight
  names(weights_l) = interim$feature
  weights_l
})


means = apply(limes, MARGIN = 1, mean)
sds   = apply(limes, MARGIN = 1, sd)


filename = paste0("images/bikes_no_bins.png")
png(filename, width = 700L, height = 500L)
book_plot(means, sds, color1 = "#df8a4f", color2 = "#c64b00", angle = 45, hjust = 1)
dev.off()

```


```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "LIME output", warning=FALSE}

plot_better_lime = function(model_smoothness = 50, sample_seed, kernel_width = 900, sample_size = 10) {
  
  # create ground truth
  black_box = function(x) sin(x / model_smoothness)
  x = 1:1000
  y = black_box(x)

  set.seed(1)
  # randomly pick data point to explain
  x_ex = runif(1, 1, 1000)
  y_ex = black_box(x_ex)
  
  
  set.seed(sample_seed)
  # sample new data points
  x_samp = rnorm(sample_size, x_ex, sqrt(kernel_width))
  y_samp = black_box(x_samp)
  data   = data.frame(x = x_samp, y = y_samp)
  
  # fit surrogate model and get predictions
  model  = lm(y ~ x, data = data)
  y_pred = predict(model, newdata = data.frame(x = x))
  
  # visualize everything
  ggplot(data = NULL, aes(y = y, x = x)) +
    geom_line(color = "#00C5CD", size = 1.5) +
    geom_point(data = NULL, aes(x = x_samp, y = y_samp)) +
    geom_line(data = NULL, aes(x = x, y = y_pred), color = "#e04d2e", size = 1) +
    geom_point(data = NULL, aes(x = x_ex, y = y_ex), color = "#c1c10d", size = 3) +
    geom_vline(aes(xintercept = x_ex - sqrt(kernel_width))) +
    geom_vline(aes(xintercept = x_ex + sqrt(kernel_width))) +
    theme_minimal() +
    ylim(c(-1.5, 1.5)) +
    ylab("target") +
    xlab("feature")

}
  
```

```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "Local sampling offers more anchor points in the neighborhood and strongly increases sampling efficiency", warning=FALSE}


plot_better_lime(sample_seed = 2, model_smoothness = 50, sample_size = 10)
  
```

```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "A rerun of the same settings reveals high similarity of the samples", warning=FALSE}


plot_better_lime(sample_seed = 1, model_smoothness = 50, sample_size = 10)
  
```


```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "LIME output", warning=FALSE}

# EVALUATE ABOVE CELLS FIRST!
plot_better_lime_boston = function(model_smoothness = 50, sample_seed, kernel_width = 900, sample_size = 10) {
  
  # create grid
  x_grid     = 1:4000 / 100
  y_grid     = predict(black_box, newdata = data.frame(lstat=x_grid))

  set.seed(1)
  # randomly pick data point to explain
  x_ex = runif(1, 1, 40)
  yret = predict(black_box, newdata = data.frame(lstat = x_ex))
  y_ex = yret$data$response
  
  set.seed(sample_seed)
  # sample new data points
  x_samp = rnorm(sample_size, x_ex, sqrt(kernel_width))
  y_samp = predict(black_box, newdata = data.frame(lstat = x_samp))
  data   = data.frame(x = x_samp, y = y_samp$data$response)
  
  # fit surrogate model and get predictions
  model  = lm(y ~ x, data = data)
  y_pred = predict(model, newdata = data.frame(x = x_grid))
  
  # visualize everything
  ggplot(data = NULL, aes(x = x_grid, y = y_grid$data$response)) +
    geom_line(color = "#00C5CD", size = 1.5) +
    geom_point(data = data, aes(x = x, y = y)) +
    geom_line( data = NULL, aes(x = x_grid, y = y_pred), color = "#e04d2e", size = 1) +
    geom_point(data = NULL, aes(x = x_ex,   y = y_ex  ), color = "#c1c10d", size = 3) +
    geom_vline(aes(xintercept = x_ex - sqrt(kernel_width))) +
    geom_vline(aes(xintercept = x_ex + sqrt(kernel_width))) +
    theme_minimal() +
    theme(
      text = element_text(size = 15),
      axis.title.x = element_text(vjust = -4),
      plot.margin = ggplot2::margin(20,20,30,20)
    ) +
    ylim(c(0, 50)) +
    ylab("target") +
    xlab("feature")
  
}

```

```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "LIME output", warning=FALSE}

filename = paste0("images/boston_sampled_tree_fst_better.png")
png(filename, width = 700L, height = 500L)
plot_better_lime_boston(sample_seed = 1, kernel_width = 1)
dev.off()
  
```

```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "Applying new sampling strategy on overfitting decision tree"}
knitr::include_graphics("images/boston_sampled_tree_fst_better.png")
```

```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "LIME output", warning=FALSE}

filename = paste0("images/boston_sampled_tree_snd_better.png")
png(filename, width = 700L, height = 500L)
plot_better_lime_boston(sample_seed = 2, kernel_width = 1)
dev.off()
  
```

```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "Even though the surface is an extreme challenge, both runs capture the increasing tendency from left to right"}
knitr::include_graphics("images/boston_sampled_tree_snd_better.png")
```


```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "LIME output", warning=FALSE}

filename = paste0("images/boston_sampled_tree_snd_better_slim.png")
png(filename, width = 700L, height = 500L)
plot_better_lime_boston(sample_seed = 2, kernel_width = 0.01)
dev.off()
  
```

```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "Kernel width can be arbitrarily small without losing stability -- all samples are close to the explained data point and correctly capture the local plateau"}
knitr::include_graphics("images/boston_sampled_tree_snd_better_slim.png")
```

```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "LIME output", warning=FALSE}

filename = paste0("images/boston_sampled_tree_snd_slim.png")
png(filename, width = 700L, height = 500L)
plot_lime_boston(sample_seed = 2, kernel_width = 0.01)
dev.off()
  
```

```{r, eval = FALSE, echo = FALSE, fig.align = 'center', fig.cap = "Global sampling misses a lot of close anchor points to offer a good fit for smaller kernel widths -- leading to a catastrophic failure in this case"}
knitr::include_graphics("images/boston_sampled_tree_snd_slim.png")
```
