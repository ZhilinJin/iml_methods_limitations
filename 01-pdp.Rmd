# Method chapters
•	Each method chapter should shortly (2-3 pages) describe the method: mathematical definition, how to estimate it, an example and visualizations. Also a short section on the limitations which links to the individual chapters.
•	Team PDP describes the partial dependence plot for 1D and 2D. Closely related are individual conditional expectation curves which should be explained as well and can be used to motivated PDP.


# Partial Dependence Plots (PDP) and Individual Conditional Expectation Curves

## Partial Dependence Plots (PDP)

The Partial Dependence Plot (PDP) is a rather intuitive and easy-to-understand visualization of the features' impact on the predicted outcome. It maps the marginal effect of the selected variable(s) and can reveal the nature of dependence structure between target and individual feature variable[^1].

The underlying function can be described as follows:

Let $x_S$ be the set of features of interest for the PDP and $x_C$ the complement set which contains all other features.
While the general model function $f(x) = f(x_S, x_C)$ depends on all input variables, the partial dependence function marginalizes over the feature distribution in set C:[^2]

$$f_{x_S}(x_S) = \mathbb{E}_{x_C}[f(x_S, x_C)]$$


The partial dependence function can be estimated by averaging the actual feature values of $x_C$ in the training data at given values of $x_S$ or, in other words, it computes the marginal effect of $x_S$ on the prediction. In order to derive realistic results, a major assumption of the PDP is that the features in $x_S$ and $x_C$ are uncorrelated.[^2]

$$\hat{f}_{x_S}(x_S)=\frac{1}{n}\sum_{i=1}^{n}f(x_S, x^{(i)}_{C})$$

[PLOT: PDP numerical features, numerical outcome]
![PDP Class](PDP_Age.jpeg)
In classification problems with probability outputs, the partial dependence function is modeled separately for all of the K different classes, i.e. it shows the probability for each respective class at given feature values of $x_S$.[^2] For instance, in the plot below it is clear the passengers in 1st class had the highest probability of survival and the 3rd class passengers the lowest.

[PLOT: PDP numerical features, multiclass outcome]
![PDP Class](PDP_Pclass.jpeg)


**Advantages and Limitations of PD plots**

Partial Dependence Plots are easy to compute and a poular way explain insights from black box Machine Learning models. With their intuitive character, PDPs perfectly qualify for the communication to non-technical audience. However, due to limited visualization techniques and the restriction of human perception to a maximum of three dimensions, only one or two features can reasonably be displayed in one PDP.[^1]
 
[PLOT: PDP with two features]

![2 Feature PDP](2_feature_pdp.jpeg)

Drawing a PDP with one or two feature variables allows a straight-forward interpretation of the marginal effects. This holds true as long as the features are not correlated. Should this assumption be violated, the partial dependence function will produce unrealistic data points. Furthermore, opposite effects of heterogeneous subgroups might remain hidden through averaging the marginal effects, which could lead to wrong conclusions.[^1]

[PLOT: PDP with correlated features]


[^1]: Molnar. "Interpretable Machine Learning". https://christophm.github.io/interpretable-ml-book/simple.html.
[^2]: Hastie, Tibshirani, Friedman. The Elements of Statistical Learning. Springer. https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf.

# Partial Dependence Plots (PDP) and Individual Conditional Expectation Curves

## Partial Dependence Plots (PDP)
## Individual Conditional Expectation Curves
A formal definition:  In ICE plots, for each instance in ${(x^{(i)}_S, x^{(i)}_C)}^N_{i=1}$, the curve $\hat{f}_S^{(i)}}$ is plotted against $x^{(i)}_S$ and $x^{(i)}_C$.[^3]


Individual Conditional Expectation (ICE) plots display one line per instance that shows how the instance’s prediction changes when a feature changes.

The equivalent to a PDP for individual data instances is called individual conditional expectation (ICE) plot (Goldstein et al. 2017)[^4]. An ICE plot visualizes the dependence of the prediction on a feature for each instance separately, resulting in one line per instance, compared to one line overall in partial dependence plots. A PDP is the average of the lines of an ICE plot. The values for a line (and one instance) can be computed by keeping all other features the same, creating variants of this instance by replacing the feature’s value with values from a grid and making predictions with the black box model for these newly created instances. The result is a set of points for an instance with the feature value from the grid and the respective predictions.


##Centered ICE Plot

There is a problem with ICE plots: Sometimes it can be hard to tell whether the ICE curves differ between individuals because they start at different predictions. A simple solution is to center the curves at a certain point in the feature and display only the difference in the prediction to this point. The resulting plot is called centered ICE plot (c-ICE). Anchoring the curves at the lower end of the feature is a good choice. The new curves are defined as:
$$\hat{f}^{(i)}_{cent} = \hat{f}^{(i)} - \mathbb{1}\hat{f}(x^a,x^{(i)}_C)$$


## Derivative ICE Plot

## Advantages and Disadvantages
[^3]: Molnar. "Interpretable Machine Learning". https://christophm.github.io/interpretable-ml-book/simple.html.
[^4]: Goldstein, Alex, et al. “Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation.” Journal of Computational and Graphical Statistics 24.1 (2015): 44-65.



## ICE

![ICE](ice_plots.jpeg)