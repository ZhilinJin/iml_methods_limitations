\contentsline {figure}{\numberline {1}{\ignorespaces Creative Commons License\relax }}{ix}{figure.caption.4}% 
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces PDP for predicted survival probability and numeric feature variable 'Age'. The probability to survive sharply drops at a young age and more moderately afterwards. The rug on the x-axis illustrates the distribution of observed training data.\relax }}{4}{figure.caption.6}% 
\contentsline {figure}{\numberline {2.2}{\ignorespaces The PDP for survival probability and categorical feature ' passenger class' reveals that passengers in lower classes had a lower probability to survive than those in a higher class.\relax }}{5}{figure.caption.7}% 
\contentsline {figure}{\numberline {2.3}{\ignorespaces Two-dimensional PDP for predicted survival probability and numerical feature 'Age', together with the categorical feature 'Sex'. The PDP shows that while the survival probability for both genders declines as age increases, there is a difference between genders. It is clear that the decrease is much steeper for males.\relax }}{6}{figure.caption.8}% 
\contentsline {figure}{\numberline {2.4}{\ignorespaces Two-dimensional PDP for predicted survival probability and numerical features 'Age' and 'Fare'. The PDP illustrates that the survival probability of younger passengers is fairly uniform for varying fares, while adults travelling at a lower fare also had a much lower probability to survive compared to those that paid a high fare.\relax }}{6}{figure.caption.9}% 
\contentsline {figure}{\numberline {2.5}{\ignorespaces ICE plot of survival probability by Age. The yellow line represents the average of the individual lines and is thus equivalent to the respective PDP. The individual conditional relationships indicate that there might be underlying heterogeneity in the complement set.\relax }}{7}{figure.caption.10}% 
\contentsline {figure}{\numberline {2.6}{\ignorespaces Centered ICE plot of survival probability by Age. All lines are fixed to 0 at the minimum observed age of 0.42. The y-axis shows the survival probability difference to age 0.42. Centrered ICE plot shows that compared to age 0.42, the predictions for most passengers decrease as age increases. However, there are quite a few passengers with opposite predictions.\relax }}{8}{figure.caption.11}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Simulated data for independent (left) and strongly correlated (right) features $x_1$ and $x_2$. The marginal distribution of $x_2$ is displayed on the right side of each plot.\relax }}{12}{figure.caption.12}% 
\contentsline {figure}{\numberline {3.2}{\ignorespaces Manipulated simulated data for independent (left) and strongly correlated (right) features $x_1$ and $x_2$. Observations where both the value of $x_1$ and $x_2$ lies between 0 and 1.5 have been deleted to artificially produce an extrapolation problem. The marginal distribution of $x_2$, which is displayed on the right side of each plot, is obviously more affected in the correlated case.\relax }}{13}{figure.caption.13}% 
\contentsline {figure}{\numberline {3.3}{\ignorespaces Matrix of Pearson correlation coefficients between all numerical variables extracted from the bike-sharing dataset.\relax }}{15}{figure.caption.14}% 
\contentsline {figure}{\numberline {3.4}{\ignorespaces PDPs based on Linear Regression learner for 'temp' in model 3.1 (top left), 'atemp' in model 3.2 (top right), 'temp' in model in model 3.3 (bottom left) and 'atemp' in model 3.3 (bottom right).\relax }}{16}{figure.caption.15}% 
\contentsline {figure}{\numberline {3.5}{\ignorespaces PDPs based on Support Vector Machines learner for 'temp' in model 3.1 (top left), 'atemp' in model 3.2 (top right), 'temp' in model in model 3.3 (bottom left) and 'atemp' in model 3.3 (bottom right).\relax }}{17}{figure.caption.16}% 
\contentsline {figure}{\numberline {3.6}{\ignorespaces PDPs based on Random Forest learner for 'temp' in model 3.1 (top left), 'atemp' in model 3.2 (top right), 'temp' in model in model 3.3 (bottom left) and 'atemp' in model 3.3 (bottom right).\relax }}{18}{figure.caption.17}% 
\contentsline {figure}{\numberline {3.7}{\ignorespaces Matrix of corrected contingency coefficients between all categorical variables extracted from the bike-sharing dataset.\relax }}{19}{figure.caption.18}% 
\contentsline {figure}{\numberline {3.8}{\ignorespaces PDPs based on Linear Regression learner for 'season' in model 3.4 (top left), 'mnth' in model 3.5 (top right), 'season' in model in model 3.6 (bottom left) and 'mnth' in model 3.6 (bottom right).\relax }}{20}{figure.caption.19}% 
\contentsline {figure}{\numberline {3.9}{\ignorespaces PDPs based on Support Vector Machines learner for 'season' in model 3.4 (top left), 'mnth' in model 3.5 (top right), 'season' in model in model 3.6 (bottom left) and 'mnth' in model 3.6 (bottom right).\relax }}{21}{figure.caption.20}% 
\contentsline {figure}{\numberline {3.10}{\ignorespaces PDPs based on Random Forest learner for 'season' in model 3.4 (top left), 'mnth' in model 3.5 (top right), 'season' in model in model 3.6 (bottom left) and 'mnth' in model 3.6 (bottom right).\relax }}{22}{figure.caption.21}% 
\contentsline {figure}{\numberline {3.11}{\ignorespaces PDPs based on Linear Regression learner for 'season' in model 3.4 (top left), 'temp' in model 3.5 (top right), 'season' in model in model 3.6 (bottom left) and 'temp' in model 3.6 (bottom right).\relax }}{23}{figure.caption.22}% 
\contentsline {figure}{\numberline {3.12}{\ignorespaces PDPs based on Support Vector Machines learner for 'season' in model 3.4 (top left), 'temp' in model 3.5 (top right), 'season' in model in model 3.6 (bottom left) and 'temp' in model 3.6 (bottom right).\relax }}{24}{figure.caption.23}% 
\contentsline {figure}{\numberline {3.13}{\ignorespaces PDPs based on Random Forest learner for 'season' in model 3.4 (top left), 'temp' in model 3.5 (top right), 'season' in model in model 3.6 (bottom left) and 'temp' in model 3.6 (bottom right).\relax }}{25}{figure.caption.24}% 
\contentsline {figure}{\numberline {3.14}{\ignorespaces PDPs for features $x_1$, $x_2$ and $x_3$ (left to right) in Setting 1, based on multiple simulations with Linear Model as learning algorithm. Top row shows independent case, second row the correlated case and bottom row the dependent case. The red line represents the true effect of the respective feature on $y$, the blue dashed line is the true commmon effect of $x_1$ and $x_2$.\relax }}{26}{figure.caption.25}% 
\contentsline {figure}{\numberline {3.15}{\ignorespaces PDPs for features $x_1$, $x_2$ and $x_3$ (left to right) in Setting 1, based on multiple simulations with Random Forest as learning algorithm. Top row shows independent case, second row the correlated case and bottom row the dependent case. The red line represents the true effect of the respective feature on $y$, the blue dashed line is the true commmon effect of $x_1$ and $x_2$.\relax }}{27}{figure.caption.26}% 
\contentsline {figure}{\numberline {3.16}{\ignorespaces PDPs for features $x_1$, $x_2$ and $x_3$ (left to right) in Setting 1, based on multiple simulations with Support Vector Machines as learning algorithm. Top row shows independent case, second row the correlated case and bottom row the dependent case. The red line represents the true effect of the respective feature on $y$, the blue dashed line is the true commmon effect of $x_1$ and $x_2$.\relax }}{29}{figure.caption.27}% 
\contentsline {figure}{\numberline {3.17}{\ignorespaces PDPs for features $x_1$, $x_2$ and $x_3$ (left to right) in Setting 2, based on multiple simulations with Random Forest as learning algorithm. Top row shows independent case, second row the correlated case and bottom row the dependent case. The red line represents the true effect of the respective feature on $y$, the blue dashed line is the true commmon effect of $x_1$ and $x_2$.\relax }}{30}{figure.caption.28}% 
\contentsline {figure}{\numberline {3.18}{\ignorespaces PDPs for features $x_1$, $x_2$ and $x_3$ (left to right) in Setting 2, based on multiple simulations with SVM as learning algorithm. Top row shows independent case, second row the correlated case and bottom row the dependent case. The red line represents the true effect of the respective feature on $y$, the blue dashed line is the true commmon effect of $x_1$ and $x_2$.\relax }}{31}{figure.caption.29}% 
\contentsline {figure}{\numberline {3.19}{\ignorespaces PDPs for features $x_1$ (left) and $x_2$ (right) in Setting 3, based on multiple simulations with LM as learning algorithm. Top row shows independent case, second row the correlated case and bottom row the dependent case. The red line represents the true effect of the respective feature on $y$, the blue dashed line is the true commmon effect of $x_1$ and $x_2$.\relax }}{33}{figure.caption.30}% 
\contentsline {figure}{\numberline {3.20}{\ignorespaces PDPs for features $x_1$ (left) and $x_2$ (right) in Setting 3, based on multiple simulations with RF as learning algorithm. Top row shows independent case, second row the correlated case and bottom row the dependent case. The red line represents the true effect of the respective feature on $y$, the blue dashed line is the true commmon effect of $x_1$ and $x_2$.\relax }}{34}{figure.caption.31}% 
\contentsline {figure}{\numberline {3.21}{\ignorespaces PDPs for features $x_1$ (left) and $x_2$ (right) in Setting 3, based on multiple simulations with SVM as learning algorithm. Top row shows independent case, second row the correlated case and bottom row the dependent case. The red line represents the true effect of the respective feature on $y$, the blue dashed line is the true commmon effect of $x_1$ and $x_2$.\relax }}{35}{figure.caption.32}% 
\contentsline {figure}{\numberline {3.22}{\ignorespaces PDPs for categorical features $x_1$, $x_2$ and numerical feature $x_3$ (left to right), based on simulated data and LM as learning algorithm. Top row shows independent case, second row the case of two dependent categorical features and the bottom row the case of a numerical feature depending on a categorical feature.\relax }}{38}{figure.caption.33}% 
\contentsline {figure}{\numberline {3.23}{\ignorespaces PDPs for categorical features $x_1$, $x_2$ and numerical feature $x_3$ (left to right), based on simulated data and RF as learning algorithm. Top row shows independent case, second row the case of two dependent categorical features and the bottom row the case of a numerical feature depending on a categorical feature.\relax }}{39}{figure.caption.34}% 
\contentsline {figure}{\numberline {3.24}{\ignorespaces PDPs for categorical features $x_1$, $x_2$ and numerical feature $x_3$ (left to right), based on simulated data and RF as learning algorithm. Top row shows independent case, second row the case of two dependent categorical features and the bottom row the case of a numerical feature depending on a categorical feature.\relax }}{40}{figure.caption.35}% 
\contentsline {figure}{\numberline {3.25}{\ignorespaces PDPs for uncorrelated features $x_1$ (left) and $x_2$ (right) based on complete simulated dataset (top row) and based on manipulated dataset with missing observations (bottom row). The red curve represents the true effect of the feature for which the PDP is drawn, while the PDPs derived from the machine learning models are represented by curves drawn in black (Random Forest) and blue (SVM).\relax }}{42}{figure.caption.36}% 
\contentsline {figure}{\numberline {3.26}{\ignorespaces PDPs for correlated features $x_1$ (left) and $x_2$ (right) based on complete simulated dataset (top row) and based on manipulated dataset with missing observations (bottom row). The red curve represents the true effect of the feature for which the PDP is drawn, while the PDPs derived from the machine learning models are represented by curves drawn in black (Random Forest) and blue (SVM).\relax }}{43}{figure.caption.37}% 
\contentsline {figure}{\numberline {3.27}{\ignorespaces Visualization of the observed data points (n=100) and the self-contructed prediction function. Dark blue background colour indicates a predicted potato size of zero which increases with the brightness of the yellow shaded background colour.\relax }}{44}{figure.caption.38}% 
\contentsline {figure}{\numberline {3.28}{\ignorespaces The first plot shows the two-dimensional PDP for features $x_1$ and $x_2$. The darker the background colour, the smaller the predicted values. The other plots are the PDPs derived for feature $x_1$ and $x_2$ respectively. Up to a value of approximately 0.5 both partial dependence curves are mostly linear and bend at larger $x_1$- / $x_2$-values.\relax }}{44}{figure.caption.39}% 
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces The data points within the 4-th interval are shifted to the interval boundaries \(z_{3,\nobreakspace {}1}\) and \(z_{4,\nobreakspace {}1}\).\relax }}{54}{figure.caption.40}% 
\contentsline {figure}{\numberline {6.2}{\ignorespaces ALE on basis of 5 intervals\relax }}{55}{figure.caption.41}% 
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {9.1}{\ignorespaces Example for Permutation Feature Importance. The tables illustrate the second step of the algorithm of PFI, in particular the permutation of the features $x_{1}$ and $x_{p}$. As shown, the respective columns in dark grey are the ones which were shuffeled. This breaks the association between the feature of interest and the target value. Based on the formular underneath the tables, the PFI is calculated.\relax }}{63}{figure.caption.42}% 
\contentsline {figure}{\numberline {9.2}{\ignorespaces Visualization of Permutation Feature Importance with a random forest applied on Boston dataset. The depicted points correspond to the median PFI over all shuffling iterations of one feature and the boundaries of the bands illustrate the 0.05- and 0.95-quantiles, respectively (see \texttt {iml} package).\relax }}{63}{figure.caption.43}% 
\contentsline {figure}{\numberline {9.3}{\ignorespaces Example for Leave-One-Covariate-Out Feature Importance. The tables illustrate the second step of the algorithm of LOCO in particular the drop of $x_{1}$ and $x_{p}$. The dark grey columns of the original dataset mark the variables that will be dropped and therefore ignored when refitting the model. This breaks the relationship between the feature of interest and the target value. Based on the formular underneath the tables, the Feature Importance of LOCO is calculated.\relax }}{65}{figure.caption.44}% 
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {13.1}{\ignorespaces Simplified graphical representaion of the LIME algorithm. Each single panel represents one step of the described algorithm. It reads from left to right.\relax }}{75}{figure.caption.45}% 
\addvspace {10\p@ }
\addvspace {10\p@ }
