# PFI, LOCO and Correlated Features

The method of Feature Importance is a powerful tool to gain insights of black box models assuming that the features of the given dataset are uncorrelated with each other. However, this assumption can be most of the time neglected in reality. The interpretablity of Feature Importance depend on the correlations between the input variables. If all features of  data set are independent and not correlated, you can can calculate the the feature importance of every feature and there is no problem in the interpretability due to correlation effects. Whereas if there are some correlated feature in the data which is highly possible in reality, then the results of the feature importance do not reflect the individually feature importance. This leads to misleading ranking of the features and thus to wrong interpreatations about the relevance of a feature in a model. 

This chapter demonstrates the (some) issue of correlated features and tries to present some reasons of the outcomes. This is done by looking at the behaviour of PFI and LOCO Feature Importance when the feature of the given data set are correlated with. To get a better understanding there will be a change in the intesity of the correlation of the features and in the learning algorithm. 

Two main problems can be derived from the following examples. The first and most crucial one [...]
The second does seem to play a role in case of PFI. If the features are correlated it can happen in the step of shuffling on of the features that there are unrealistic instances of datapoints. In this case it breaks not only the association to the outcome variable, but also the association with the correlated feature. For instance, there exists the variable indicating the amount of precipitation and an other variable indicating the weather. Shuffeling the the variable precipitation can lead to data instances where the precipitation is high and the weather is sunny. So there are cases where the new data points are unlikely all the way up to completly impossible. The question is can we still trust the informative value of the PFI, if it is caluclated with data instances that are not observed in reality and therefore biased?

## Effect on Feature Importance by Adding Correlated Features

* The overall emphasis is on this chapter 
* Comparison of the two methods LOCO and PFI

### Simulation

A good way to show the effects of correlated features on the feature importance measures is just to simulate some data with the desired dependence. The main focus is here on the simulation of correlated feature and not the application on real data, because the features of real data are most of the time correlated with different kind of intesity among one another. To filter out the real effect it is neccessary to hold the influence of other features as small as possible, so there will be no misinterpretations. 

The simulation design 

* Simulation of different kind of szenarios:
  + Strength of correlation: complete correlation r = 0.99 (complete correlation); r = 0.75; r = 0.5; r = 0.25; r = 0 (no correlation) of feature $X1$ and $X2$
  
* added a random variable to see if the importance is higher than a random effect
  
**Simulation of uncorrelated feature 1 and 2 (r = 0):**

```{r Simulation01, message = FALSE, fig.height=6, fig.width=12, fig.cap= "Simulation of uncorrelated features 1 and 2 (r=0)"}

library(mvtnorm)
library(matrixcalc)

set.seed(456)
# specify entries for covariance matrix
sigma_u <- diag(1, nrow = 4)
# simulate data from normal distribution 
data_u <- as.data.frame(rmvnorm(n = 1000, 
                              mean = rep(0, times = 4), 
                              sigma = sigma_u))
colnames(data_u) <- c("X1", "X2", "X3", "X4")
# check covariance and correlation matrices
cov(data_u)
is.positive.definite(cov(data_u))

cor(data_u)
is.positive.definite(cor(data_u))

data_u <- as.data.frame(data_u)
data_u['random'] <- runif(nrow(data_u))
```

```{r Simulation02, message = FALSE, fig.height=6, fig.width=12, fig.cap="Simulation of correlated features 1 and 2 (r=0.25) "}

library(mvtnorm)
library(matrixcalc)

set.seed(456)
# specify entries for covariance matrix
sigma_c25 <- diag(1, nrow = 4)
sigma_c25[1,2] <-  0.25
sigma_c25[2,1] <- 0.25
# simulate data from normal distribution 
data_c25 <- as.data.frame(rmvnorm(n = 1000, 
                              mean = rep(0, times = 4), 
                              sigma = sigma_c25))
colnames(data_c25) <- c("X1", "X2", "X3", "X4")
# check covariance and correlation matrices 
cov(data_c25)
is.positive.definite(cov(data_c25))

cor(data_c25)
is.positive.definite(cor(data_c25))

data_c25 <- as.data.frame(data_c25)
data_c25['random'] <- runif(nrow(data_c25))
```

```{r Simulation03, message = FALSE, fig.height=6, fig.width=12, fig.cap="Simulation of medium correlated features 1 and 2 (r=0.5) "}

library(mvtnorm)
library(matrixcalc)

set.seed(456)
# specify entries for covariance matrix
sigma_c5 <- diag(1, nrow = 4)
sigma_c5[1,2] <-  0.5
sigma_c5[2,1] <- 0.5
# simulate data from normal distribution 
data_c5 <- as.data.frame(rmvnorm(n = 1000, 
                              mean = rep(0, times = 4), 
                              sigma = sigma_c5))
colnames(data_c5) <- c("X1", "X2", "X3", "X4")
# check covariance and correlation matrices 
cov(data_c5)
is.positive.definite(cov(data_c5))

cor(data_c5)
is.positive.definite(cor(data_c5))

data_c5 <- as.data.frame(data_c5)
data_c5['random'] <- runif(nrow(data_c5))
```

```{r Simulation04, message = FALSE, fig.height=6, fig.width=12, fig.cap="Simulation of high correlated features 1 and 2 (r=0.75) "}

library(mvtnorm)
library(matrixcalc)

set.seed(456)
# specify entries for covariance matrix
sigma_c75 <- diag(1, nrow = 4)
sigma_c75[1,2] <-  0.75
sigma_c75[2,1] <- 0.75
# simulate data from normal distribution 
data_c75 <- as.data.frame(rmvnorm(n = 1000, 
                              mean = rep(0, times = 4), 
                              sigma = sigma_c75))
colnames(data_c75) <- c("X1", "X2", "X3", "X4")
# check covariance and correlation matrices 
cov(data_c75)
is.positive.definite(cov(data_c75))

cor(data_c75)
is.positive.definite(cor(data_c75))

data_c75 <- as.data.frame(data_c75)
data_c75['random'] <- runif(nrow(data_c75))
```

Simulation of highly correlated features 1 and 2 (r=0.99):

*we chosse r= 0.99 here to avoid problems to calculate matrices. If r=1 we have perfect multicollinearity and the problem that the rank of the matrix is not full. 


```{r Simulation05, message = FALSE, fig.height=6, fig.width=12, fig.cap="Simulation of complete correlated features 1 and 2 (r=0.99) "}

library(mvtnorm)
library(matrixcalc)

set.seed(456)
# specify entries for covariance matrix
sigma_c <- diag(1, nrow = 4)
sigma_c[1,2] <-  0.99
sigma_c[2,1] <- 0.99
# simulate data from normal distribution 
data_c <- as.data.frame(rmvnorm(n = 1000, 
                              mean = rep(0, times = 4), 
                              sigma = sigma_c))
colnames(data_c) <- c("X1", "X2", "X3", "X4")
# check covariance and correlation matrices 
cov(data_c)
is.positive.definite(cov(data_c))

cor(data_c)
is.positive.definite(cor(data_c))

data_c <- as.data.frame(data_c)
data_c['random'] <- runif(nrow(data_c))
```


**Scenario Settings:**

Here we want to investigate the effect on PFI and LOCO Feature Importance of the correlated features with different kind of dependencies on the target value y. There are four features of which two ($X1$ and $X2$) show different correlaion intensities $r$ betweeen eah other. Otherwise, there is no correlation between any feature. Only in scenario 4) the setting changes to more features and more correlations to investigate the behaviour, if there is an increase in the features which are correlated.

(only the scenarios which show interesting results will be shown later). There are many interesting instances.  

**1) Linear Dependence:**

In the first scenario setting the dependence of the features $X_i$ on the target value $y$ is a linear one. 

The choice of noise variance $sd_i^2$ should be hold small in order to make the behavior we observe clearer and there will be no misinterpretation. In this case we assume that there is only a standard deviation of ten percent of the mean of the [...].


```{r szenariolm, message = FALSE, fig.height=6, fig.width=12, fig.cap="Szenario Linear Dependence with differrent correlation of feature 1 and 2"}

sd_u <- mean(data_u$X1 + data_u$X2 + data_u$X3 + data_u$X4)*0.1

sd_c25 <- mean(data_c25$X1 + data_c25$X2 + data_c25$X3 + data_c25$X4)*0.1

sd_c5 <- mean(data_c5$X1 + data_c5$X2 + data_c5$X3 + data_c5$X4)*0.1

sd_c75 <- mean(data_c75$X1 + data_c75$X2 + data_c75$X3 + data_c75$X4)*0.1

sd_c <- mean(data_c$X1 + data_c$X2 + data_c$X3 + data_c$X4)*0.1

#The choice of noise variance should be hold small in order to make the behavior we observe clearer and there will be no misinterpretation.

y1u <- data_u$X1 + data_u$X2 + data_u$X3 + data_u$X4 + rnorm(n = 1, mean = 0, sd = sd_u) 

y1c25 <- data_c25$X1 + data_c25$X2 + data_c25$X3 + data_c25$X4 + rnorm(n = 1, mean = 0, sd = sd_c25)

y1c5 <- data_c5$X1 + data_c5$X2 + data_c5$X3 + data_c5$X4 + rnorm(n = 1, mean = 0, sd = sd_c5)

y1c75 <- data_c75$X1 + data_c75$X2 + data_c75$X3 + data_c75$X4 + rnorm(n = 1, mean = 0, sd = sd_c75)

y1c <- data_c$X1 + data_c$X2 + data_c$X3 + data_c$X4 + rnorm(n = 1, mean = 0, sd = sd_c) 
```


```{r PFI01, message = FALSE, fig.height=6, fig.width=12, fig.cap= "PFI with different correlations of features 1 and 2 on a random forest model"}

library(mlr)
library(iml)
library(ggplot2)
library(randomForest)

# For a random forest model: 
rfu <- randomForest(y1u~. , data = data_u)
rf25 <- randomForest(y1c25~. , data = data_c25)
rf5 <- randomForest(y1c5~. , data = data_c5)
rf75 <- randomForest(y1c75~. , data = data_c75)
rfc <- randomForest(y1c~. , data = data_c)

modrfu <- Predictor$new(rfu, data=data_u, y = y1u)
modrf25 <- Predictor$new(rf25, data=data_c25, y = y1c25)
modrf5 <- Predictor$new(rf5, data=data_c5, y = y1c5)
modrf75 <- Predictor$new(rf75, data=data_c75, y = y1c75)
modrfc <- Predictor$new(rfc, data=data_c, y = y1c)

imprfu <- FeatureImp$new(modrfu, loss = "mse" , compare = "ratio")
imprfc25 <- FeatureImp$new(modrf25, loss = "mse" , compare = "ratio")
imprfc5 <- FeatureImp$new(modrf5, loss = "mse" , compare = "ratio")
imprfc75 <- FeatureImp$new(modrf75, loss = "mse" , compare = "ratio")
imprfc <- FeatureImp$new(modrfc, loss = "mse" , compare = "ratio")

imp.datrfu <- imprfu$results
imp.datrfc25 <- imprfc25$results
imp.datrfc5 <- imprfc5$results
imp.datrfc75 <- imprfc75$results
imp.datrfc <- imprfc$results

imp.datrfu$feature

#Plot:
prf <- ggplot(imp.datrfu, aes(x=feature,y= importance, colour = "0"))+
    geom_point(mapping=aes(x=feature, y=importance), size=0, stroke =3, shape=21, fill="white") +
    geom_point(mapping=aes(x=imp.datrfc25$feature, y=imp.datrfc25$importance, colour = "0.25"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
    geom_point(mapping=aes(x=imp.datrfc5$feature, y=imp.datrfc5$importance, colour = "0.5"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
    geom_point(mapping=aes(x=imp.datrfc75$feature, y=imp.datrfc75$importance, colour = "0.75"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
    geom_point(mapping=aes(x=imp.datrfc$feature, y=imp.datrfc$importance, colour = "0.99"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
    coord_flip() +
    theme(plot.subtitle = element_text(size = 10)) +
    theme(text = element_text(size = 14))+
    labs(y = "Feature Importance MSE", x= "") +
    scale_color_manual(labels = c("0","0.25","0.5","0.75","0.99"), values=c("0" = "white", "0.25" = "lightblue", "0.5"= "blue", "0.75"="darkblue", "0.99" = "black"), name = "Correlation Intensity",breaks=c("0", "0.25", "0.5", "0.75", "0.99"))+
    ggtitle("PFI on Random Forest")

prf
```

```{r PFI02, message = FALSE, fig.height=6, fig.width=12, fig.cap="PFI with different correlations of features 1 and 2 on a linear model"}

library(mlr)
library(iml)
library(ggplot2)
library(randomForest)

lmu <- lm(y1u~., data=data_u)
lmc25 <- lm(y1c25~., data = data_c25)
lmc5 <- lm(y1c5~., data = data_c5)
lmc75 <- lm(y1c75~., data = data_c75)
lmc <- lm(y1c~., data = data_c)

modlmu <- Predictor$new(lmu, data=data_u, y = y1u)
modlmc25 <- Predictor$new(lmc25, data=data_c25, y = y1c25)
modlmc5 <- Predictor$new(lmc5, data=data_c5, y = y1c5)
modlmc75 <- Predictor$new(lmc75, data=data_c75, y = y1c75)
modlmc <- Predictor$new(lmc, data=data_c, y = y1c)

implmu <- FeatureImp$new(modlmu, loss = "mse" , compare = "ratio")
implmc25 <- FeatureImp$new(modlmc25, loss = "mse" , compare = "ratio")
implmc5 <- FeatureImp$new(modlmc5, loss = "mse" , compare = "ratio")
implmc75 <- FeatureImp$new(modlmc75, loss = "mse" , compare = "ratio")
implmc <- FeatureImp$new(modlmc, loss = "mse" , compare = "ratio")

imp.datlmu <- implmu$results
imp.datlmc25 <- implmc25$results
imp.datlmc5 <- implmc5$results
imp.datlmc75 <- implmc75$results
imp.datlmc <- implmc$results

#Plot:
plm <- ggplot(imp.datlmu, aes(x=feature, y= importance, colour = "0")) +
    geom_point(mapping=aes(x=feature, y=importance), size=0, stroke = 3, shape=21, fill="white") +
    geom_point(mapping=aes(x=imp.datlmc25$feature, y=imp.datlmc25$importance, colour = "0.25"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
    geom_point(mapping=aes(x=imp.datlmc5$feature, y=imp.datlmc5$importance, colour = "0.5"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
    geom_point(mapping=aes(x=imp.datlmc75$feature, y=imp.datlmc75$importance, colour = "0.75"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
    geom_point(mapping=aes(x=imp.datlmc$feature, y=imp.datlmc$importance, colour = "0.99"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
    coord_flip() +
    theme(plot.subtitle = element_text(size = 10)) +
    labs(y = "Feature Importance MSE", x= "") + 
    theme(text = element_text(size = 14))+
    scale_color_manual(labels = c("0","0.25","0.5","0.75","0.99"), values=c("0" = "white", "0.25" = "lightblue", "0.5"= "blue", "0.75"="darkblue", "0.99" = "black"), name = "Correlation Intensity",breaks=c("0", "0.25", "0.5", "0.75", "0.99"))+
    ggtitle("PFI on Linear Model")

plm
```

```{r LOCO01, message = FALSE, fig.height=6, fig.width=12, fig.cap= "LOCO Feature Importance with different kind of correlation intensity on a random forest model"}
library(mlr)
library(iml)
library(ggplot2)
library(randomForest)

res_desc <- makeResampleDesc("Subsample", iters = 5, split = 4/5)

du <- data_u
du["y1u"] <- y1u
tasku = makeRegrTask(data = du , target= "y1u")
learneru = makeLearner("regr.randomForest")
featu = getTaskFeatureNames(tasku)
featu
mod.fullu = train(learneru, tasku)
resu = resample(learner = learneru, task = tasku, 
               resampling = res_desc,show.info = FALSE)

resultu <- data.frame(matrix(nrow=1,ncol=length(featu))) 

for(i in 1:length(featu)){
  taskfeatu = dropFeatures(tasku, featu[i])
  mod.featu = train(learneru, dropFeatures(tasku, featu[i]))
  resfeatu = resample(learner = learneru, task = taskfeatu, resampling = res_desc ,show.info = FALSE);
  importance_u = data.frame(resfeatu$aggr/resu$aggr)
  featureu = c(getTaskFeatureNames(tasku))
  resultu[i] <- importance_u
  #print(importance)
}
resultu
colnames(resultu) <- getTaskFeatureNames(tasku)
rownames(resultu) <- "importance_u"
FIPu <- data.frame(
  importanceu= round((t(resultu)),3),
  featureu= getTaskFeatureNames(tasku))


dc25 <- data_c25
dc25["y1c25"] <- y1c25
taskc25 = makeRegrTask(data = dc25 , target= "y1c25")
learnerc25 = makeLearner("regr.randomForest")
featc25 = getTaskFeatureNames(taskc25)
featc25
mod.fullc25 = train(learnerc25, taskc25)
resc25 = resample(learner = learnerc25, task = taskc25, 
               resampling = res_desc,show.info = FALSE)

resultc25 <- data.frame(matrix(nrow=1,ncol=length(featc25))) 

for(i in 1:length(featc25)){
  taskfeatc25 = dropFeatures(taskc25, featc25[i])
  mod.featc25 = train(learnerc25, dropFeatures(taskc25, featc25[i]))
  resfeatc25 = resample(learner = learnerc25, task = taskfeatc25, resampling = res_desc ,show.info = FALSE);
  importance_c25 = data.frame(resfeatc25$aggr/resc25$aggr)
  featurec25 = c(getTaskFeatureNames(taskc25))
  resultc25[i] <- importance_c25
  #print(importance)
}
resultc25
colnames(resultc25) <- getTaskFeatureNames(taskc25)
rownames(resultc25) <- "importance_c25"
FIPc25 <- data.frame(
  importancec25= round((t(resultc25)),3),
  featurec25= getTaskFeatureNames(taskc25))


dc5 <- data_c5
dc5["y1c5"] <- y1c5
taskc5 = makeRegrTask(data = dc5 , target= "y1c5")
learnerc5 = makeLearner("regr.randomForest")
featc5 = getTaskFeatureNames(taskc5)
featc5
mod.fullc5 = train(learnerc5, taskc5)
resc5 = resample(learner = learnerc5, task = taskc5, 
               resampling = res_desc,show.info = FALSE)

resultc5 <- data.frame(matrix(nrow=1,ncol=length(featc5))) 

for(i in 1:length(featc5)){
  taskfeatc5 = dropFeatures(taskc5, featc5[i])
  mod.featc5 = train(learnerc5, dropFeatures(taskc5, featc5[i]))
  resfeatc5 = resample(learner = learnerc5, task = taskfeatc5, resampling = res_desc ,show.info = FALSE);
  importance_c5 = data.frame(resfeatc5$aggr/resc5$aggr)
  featurec5 = c(getTaskFeatureNames(taskc5))
  resultc5[i] <- importance_c5
  #print(importance)
}
resultc5
colnames(resultc5) <- getTaskFeatureNames(taskc5)
rownames(resultc5) <- "importance_c5"
FIPc5 <- data.frame(
  importancec5= round((t(resultc5)),3),
  featurec5= getTaskFeatureNames(taskc5))


dc75 <- data_c75
dc75["y1c75"] <- y1c75
taskc75 = makeRegrTask(data = dc75 , target= "y1c75")
learnerc75 = makeLearner("regr.randomForest")
featc75 = getTaskFeatureNames(taskc75)
featc75
mod.fullc75 = train(learnerc75, taskc75)
resc75 = resample(learner = learnerc75, task = taskc75, 
               resampling = res_desc,show.info = FALSE)

resultc75 <- data.frame(matrix(nrow=1,ncol=length(featc75))) 

for(i in 1:length(featc75)){
  taskfeatc75 = dropFeatures(taskc75, featc75[i])
  mod.featc75 = train(learnerc75, dropFeatures(taskc75, featc75[i]))
  resfeatc75 = resample(learner = learnerc75, task = taskfeatc75, resampling = res_desc ,show.info = FALSE);
  importance_c75 = data.frame(resfeatc75$aggr/resc75$aggr)
  featurec75 = c(getTaskFeatureNames(taskc75))
  resultc75[i] <- importance_c75
  #print(importance)
}
resultc75
colnames(resultc75) <- getTaskFeatureNames(taskc75)
rownames(resultc75) <- "importance_c75"
FIPc75 <- data.frame(
  importancec75= round((t(resultc75)),3),
  featurec75= getTaskFeatureNames(taskc75))

dc <- data_c
dc["y1c"] <- y1c
taskc = makeRegrTask(data = dc , target= "y1c")
learnerc = makeLearner("regr.randomForest")
featc = getTaskFeatureNames(taskc)
featc
mod.fullc = train(learnerc, taskc)
resc = resample(learner = learnerc, task = taskc, 
               resampling = res_desc,show.info = FALSE)

resultc <- data.frame(matrix(nrow=1,ncol=length(featc))) 

for(i in 1:length(featc)){
  taskfeatc = dropFeatures(taskc, featc[i])
  mod.featc = train(learnerc, dropFeatures(taskc, featc[i]))
  resfeatc = resample(learner = learnerc, task = taskfeatc, resampling = res_desc ,show.info = FALSE);
  importance_c = data.frame(resfeatc$aggr/resc$aggr)
  featurec = c(getTaskFeatureNames(taskc))
  resultc[i] <- importance_c
  #print(importance)
}
resultc
colnames(resultc) <- getTaskFeatureNames(taskc)
rownames(resultc) <- "importance_c5"
FIPc <- data.frame(
  importancec= round((t(resultc)),3),
  featurec= getTaskFeatureNames(taskc))

#Plot:
ploco <- ggplot(data = FIPu ,aes(x=featureu,y = importance_u, colour = "0")) + #### hier fehler 
  geom_point(mapping=aes(x=featureu, y=importance_u), size=0, shape=21,stroke = 3, fill="white") +
  geom_point(mapping=aes(x=FIPc25$featurec25, y=FIPc25$importance_c25, colour = "0.25"), size=0,stroke = 3, shape=21, fill="white", show.legend = TRUE)+
  geom_point(mapping=aes(x=FIPc5$featurec5, y=FIPc5$importance_c5, colour = "0.5"), size=0,stroke = 3, shape=21, fill="white", show.legend = TRUE)+
  geom_point(mapping=aes(x=FIPc75$featurec75, y=FIPc75$importance_c75, colour = "0.75"), size=0,stroke = 3, shape=21, fill="white", show.legend = TRUE)+
  geom_point(mapping=aes(x=FIPc$featurec, y=FIPc$importance_c, colour = "0.99"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
  coord_flip() +
  theme(plot.subtitle = element_text(size = 10)) +
  labs(y = "Feature Importance MSE", x= "") + 
  theme(text = element_text(size = 14))+
  scale_color_manual(labels = c("0","0.25","0.5","0.75","0.99"), values=c("0" = "white", "0.25" = "lightblue", "0.5"= "blue", "0.75"="darkblue", "0.99" = "black"), name = "Correlation Intensity",breaks=c("0", "0.25", "0.5", "0.75", "0.99"))+
  ggtitle("LOCO on Random Forest")

ploco

#Problem to order the importance within the graphic. This issue should be fixed.

```


```{r LOCO02, message = FALSE, fig.height=6, fig.width=12, fig.cap="LOCO Feature Importance with different kind of correlation intensity on a linear model"}
library(mlr)
library(iml)
library(ggplot2)
library(randomForest)


res_desc <- makeResampleDesc("Subsample", iters = 5, split = 4/5)

du <- data_u
du["y1u"] <- y1u
tasku = makeRegrTask(data = du , target= "y1u")
learneru = makeLearner("regr.lm")
featu = getTaskFeatureNames(tasku)
featu
mod.fullu = train(learneru, tasku)
resu = resample(learner = learneru, task = tasku, 
               resampling = res_desc,show.info = FALSE)

resultu <- data.frame(matrix(nrow=1,ncol=length(featu))) 

for(i in 1:length(featu)){
  taskfeatu = dropFeatures(tasku, featu[i])
  mod.featu = train(learneru, dropFeatures(tasku, featu[i]))
  resfeatu = resample(learner = learneru, task = taskfeatu, resampling = res_desc ,show.info = FALSE);
  importance_u = data.frame(resfeatu$aggr/resu$aggr)
  featureu = c(getTaskFeatureNames(tasku))
  resultu[i] <- importance_u
  #print(importance)
}
resultu
colnames(resultu) <- getTaskFeatureNames(tasku)
rownames(resultu) <- "importance_u"
FIPulm <- data.frame(
  importanceu= round((t(resultu)),3),
  featureu= getTaskFeatureNames(tasku))


dc25 <- data_c25
dc25["y1c25"] <- y1c25
taskc25 = makeRegrTask(data = dc25 , target= "y1c25")
learnerc25 = makeLearner("regr.lm")
featc25 = getTaskFeatureNames(taskc25)
featc25
mod.fullc25 = train(learnerc25, taskc25)
resc25 = resample(learner = learnerc25, task = taskc25, 
               resampling = res_desc,show.info = FALSE)

resultc25 <- data.frame(matrix(nrow=1,ncol=length(featc25))) 

for(i in 1:length(featc25)){
  taskfeatc25 = dropFeatures(taskc25, featc25[i])
  mod.featc25 = train(learnerc25, dropFeatures(taskc25, featc25[i]))
  resfeatc25 = resample(learner = learnerc25, task = taskfeatc25, resampling = res_desc ,show.info = FALSE);
  importance_c25 = data.frame(resfeatc25$aggr/resc25$aggr)
  featurec25 = c(getTaskFeatureNames(taskc25))
  resultc25[i] <- importance_c25
  #print(importance)
}
resultc25
colnames(resultc25) <- getTaskFeatureNames(taskc25)
rownames(resultc25) <- "importance_c25"
FIPc25lm <- data.frame(
  importancec25= round((t(resultc25)),3),
  featurec25= getTaskFeatureNames(taskc25))


dc5 <- data_c5
dc5["y1c5"] <- y1c5
taskc5 = makeRegrTask(data = dc5 , target= "y1c5")
learnerc5 = makeLearner("regr.lm")
featc5 = getTaskFeatureNames(taskc5)
featc5
mod.fullc5 = train(learnerc5, taskc5)
resc5 = resample(learner = learnerc5, task = taskc5, 
               resampling = res_desc,show.info = FALSE)

resultc5 <- data.frame(matrix(nrow=1,ncol=length(featc5))) 

for(i in 1:length(featc5)){
  taskfeatc5 = dropFeatures(taskc5, featc5[i])
  mod.featc5 = train(learnerc5, dropFeatures(taskc5, featc5[i]))
  resfeatc5 = resample(learner = learnerc5, task = taskfeatc5, resampling = res_desc ,show.info = FALSE);
  importance_c5 = data.frame(resfeatc5$aggr/resc5$aggr)
  featurec5 = c(getTaskFeatureNames(taskc5))
  resultc5[i] <- importance_c5
  #print(importance)
}
resultc5
colnames(resultc5) <- getTaskFeatureNames(taskc5)
rownames(resultc5) <- "importance_c5"
FIPc5lm <- data.frame(
  importancec5= round((t(resultc5)),3),
  featurec5= getTaskFeatureNames(taskc5))


dc75 <- data_c75
dc75["y1c75"] <- y1c75
taskc75 = makeRegrTask(data = dc75 , target= "y1c75")
learnerc75 = makeLearner("regr.lm")
featc75 = getTaskFeatureNames(taskc75)
featc75
mod.fullc75 = train(learnerc75, taskc75)
resc75 = resample(learner = learnerc75, task = taskc75, 
               resampling = res_desc,show.info = FALSE)

resultc75 <- data.frame(matrix(nrow=1,ncol=length(featc75))) 

for(i in 1:length(featc75)){
  taskfeatc75 = dropFeatures(taskc75, featc75[i])
  mod.featc75 = train(learnerc75, dropFeatures(taskc75, featc75[i]))
  resfeatc75 = resample(learner = learnerc75, task = taskfeatc75, resampling = res_desc ,show.info = FALSE);
  importance_c75 = data.frame(resfeatc75$aggr/resc75$aggr)
  featurec75 = c(getTaskFeatureNames(taskc75))
  resultc75[i] <- importance_c75
  #print(importance)
}
resultc75
colnames(resultc75) <- getTaskFeatureNames(taskc75)
rownames(resultc75) <- "importance_c75"
FIPc75lm <- data.frame(
  importancec75= round((t(resultc75)),3),
  featurec75= getTaskFeatureNames(taskc75))

dc <- data_c
dc["y1c"] <- y1c
taskc = makeRegrTask(data = dc , target= "y1c")
learnerc = makeLearner("regr.lm")
featc = getTaskFeatureNames(taskc)
featc
mod.fullc = train(learnerc, taskc)
resc = resample(learner = learnerc, task = taskc, 
               resampling = res_desc,show.info = FALSE)

resultc <- data.frame(matrix(nrow=1,ncol=length(featc))) 

for(i in 1:length(featc)){
  taskfeatc = dropFeatures(taskc, featc[i])
  mod.featc = train(learnerc, dropFeatures(taskc, featc[i]))
  resfeatc = resample(learner = learnerc, task = taskfeatc, resampling = res_desc ,show.info = FALSE);
  importance_c = data.frame(resfeatc$aggr/resc$aggr)
  featurec = c(getTaskFeatureNames(taskc))
  resultc[i] <- importance_c
  #print(importance)
}
resultc
colnames(resultc) <- getTaskFeatureNames(taskc)
rownames(resultc) <- "importance_c5"
FIPclm <- data.frame(
  importancec= round((t(resultc)),3),
  featurec= getTaskFeatureNames(taskc))

#Plot:
plocolm <- ggplot(data = FIPulm ,aes(x=featureu,y = importance_u, colour = "0"))+ 
  geom_point(mapping=aes(x=featureu, y=importance_u), size=0, stroke =3, shape=21, fill="white") +
  geom_point(mapping=aes(x=FIPc25lm$featurec25, y=FIPc25lm$importance_c25, colour = "0.25"), size=0,stroke = 3, shape=21, fill="white", show.legend = TRUE)+
  geom_point(mapping=aes(x=FIPc5lm$featurec5, y=FIPc5lm$importance_c5, colour = "0.5"), size=0,stroke = 3, shape=21, fill="white", show.legend = TRUE)+
  geom_point(mapping=aes(x=FIPc75lm$featurec75, y=FIPc75lm$importance_c75, colour = "0.75"), size=0,stroke = 3, shape=21, fill="white", show.legend = TRUE)+
  geom_point(mapping=aes(x=FIPclm$featurec, y=FIPclm$importance_c, colour = "0.99"), size=0,stroke = 3, shape=21, fill="white", show.legend = TRUE)+
  coord_flip() +
  theme(plot.subtitle = element_text(size = 10)) +
  labs(y = "Feature Importance MSE", x= "") + 
  theme(text = element_text(size = 14))+
  scale_color_manual(labels = c("0","0.25","0.5","0.75","0.99"), values=c("0" = "white", "0.25" = "lightblue", "0.5"= "blue", "0.75"="darkblue", "0.99" = "black"), name = "Correlation Intensity",breaks=c("0", "0.25", "0.5", "0.75", "0.99"))+
  ggtitle("LOCO on Linear Model")

plocolm
```


```{r arrange01, message = FALSE, fig.height=6, fig.width=12, fig.cap= "Comparison"}
library(ggpubr)
ggarrange(prf, plm, ploco, plocolm , ncol=2, nrow = 2, legend = "bottom", common.legend = TRUE)
``` 


**2) Nonlinear Dependence:**

In the second scenario there is no pure linear relationship between the target value $y$ and the features $X_{i}$. Here $X_{1}$ has a non linear dependence in form of the sine function. 

(For  better comparison: Maybe take also sin(X3))



```{r scenariononnld, message = FALSE, fig.height=6, fig.width=12, fig.cap=" Simulation of Nonlinear Dependence"}


sd_u <- mean(sin(data_u$X1) + data_u$X2 + data_u$X3 + data_u$X4)*0.1

sd_c25 <- mean(sin(data_c25$X1) + data_c25$X2 + data_c25$X3 + data_c25$X4)*0.1

sd_c5 <- mean(sin(data_c5$X1) + data_c5$X2 + data_c5$X3 + data_c5$X4)*0.1

sd_c75 <- mean(sin(data_c75$X1) + data_c75$X2 + data_c75$X3 + data_c75$X4)*0.1

sd_c <- mean(sin(data_c$X1) + data_c$X2 + data_c$X3 + data_c$X4)*0.1


y1u <- sin(data_u$X1) + data_u$X2 + data_u$X3 + data_u$X4 + rnorm(n = 1, mean = 0, sd = sd_u) 

y1c25 <- sin(data_c25$X1) + data_c25$X2 + data_c25$X3 + data_c25$X4 + rnorm(n = 1, mean = 0, sd = sd_c25)

y1c5 <- sin(data_c5$X1) + data_c5$X2 + data_c5$X3 + data_c5$X4 + rnorm(n = 1, mean = 0, sd = sd_c5)

y1c75 <- sin(data_c75$X1) + data_c75$X2 + data_c75$X3 + data_c75$X4 + rnorm(n = 1, mean = 0, sd = sd_c75)

y1c <- sin(data_c$X1) + data_c$X2 + data_c$X3 + data_c$X4 + rnorm(n = 1, mean = 0, sd = sd_c)  
```  


```{r PFI03, message = FALSE, fig.height=6, fig.width=12, fig.cap= "PFI with different correlations of features 1 and 2 on a random forest"}
library(mlr)
library(iml)
library(ggplot2)
library(randomForest)

rfu <- randomForest(y1u~. , data = data_u)
rf25 <- randomForest(y1c25~. , data = data_c25)
rf5 <- randomForest(y1c5~. , data = data_c5)
rf75 <- randomForest(y1c75~. , data = data_c75)
rfc <- randomForest(y1c~. , data = data_c)

modrfu <- Predictor$new(rfu, data=data_u, y = y1u)
modrf25 <- Predictor$new(rf25, data=data_c25, y = y1c25)
modrf5 <- Predictor$new(rf5, data=data_c5, y = y1c5)
modrf75 <- Predictor$new(rf75, data=data_c75, y = y1c75)
modrfc <- Predictor$new(rfc, data=data_c, y = y1c)

imprfu <- FeatureImp$new(modrfu, loss = "mse" , compare = "ratio")
imprfc25 <- FeatureImp$new(modrf25, loss = "mse" , compare = "ratio")
imprfc5 <- FeatureImp$new(modrf5, loss = "mse" , compare = "ratio")
imprfc75 <- FeatureImp$new(modrf75, loss = "mse" , compare = "ratio")
imprfc <- FeatureImp$new(modrfc, loss = "mse" , compare = "ratio")

imp.datrfu <- imprfu$results
imp.datrfc25 <- imprfc25$results
imp.datrfc5 <- imprfc5$results
imp.datrfc75 <- imprfc75$results
imp.datrfc <- imprfc$results


prf <- ggplot(imp.datrfu, aes(x=feature,y= importance, colour = "0"))+
    geom_point(mapping=aes(x=feature, y=importance), size=0, stroke =3, shape=21, fill="white") +
    geom_point(mapping=aes(x=imp.datrfc25$feature, y=imp.datrfc25$importance, colour = "0.25"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
    geom_point(mapping=aes(x=imp.datrfc5$feature, y=imp.datrfc5$importance, colour = "0.5"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
    geom_point(mapping=aes(x=imp.datrfc75$feature, y=imp.datrfc75$importance, colour = "0.75"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
    geom_point(mapping=aes(x=imp.datrfc$feature, y=imp.datrfc$importance, colour = "0.99"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
    coord_flip() +
    theme(plot.subtitle = element_text(size = 10)) +
    theme(text = element_text(size = 14))+
    labs(y = "Feature Importance MSE", x= "") +
    scale_color_manual(labels = c("0","0.25","0.5","0.75","0.99"), values=c("0" = "white", "0.25" = "lightblue", "0.5"= "blue", "0.75"="darkblue", "0.99" = "black"), name = "Correlation Intensity",breaks=c("0", "0.25", "0.5", "0.75", "0.99"))+
    ggtitle("PFI on Random Forest")

prf
```


```{r PFI04, message = FALSE, fig.height=6, fig.width=12, fig.cap="PFI with different correlations of features 1 and 2 on a linear model"}

library(mlr)
library(iml)
library(ggplot2)
library(randomForest)

lmu <- lm(y1u~., data=data_u)
lmc25 <- lm(y1c25~., data = data_c25)
lmc5 <- lm(y1c5~., data = data_c5)
lmc75 <- lm(y1c75~., data = data_c75)
lmc <- lm(y1c~., data = data_c)

modlmu <- Predictor$new(lmu, data=data_u, y = y1u)
modlmc25 <- Predictor$new(lmc25, data=data_c25, y = y1c25)
modlmc5 <- Predictor$new(lmc5, data=data_c5, y = y1c5)
modlmc75 <- Predictor$new(lmc75, data=data_c75, y = y1c75)
modlmc <- Predictor$new(lmc, data=data_c, y = y1c)

implmu <- FeatureImp$new(modlmu, loss = "mse" , compare = "ratio")
implmc25 <- FeatureImp$new(modlmc25, loss = "mse" , compare = "ratio")
implmc5 <- FeatureImp$new(modlmc5, loss = "mse" , compare = "ratio")
implmc75 <- FeatureImp$new(modlmc75, loss = "mse" , compare = "ratio")
implmc <- FeatureImp$new(modlmc, loss = "mse" , compare = "ratio")

imp.datlmu <- implmu$results
imp.datlmc25 <- implmc25$results
imp.datlmc5 <- implmc5$results
imp.datlmc75 <- implmc75$results
imp.datlmc <- implmc$results

plm <- ggplot(imp.datlmu, aes(x=feature, y= importance, colour = "0")) +
    geom_point(mapping=aes(x=feature, y=importance), size=0, stroke = 3, shape=21, fill="white") +
    geom_point(mapping=aes(x=imp.datlmc25$feature, y=imp.datlmc25$importance, colour = "0.25"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
    geom_point(mapping=aes(x=imp.datlmc5$feature, y=imp.datlmc5$importance, colour = "0.5"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
    geom_point(mapping=aes(x=imp.datlmc75$feature, y=imp.datlmc75$importance, colour = "0.75"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
    geom_point(mapping=aes(x=imp.datlmc$feature, y=imp.datlmc$importance, colour = "0.99"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
    coord_flip() +
    theme(plot.subtitle = element_text(size = 10)) +
    labs(y = "Feature Importance MSE", x= "") + 
    theme(text = element_text(size = 14))+
    scale_color_manual(labels = c("0","0.25","0.5","0.75","0.99"), values=c("0" = "white", "0.25" = "lightblue", "0.5"= "blue", "0.75"="darkblue", "0.99" = "black"), name = "Correlation Intensity",breaks=c("0", "0.25", "0.5", "0.75", "0.99"))+
    ggtitle("PFI on Linear Model")

plm

```

```{r LOCO03, message = FALSE, fig.height=6, fig.width=12, fig.cap="LOCO Feature Importance with different kind of correlation intensity on a random forest model"}
library(mlr)
library(iml)
library(ggplot2)
library(randomForest)

res_desc <- makeResampleDesc("Subsample", iters = 5, split = 4/5)

du <- data_u
du["y1u"] <- y1u
tasku = makeRegrTask(data = du , target= "y1u")
learneru = makeLearner("regr.randomForest")
featu = getTaskFeatureNames(tasku)
featu
mod.fullu = train(learneru, tasku)
resu = resample(learner = learneru, task = tasku, 
               resampling = res_desc,show.info = FALSE)

resultu <- data.frame(matrix(nrow=1,ncol=length(featu))) 

for(i in 1:length(featu)){
  taskfeatu = dropFeatures(tasku, featu[i])
  mod.featu = train(learneru, dropFeatures(tasku, featu[i]))
  resfeatu = resample(learner = learneru, task = taskfeatu, resampling = res_desc ,show.info = FALSE);
  importance_u = data.frame(resfeatu$aggr/resu$aggr)
  featureu = c(getTaskFeatureNames(tasku))
  resultu[i] <- importance_u
  #print(importance)
}
resultu
colnames(resultu) <- getTaskFeatureNames(tasku)
rownames(resultu) <- "importance_u"
FIPu <- data.frame(
  importanceu= round((t(resultu)),3),
  featureu= getTaskFeatureNames(tasku))


dc25 <- data_c25
dc25["y1c25"] <- y1c25
taskc25 = makeRegrTask(data = dc25 , target= "y1c25")
learnerc25 = makeLearner("regr.randomForest")
featc25 = getTaskFeatureNames(taskc25)
featc25
mod.fullc25 = train(learnerc25, taskc25)
resc25 = resample(learner = learnerc25, task = taskc25, 
               resampling = res_desc,show.info = FALSE)

resultc25 <- data.frame(matrix(nrow=1,ncol=length(featc25))) 

for(i in 1:length(featc25)){
  taskfeatc25 = dropFeatures(taskc25, featc25[i])
  mod.featc25 = train(learnerc25, dropFeatures(taskc25, featc25[i]))
  resfeatc25 = resample(learner = learnerc25, task = taskfeatc25, resampling = res_desc ,show.info = FALSE);
  importance_c25 = data.frame(resfeatc25$aggr/resc25$aggr)
  featurec25 = c(getTaskFeatureNames(taskc25))
  resultc25[i] <- importance_c25
  #print(importance)
}
resultc25
colnames(resultc25) <- getTaskFeatureNames(taskc25)
rownames(resultc25) <- "importance_c25"
FIPc25 <- data.frame(
  importancec25= round((t(resultc25)),3),
  featurec25= getTaskFeatureNames(taskc25))


dc5 <- data_c5
dc5["y1c5"] <- y1c5
taskc5 = makeRegrTask(data = dc5 , target= "y1c5")
learnerc5 = makeLearner("regr.randomForest")
featc5 = getTaskFeatureNames(taskc5)
featc5
mod.fullc5 = train(learnerc5, taskc5)
resc5 = resample(learner = learnerc5, task = taskc5, 
               resampling = res_desc,show.info = FALSE)

resultc5 <- data.frame(matrix(nrow=1,ncol=length(featc5))) 

for(i in 1:length(featc5)){
  taskfeatc5 = dropFeatures(taskc5, featc5[i])
  mod.featc5 = train(learnerc5, dropFeatures(taskc5, featc5[i]))
  resfeatc5 = resample(learner = learnerc5, task = taskfeatc5, resampling = res_desc ,show.info = FALSE);
  importance_c5 = data.frame(resfeatc5$aggr/resc5$aggr)
  featurec5 = c(getTaskFeatureNames(taskc5))
  resultc5[i] <- importance_c5
  #print(importance)
}
resultc5
colnames(resultc5) <- getTaskFeatureNames(taskc5)
rownames(resultc5) <- "importance_c5"
FIPc5 <- data.frame(
  importancec5= round((t(resultc5)),3),
  featurec5= getTaskFeatureNames(taskc5))


dc75 <- data_c75
dc75["y1c75"] <- y1c75
taskc75 = makeRegrTask(data = dc75 , target= "y1c75")
learnerc75 = makeLearner("regr.randomForest")
featc75 = getTaskFeatureNames(taskc75)
featc75
mod.fullc75 = train(learnerc75, taskc75)
resc75 = resample(learner = learnerc75, task = taskc75, 
               resampling = res_desc,show.info = FALSE)

resultc75 <- data.frame(matrix(nrow=1,ncol=length(featc75))) 

for(i in 1:length(featc75)){
  taskfeatc75 = dropFeatures(taskc75, featc75[i])
  mod.featc75 = train(learnerc75, dropFeatures(taskc75, featc75[i]))
  resfeatc75 = resample(learner = learnerc75, task = taskfeatc75, resampling = res_desc ,show.info = FALSE);
  importance_c75 = data.frame(resfeatc75$aggr/resc75$aggr)
  featurec75 = c(getTaskFeatureNames(taskc75))
  resultc75[i] <- importance_c75
  #print(importance)
}
resultc75
colnames(resultc75) <- getTaskFeatureNames(taskc75)
rownames(resultc75) <- "importance_c75"
FIPc75 <- data.frame(
  importancec75= round((t(resultc75)),3),
  featurec75= getTaskFeatureNames(taskc75))

dc <- data_c
dc["y1c"] <- y1c
taskc = makeRegrTask(data = dc , target= "y1c")
learnerc = makeLearner("regr.randomForest")
featc = getTaskFeatureNames(taskc)
featc
mod.fullc = train(learnerc, taskc)
resc = resample(learner = learnerc, task = taskc, 
               resampling = res_desc,show.info = FALSE)

resultc <- data.frame(matrix(nrow=1,ncol=length(featc))) 

for(i in 1:length(featc)){
  taskfeatc = dropFeatures(taskc, featc[i])
  mod.featc = train(learnerc, dropFeatures(taskc, featc[i]))
  resfeatc = resample(learner = learnerc, task = taskfeatc, resampling = res_desc ,show.info = FALSE);
  importance_c = data.frame(resfeatc$aggr/resc$aggr)
  featurec = c(getTaskFeatureNames(taskc))
  resultc[i] <- importance_c
  #print(importance)
}
resultc
colnames(resultc) <- getTaskFeatureNames(taskc)
rownames(resultc) <- "importance_c5"
FIPc <- data.frame(
  importancec= round((t(resultc)),3),
  featurec= getTaskFeatureNames(taskc))

ploco <- ggplot(data = FIPu ,aes(x=featureu,y = importance_u, colour = "0")) + #### hier fehler 
  geom_point(mapping=aes(x=featureu, y=importance_u), size=0, shape=21,stroke = 3, fill="white") +
  geom_point(mapping=aes(x=FIPc25$featurec25, y=FIPc25$importance_c25, colour = "0.25"), size=0,stroke = 3, shape=21, fill="white", show.legend = TRUE)+
  geom_point(mapping=aes(x=FIPc5$featurec5, y=FIPc5$importance_c5, colour = "0.5"), size=0,stroke = 3, shape=21, fill="white", show.legend = TRUE)+
  geom_point(mapping=aes(x=FIPc75$featurec75, y=FIPc75$importance_c75, colour = "0.75"), size=0,stroke = 3, shape=21, fill="white", show.legend = TRUE)+
  geom_point(mapping=aes(x=FIPc$featurec, y=FIPc$importance_c, colour = "0.99"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
  coord_flip() +
  theme(plot.subtitle = element_text(size = 10)) +
  labs(y = "Feature Importance MSE", x= "") + 
  theme(text = element_text(size = 14))+
  scale_color_manual(labels = c("0","0.25","0.5","0.75","0.99"), values=c("0" = "white", "0.25" = "lightblue", "0.5"= "blue", "0.75"="darkblue", "0.99" = "black"), name = "Correlation Intensity",breaks=c("0", "0.25", "0.5", "0.75", "0.99"))+
  ggtitle("LOCO on Random Forest")

ploco
```




```{r LOCO04, message = FALSE, fig.height=6, fig.width=12, fig.cap="LOCO Feature Importance with different kind of correlation intensity on a linear model"}
library(mlr)
library(iml)
library(ggplot2)
library(randomForest)

res_desc <- makeResampleDesc("Subsample", iters = 5, split = 4/5)

du <- data_u
du["y1u"] <- y1u
tasku = makeRegrTask(data = du , target= "y1u")
learneru = makeLearner("regr.lm")
featu = getTaskFeatureNames(tasku)
featu
mod.fullu = train(learneru, tasku)
resu = resample(learner = learneru, task = tasku, 
               resampling = res_desc,show.info = FALSE)

resultu <- data.frame(matrix(nrow=1,ncol=length(featu))) 

for(i in 1:length(featu)){
  taskfeatu = dropFeatures(tasku, featu[i])
  mod.featu = train(learneru, dropFeatures(tasku, featu[i]))
  resfeatu = resample(learner = learneru, task = taskfeatu, resampling = res_desc ,show.info = FALSE);
  importance_u = data.frame(resfeatu$aggr/resu$aggr)
  featureu = c(getTaskFeatureNames(tasku))
  resultu[i] <- importance_u
  #print(importance)
}
resultu
colnames(resultu) <- getTaskFeatureNames(tasku)
rownames(resultu) <- "importance_u"
FIPulm <- data.frame(
  importanceu= round((t(resultu)),3),
  featureu= getTaskFeatureNames(tasku))


dc25 <- data_c25
dc25["y1c25"] <- y1c25
taskc25 = makeRegrTask(data = dc25 , target= "y1c25")
learnerc25 = makeLearner("regr.lm")
featc25 = getTaskFeatureNames(taskc25)
featc25
mod.fullc25 = train(learnerc25, taskc25)
resc25 = resample(learner = learnerc25, task = taskc25, 
               resampling = res_desc,show.info = FALSE)

resultc25 <- data.frame(matrix(nrow=1,ncol=length(featc25))) 

for(i in 1:length(featc25)){
  taskfeatc25 = dropFeatures(taskc25, featc25[i])
  mod.featc25 = train(learnerc25, dropFeatures(taskc25, featc25[i]))
  resfeatc25 = resample(learner = learnerc25, task = taskfeatc25, resampling = res_desc ,show.info = FALSE);
  importance_c25 = data.frame(resfeatc25$aggr/resc25$aggr)
  featurec25 = c(getTaskFeatureNames(taskc25))
  resultc25[i] <- importance_c25
  #print(importance)
}
resultc25
colnames(resultc25) <- getTaskFeatureNames(taskc25)
rownames(resultc25) <- "importance_c25"
FIPc25lm <- data.frame(
  importancec25= round((t(resultc25)),3),
  featurec25= getTaskFeatureNames(taskc25))


dc5 <- data_c5
dc5["y1c5"] <- y1c5
taskc5 = makeRegrTask(data = dc5 , target= "y1c5")
learnerc5 = makeLearner("regr.lm")
featc5 = getTaskFeatureNames(taskc5)
featc5
mod.fullc5 = train(learnerc5, taskc5)
resc5 = resample(learner = learnerc5, task = taskc5, 
               resampling = res_desc,show.info = FALSE)

resultc5 <- data.frame(matrix(nrow=1,ncol=length(featc5))) 

for(i in 1:length(featc5)){
  taskfeatc5 = dropFeatures(taskc5, featc5[i])
  mod.featc5 = train(learnerc5, dropFeatures(taskc5, featc5[i]))
  resfeatc5 = resample(learner = learnerc5, task = taskfeatc5, resampling = res_desc ,show.info = FALSE);
  importance_c5 = data.frame(resfeatc5$aggr/resc5$aggr)
  featurec5 = c(getTaskFeatureNames(taskc5))
  resultc5[i] <- importance_c5
  #print(importance)
}
resultc5
colnames(resultc5) <- getTaskFeatureNames(taskc5)
rownames(resultc5) <- "importance_c5"
FIPc5lm <- data.frame(
  importancec5= round((t(resultc5)),3),
  featurec5= getTaskFeatureNames(taskc5))


dc75 <- data_c75
dc75["y1c75"] <- y1c75
taskc75 = makeRegrTask(data = dc75 , target= "y1c75")
learnerc75 = makeLearner("regr.lm")
featc75 = getTaskFeatureNames(taskc75)
featc75
mod.fullc75 = train(learnerc75, taskc75)
resc75 = resample(learner = learnerc75, task = taskc75, 
               resampling = res_desc,show.info = FALSE)

resultc75 <- data.frame(matrix(nrow=1,ncol=length(featc75))) 

for(i in 1:length(featc75)){
  taskfeatc75 = dropFeatures(taskc75, featc75[i])
  mod.featc75 = train(learnerc75, dropFeatures(taskc75, featc75[i]))
  resfeatc75 = resample(learner = learnerc75, task = taskfeatc75, resampling = res_desc ,show.info = FALSE);
  importance_c75 = data.frame(resfeatc75$aggr/resc75$aggr)
  featurec75 = c(getTaskFeatureNames(taskc75))
  resultc75[i] <- importance_c75
  #print(importance)
}
resultc75
colnames(resultc75) <- getTaskFeatureNames(taskc75)
rownames(resultc75) <- "importance_c75"
FIPc75lm <- data.frame(
  importancec75= round((t(resultc75)),3),
  featurec75= getTaskFeatureNames(taskc75))

dc <- data_c
dc["y1c"] <- y1c
taskc = makeRegrTask(data = dc , target= "y1c")
learnerc = makeLearner("regr.lm")
featc = getTaskFeatureNames(taskc)
featc
mod.fullc = train(learnerc, taskc)
resc = resample(learner = learnerc, task = taskc, 
               resampling = res_desc,show.info = FALSE)

resultc <- data.frame(matrix(nrow=1,ncol=length(featc))) 

for(i in 1:length(featc)){
  taskfeatc = dropFeatures(taskc, featc[i])
  mod.featc = train(learnerc, dropFeatures(taskc, featc[i]))
  resfeatc = resample(learner = learnerc, task = taskfeatc, resampling = res_desc ,show.info = FALSE);
  importance_c = data.frame(resfeatc$aggr/resc$aggr)
  featurec = c(getTaskFeatureNames(taskc))
  resultc[i] <- importance_c
  #print(importance)
}
resultc
colnames(resultc) <- getTaskFeatureNames(taskc)
rownames(resultc) <- "importance_c5"
FIPclm <- data.frame(
  importancec= round((t(resultc)),3),
  featurec= getTaskFeatureNames(taskc))

plocolm <- ggplot(data = FIPulm ,aes(x=featureu,y = importance_u, colour = "0"))+ 
  geom_point(mapping=aes(x=featureu, y=importance_u), size=0, stroke =3, shape=21, fill="white") +
  geom_point(mapping=aes(x=FIPc25lm$featurec25, y=FIPc25lm$importance_c25, colour = "0.25"), size=0,stroke = 3, shape=21, fill="white", show.legend = TRUE)+
  geom_point(mapping=aes(x=FIPc5lm$featurec5, y=FIPc5lm$importance_c5, colour = "0.5"), size=0,stroke = 3, shape=21, fill="white", show.legend = TRUE)+
  geom_point(mapping=aes(x=FIPc75lm$featurec75, y=FIPc75lm$importance_c75, colour = "0.75"), size=0,stroke = 3, shape=21, fill="white", show.legend = TRUE)+
  geom_point(mapping=aes(x=FIPclm$featurec, y=FIPclm$importance_c, colour = "0.99"), size=0,stroke = 3, shape=21, fill="white", show.legend = TRUE)+
  coord_flip() +
  theme(plot.subtitle = element_text(size = 10)) +
  labs(y = "Feature Importance MSE", x= "") + 
  theme(text = element_text(size = 14))+
  scale_color_manual(labels = c("0","0.25","0.5","0.75","0.99"), values=c("0" = "white", "0.25" = "lightblue", "0.5"= "blue", "0.75"="darkblue", "0.99" = "black"), name = "Correlation Intensity",breaks=c("0", "0.25", "0.5", "0.75", "0.99"))+
  ggtitle("LOCO on Linear Model")

plocolm
```


```{r arrange02, message = FALSE, fig.height=6, fig.width=12, fig.cap= "Comparison"}
library(ggpubr)
ggarrange(prf, plm, ploco, plocolm , ncol=2, nrow = 2, legend = "bottom", common.legend = TRUE)
``` 


**3) Uninformative Feature:**
Uninformative which means that one feature (in this case X4) is considered for the dependence on Y, but not considered for the model. So here it is the same case as in 1)

```{r szenariouif, message = FALSE, fig.height=6, fig.width=12, fig.cap= "Scenario Linear Dependence with uninformative feature 4"}

sd_u <- mean(data_u$X1 + data_u$X2 + data_u$X3 + data_u$X4)*0.1

sd_c25 <- mean(data_c25$X1 + data_c25$X2 + data_c25$X3 + data_c25$X4)*0.1

sd_c5 <- mean(data_c5$X1 + data_c5$X2 + data_c5$X3 + data_c5$X4)*0.1

sd_c75 <- mean(data_c75$X1 + data_c75$X2 + data_c75$X3 + data_c75$X4)*0.1

sd_c <- mean(data_c$X1 + data_c$X2 + data_c$X3 + data_c$X4)*0.1



y1u <- data_u$X1 + data_u$X2 + data_u$X3 + data_u$X4 + rnorm(n = 1, mean = 0, sd = sd_u) 

y1c25 <- data_c25$X1 + data_c25$X2 + data_c25$X3 + data_c25$X4 + rnorm(n = 1, mean = 0, sd = sd_c25)

y1c5 <- data_c5$X1 + data_c5$X2 + data_c5$X3 + data_c5$X4 + rnorm(n = 1, mean = 0, sd = sd_c5)

y1c75 <- data_c75$X1 + data_c75$X2 + data_c75$X3 + data_c75$X4 + rnorm(n = 1, mean = 0, sd = sd_c75)

y1c <- data_c$X1 + data_c$X2 + data_c$X3 + data_c$X4 + rnorm(n = 1, mean = 0, sd = sd_c) 

#The choice of noise variance should be hold small in order to make the behavior we observe clearer and there will be no misinterpretation. (umschreiben)
```

```{r PFI05, message = FALSE, fig.height=6, fig.width=12, fig.cap="PFI with different correlations of features 1 and 2 on a random forest model"}

library(mlr)
library(iml)
library(ggplot2)
library(randomForest)

rfu <- randomForest(y1u~ data_u$X1 + data_u$X2 + data_u$X3 , data = data_u)
rf25 <- randomForest(y1c25~ data_c25$X1 + data_c25$X2 + data_c25$X3 , data = data_c25)
rf5 <- randomForest(y1c5~ data_c5$X1 + data_c5$X2 + data_c5$X3, data = data_c5)
rf75 <- randomForest(y1c75~ data_c75$X1 + data_c75$X2 + data_c75$X3 , data = data_c75)
rfc <- randomForest(y1c~ data_c$X1 + data_c$X2 + data_c$X3 , data = data_c)

modrfu <- Predictor$new(rfu, data=data_u, y = y1u)
modrf25 <- Predictor$new(rf25, data=data_c25, y = y1c25)
modrf5 <- Predictor$new(rf5, data=data_c5, y = y1c5)
modrf75 <- Predictor$new(rf75, data=data_c75, y = y1c75)
modrfc <- Predictor$new(rfc, data=data_c, y = y1c)

imprfu <- FeatureImp$new(modrfu, loss = "mse" , compare = "ratio")
imprfc25 <- FeatureImp$new(modrf25, loss = "mse" , compare = "ratio")
imprfc5 <- FeatureImp$new(modrf5, loss = "mse" , compare = "ratio")
imprfc75 <- FeatureImp$new(modrf75, loss = "mse" , compare = "ratio")
imprfc <- FeatureImp$new(modrfc, loss = "mse" , compare = "ratio")

imp.datrfu <- imprfu$results
imp.datrfc25 <- imprfc25$results
imp.datrfc5 <- imprfc5$results
imp.datrfc75 <- imprfc75$results
imp.datrfc <- imprfc$results

prf <- ggplot(imp.datrfu, aes(x=feature,y= importance, colour = "0"))+
    geom_point(mapping=aes(x=feature, y=importance), size=0, stroke =3, shape=21, fill="white") +
    geom_point(mapping=aes(x=imp.datrfc25$feature, y=imp.datrfc25$importance, colour = "0.25"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
    geom_point(mapping=aes(x=imp.datrfc5$feature, y=imp.datrfc5$importance, colour = "0.5"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
    geom_point(mapping=aes(x=imp.datrfc75$feature, y=imp.datrfc75$importance, colour = "0.75"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
    geom_point(mapping=aes(x=imp.datrfc$feature, y=imp.datrfc$importance, colour = "0.99"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
    coord_flip() +
    theme(plot.subtitle = element_text(size = 10)) +
    theme(text = element_text(size = 14))+
    labs(y = "Feature Importance MSE", x= "") +
    scale_color_manual(labels = c("0","0.25","0.5","0.75","0.99"), values=c("0" = "white", "0.25" = "lightblue", "0.5"= "blue", "0.75"="darkblue", "0.99" = "black"), name = "Correlation Intensity",breaks=c("0", "0.25", "0.5", "0.75", "0.99"))+
    ggtitle("PFI on Random Forest")

prf
```

```{r PFI06, message = FALSE, fig.height=6, fig.width=12, fig.cap="PFI with different correlations of features 1 and 2 on a linear model"}

library(mlr)
library(iml)
library(ggplot2)
library(randomForest)

lmu <- lm(y1u~., data=data_u)
lmc25 <- lm(y1c25~., data = data_c25)
lmc5 <- lm(y1c5~., data = data_c5)
lmc75 <- lm(y1c75~., data = data_c75)
lmc <- lm(y1c~., data = data_c)

modlmu <- Predictor$new(lmu, data=data_u, y = y1u)
modlmc25 <- Predictor$new(lmc25, data=data_c25, y = y1c25)
modlmc5 <- Predictor$new(lmc5, data=data_c5, y = y1c5)
modlmc75 <- Predictor$new(lmc75, data=data_c75, y = y1c75)
modlmc <- Predictor$new(lmc, data=data_c, y = y1c)

implmu <- FeatureImp$new(modlmu, loss = "mse" , compare = "ratio")
implmc25 <- FeatureImp$new(modlmc25, loss = "mse" , compare = "ratio")
implmc5 <- FeatureImp$new(modlmc5, loss = "mse" , compare = "ratio")
implmc75 <- FeatureImp$new(modlmc75, loss = "mse" , compare = "ratio")
implmc <- FeatureImp$new(modlmc, loss = "mse" , compare = "ratio")

imp.datlmu <- implmu$results
imp.datlmc25 <- implmc25$results
imp.datlmc5 <- implmc5$results
imp.datlmc75 <- implmc75$results
imp.datlmc <- implmc$results

plm <- ggplot(imp.datlmu, aes(x=feature, y= importance, colour = "0")) +
    geom_point(mapping=aes(x=feature, y=importance), size=0, stroke = 3, shape=21, fill="white") +
    geom_point(mapping=aes(x=imp.datlmc25$feature, y=imp.datlmc25$importance, colour = "0.25"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
    geom_point(mapping=aes(x=imp.datlmc5$feature, y=imp.datlmc5$importance, colour = "0.5"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
    geom_point(mapping=aes(x=imp.datlmc75$feature, y=imp.datlmc75$importance, colour = "0.75"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
    geom_point(mapping=aes(x=imp.datlmc$feature, y=imp.datlmc$importance, colour = "0.99"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
    coord_flip() +
    theme(plot.subtitle = element_text(size = 10)) +
    labs(y = "Feature Importance MSE", x= "") + 
    theme(text = element_text(size = 14))+
    scale_color_manual(labels = c("0","0.25","0.5","0.75","0.99"), values=c("0" = "white", "0.25" = "lightblue", "0.5"= "blue", "0.75"="darkblue", "0.99" = "black"), name = "Correlation Intensity",breaks=c("0", "0.25", "0.5", "0.75", "0.99"))+
    ggtitle("PFI on Linear Model")

plm

```



```{r LOCO05, message = FALSE, fig.height=6, fig.width=12, fig.cap="LOCO Feature Importance with different kind of correlation intensity on a random forest model"}
library(mlr)
library(iml)
library(ggplot2)
library(randomForest)

res_desc <- makeResampleDesc("Subsample", iters = 5, split = 4/5)

du <- data_u
du["y1u"] <- y1u
tasku = makeRegrTask(data = du , target= "y1u")
learneru = makeLearner("regr.randomForest")
featu = getTaskFeatureNames(tasku)
featu
mod.fullu = train(learneru, tasku)
resu = resample(learner = learneru, task = tasku, 
               resampling = res_desc,show.info = FALSE)

resultu <- data.frame(matrix(nrow=1,ncol=length(featu))) 

for(i in 1:length(featu)){
  taskfeatu = dropFeatures(tasku, featu[i])
  mod.featu = train(learneru, dropFeatures(tasku, featu[i]))
  resfeatu = resample(learner = learneru, task = taskfeatu, resampling = res_desc ,show.info = FALSE);
  importance_u = data.frame(resfeatu$aggr/resu$aggr)
  featureu = c(getTaskFeatureNames(tasku))
  resultu[i] <- importance_u
  #print(importance)
}
resultu
colnames(resultu) <- getTaskFeatureNames(tasku)
rownames(resultu) <- "importance_u"
FIPu <- data.frame(
  importanceu= round((t(resultu)),3),
  featureu= getTaskFeatureNames(tasku))


dc25 <- data_c25
dc25["y1c25"] <- y1c25
taskc25 = makeRegrTask(data = dc25 , target= "y1c25")
learnerc25 = makeLearner("regr.randomForest")
featc25 = getTaskFeatureNames(taskc25)
featc25
mod.fullc25 = train(learnerc25, taskc25)
resc25 = resample(learner = learnerc25, task = taskc25, 
               resampling = res_desc,show.info = FALSE)

resultc25 <- data.frame(matrix(nrow=1,ncol=length(featc25))) 

for(i in 1:length(featc25)){
  taskfeatc25 = dropFeatures(taskc25, featc25[i])
  mod.featc25 = train(learnerc25, dropFeatures(taskc25, featc25[i]))
  resfeatc25 = resample(learner = learnerc25, task = taskfeatc25, resampling = res_desc ,show.info = FALSE);
  importance_c25 = data.frame(resfeatc25$aggr/resc25$aggr)
  featurec25 = c(getTaskFeatureNames(taskc25))
  resultc25[i] <- importance_c25
  #print(importance)
}
resultc25
colnames(resultc25) <- getTaskFeatureNames(taskc25)
rownames(resultc25) <- "importance_c25"
FIPc25 <- data.frame(
  importancec25= round((t(resultc25)),3),
  featurec25= getTaskFeatureNames(taskc25))


dc5 <- data_c5
dc5["y1c5"] <- y1c5
taskc5 = makeRegrTask(data = dc5 , target= "y1c5")
learnerc5 = makeLearner("regr.randomForest")
featc5 = getTaskFeatureNames(taskc5)
featc5
mod.fullc5 = train(learnerc5, taskc5)
resc5 = resample(learner = learnerc5, task = taskc5, 
               resampling = res_desc,show.info = FALSE)

resultc5 <- data.frame(matrix(nrow=1,ncol=length(featc5))) 

for(i in 1:length(featc5)){
  taskfeatc5 = dropFeatures(taskc5, featc5[i])
  mod.featc5 = train(learnerc5, dropFeatures(taskc5, featc5[i]))
  resfeatc5 = resample(learner = learnerc5, task = taskfeatc5, resampling = res_desc ,show.info = FALSE);
  importance_c5 = data.frame(resfeatc5$aggr/resc5$aggr)
  featurec5 = c(getTaskFeatureNames(taskc5))
  resultc5[i] <- importance_c5
  #print(importance)
}
resultc5
colnames(resultc5) <- getTaskFeatureNames(taskc5)
rownames(resultc5) <- "importance_c5"
FIPc5 <- data.frame(
  importancec5= round((t(resultc5)),3),
  featurec5= getTaskFeatureNames(taskc5))


dc75 <- data_c75
dc75["y1c75"] <- y1c75
taskc75 = makeRegrTask(data = dc75 , target= "y1c75")
learnerc75 = makeLearner("regr.randomForest")
featc75 = getTaskFeatureNames(taskc75)
featc75
mod.fullc75 = train(learnerc75, taskc75)
resc75 = resample(learner = learnerc75, task = taskc75, 
               resampling = res_desc,show.info = FALSE)

resultc75 <- data.frame(matrix(nrow=1,ncol=length(featc75))) 

for(i in 1:length(featc75)){
  taskfeatc75 = dropFeatures(taskc75, featc75[i])
  mod.featc75 = train(learnerc75, dropFeatures(taskc75, featc75[i]))
  resfeatc75 = resample(learner = learnerc75, task = taskfeatc75, resampling = res_desc ,show.info = FALSE);
  importance_c75 = data.frame(resfeatc75$aggr/resc75$aggr)
  featurec75 = c(getTaskFeatureNames(taskc75))
  resultc75[i] <- importance_c75
  #print(importance)
}
resultc75
colnames(resultc75) <- getTaskFeatureNames(taskc75)
rownames(resultc75) <- "importance_c75"
FIPc75 <- data.frame(
  importancec75= round((t(resultc75)),3),
  featurec75= getTaskFeatureNames(taskc75))

dc <- data_c
dc["y1c"] <- y1c
taskc = makeRegrTask(data = dc , target= "y1c")
learnerc = makeLearner("regr.randomForest")
featc = getTaskFeatureNames(taskc)
featc
mod.fullc = train(learnerc, taskc)
resc = resample(learner = learnerc, task = taskc, 
               resampling = res_desc,show.info = FALSE)

resultc <- data.frame(matrix(nrow=1,ncol=length(featc))) 

for(i in 1:length(featc)){
  taskfeatc = dropFeatures(taskc, featc[i])
  mod.featc = train(learnerc, dropFeatures(taskc, featc[i]))
  resfeatc = resample(learner = learnerc, task = taskfeatc, resampling = res_desc ,show.info = FALSE);
  importance_c = data.frame(resfeatc$aggr/resc$aggr)
  featurec = c(getTaskFeatureNames(taskc))
  resultc[i] <- importance_c
  #print(importance)
}
resultc
colnames(resultc) <- getTaskFeatureNames(taskc)
rownames(resultc) <- "importance_c5"
FIPc <- data.frame(
  importancec= round((t(resultc)),3),
  featurec= getTaskFeatureNames(taskc))

ploco <- ggplot(data = FIPu ,aes(x=featureu,y = importance_u, colour = "0")) + #### hier fehler 
  geom_point(mapping=aes(x=featureu, y=importance_u), size=0, shape=21,stroke = 3, fill="white") +
  geom_point(mapping=aes(x=FIPc25$featurec25, y=FIPc25$importance_c25, colour = "0.25"), size=0,stroke = 3, shape=21, fill="white", show.legend = TRUE)+
  geom_point(mapping=aes(x=FIPc5$featurec5, y=FIPc5$importance_c5, colour = "0.5"), size=0,stroke = 3, shape=21, fill="white", show.legend = TRUE)+
  geom_point(mapping=aes(x=FIPc75$featurec75, y=FIPc75$importance_c75, colour = "0.75"), size=0,stroke = 3, shape=21, fill="white", show.legend = TRUE)+
  geom_point(mapping=aes(x=FIPc$featurec, y=FIPc$importance_c, colour = "0.99"), size=0, shape=21,stroke = 3, fill="white", show.legend = TRUE)+
  coord_flip() +
  theme(plot.subtitle = element_text(size = 10)) +
  labs(y = "Feature Importance MSE", x= "") + 
  theme(text = element_text(size = 14))+
  scale_color_manual(labels = c("0","0.25","0.5","0.75","0.99"), values=c("0" = "white", "0.25" = "lightblue", "0.5"= "blue", "0.75"="darkblue", "0.99" = "black"), name = "Correlation Intensity",breaks=c("0", "0.25", "0.5", "0.75", "0.99"))+
  ggtitle("LOCO on Random Forest")

ploco
```




```{r LOCO06, message = FALSE, fig.height=6, fig.width=12, fig.cap="LOCO Feature Importance with different kind of correlation intensity on a linear model"}
library(mlr)
library(iml)
library(ggplot2)
library(randomForest)

res_desc <- makeResampleDesc("Subsample", iters = 5, split = 4/5)

du <- data_u
du["y1u"] <- y1u
tasku = makeRegrTask(data = du , target= "y1u")
learneru = makeLearner("regr.lm")
featu = getTaskFeatureNames(tasku)
featu
mod.fullu = train(learneru, tasku)
resu = resample(learner = learneru, task = tasku, 
               resampling = res_desc,show.info = FALSE)

resultu <- data.frame(matrix(nrow=1,ncol=length(featu))) 

for(i in 1:length(featu)){
  taskfeatu = dropFeatures(tasku, featu[i])
  mod.featu = train(learneru, dropFeatures(tasku, featu[i]))
  resfeatu = resample(learner = learneru, task = taskfeatu, resampling = res_desc ,show.info = FALSE);
  importance_u = data.frame(resfeatu$aggr/resu$aggr)
  featureu = c(getTaskFeatureNames(tasku))
  resultu[i] <- importance_u
  #print(importance)
}
resultu
colnames(resultu) <- getTaskFeatureNames(tasku)
rownames(resultu) <- "importance_u"
FIPulm <- data.frame(
  importanceu= round((t(resultu)),3),
  featureu= getTaskFeatureNames(tasku))


dc25 <- data_c25
dc25["y1c25"] <- y1c25
taskc25 = makeRegrTask(data = dc25 , target= "y1c25")
learnerc25 = makeLearner("regr.lm")
featc25 = getTaskFeatureNames(taskc25)
featc25
mod.fullc25 = train(learnerc25, taskc25)
resc25 = resample(learner = learnerc25, task = taskc25, 
               resampling = res_desc,show.info = FALSE)

resultc25 <- data.frame(matrix(nrow=1,ncol=length(featc25))) 

for(i in 1:length(featc25)){
  taskfeatc25 = dropFeatures(taskc25, featc25[i])
  mod.featc25 = train(learnerc25, dropFeatures(taskc25, featc25[i]))
  resfeatc25 = resample(learner = learnerc25, task = taskfeatc25, resampling = res_desc ,show.info = FALSE);
  importance_c25 = data.frame(resfeatc25$aggr/resc25$aggr)
  featurec25 = c(getTaskFeatureNames(taskc25))
  resultc25[i] <- importance_c25
  #print(importance)
}
resultc25
colnames(resultc25) <- getTaskFeatureNames(taskc25)
rownames(resultc25) <- "importance_c25"
FIPc25lm <- data.frame(
  importancec25= round((t(resultc25)),3),
  featurec25= getTaskFeatureNames(taskc25))


dc5 <- data_c5
dc5["y1c5"] <- y1c5
taskc5 = makeRegrTask(data = dc5 , target= "y1c5")
learnerc5 = makeLearner("regr.lm")
featc5 = getTaskFeatureNames(taskc5)
featc5
mod.fullc5 = train(learnerc5, taskc5)
resc5 = resample(learner = learnerc5, task = taskc5, 
               resampling = res_desc,show.info = FALSE)

resultc5 <- data.frame(matrix(nrow=1,ncol=length(featc5))) 

for(i in 1:length(featc5)){
  taskfeatc5 = dropFeatures(taskc5, featc5[i])
  mod.featc5 = train(learnerc5, dropFeatures(taskc5, featc5[i]))
  resfeatc5 = resample(learner = learnerc5, task = taskfeatc5, resampling = res_desc ,show.info = FALSE);
  importance_c5 = data.frame(resfeatc5$aggr/resc5$aggr)
  featurec5 = c(getTaskFeatureNames(taskc5))
  resultc5[i] <- importance_c5
  #print(importance)
}
resultc5
colnames(resultc5) <- getTaskFeatureNames(taskc5)
rownames(resultc5) <- "importance_c5"
FIPc5lm <- data.frame(
  importancec5= round((t(resultc5)),3),
  featurec5= getTaskFeatureNames(taskc5))


dc75 <- data_c75
dc75["y1c75"] <- y1c75
taskc75 = makeRegrTask(data = dc75 , target= "y1c75")
learnerc75 = makeLearner("regr.lm")
featc75 = getTaskFeatureNames(taskc75)
featc75
mod.fullc75 = train(learnerc75, taskc75)
resc75 = resample(learner = learnerc75, task = taskc75, 
               resampling = res_desc,show.info = FALSE)

resultc75 <- data.frame(matrix(nrow=1,ncol=length(featc75))) 

for(i in 1:length(featc75)){
  taskfeatc75 = dropFeatures(taskc75, featc75[i])
  mod.featc75 = train(learnerc75, dropFeatures(taskc75, featc75[i]))
  resfeatc75 = resample(learner = learnerc75, task = taskfeatc75, resampling = res_desc ,show.info = FALSE);
  importance_c75 = data.frame(resfeatc75$aggr/resc75$aggr)
  featurec75 = c(getTaskFeatureNames(taskc75))
  resultc75[i] <- importance_c75
  #print(importance)
}
resultc75
colnames(resultc75) <- getTaskFeatureNames(taskc75)
rownames(resultc75) <- "importance_c75"
FIPc75lm <- data.frame(
  importancec75= round((t(resultc75)),3),
  featurec75= getTaskFeatureNames(taskc75))

dc <- data_c
dc["y1c"] <- y1c
taskc = makeRegrTask(data = dc , target= "y1c")
learnerc = makeLearner("regr.lm")
featc = getTaskFeatureNames(taskc)
featc
mod.fullc = train(learnerc, taskc)
resc = resample(learner = learnerc, task = taskc, 
               resampling = res_desc,show.info = FALSE)

resultc <- data.frame(matrix(nrow=1,ncol=length(featc))) 

for(i in 1:length(featc)){
  taskfeatc = dropFeatures(taskc, featc[i])
  mod.featc = train(learnerc, dropFeatures(taskc, featc[i]))
  resfeatc = resample(learner = learnerc, task = taskfeatc, resampling = res_desc ,show.info = FALSE);
  importance_c = data.frame(resfeatc$aggr/resc$aggr)
  featurec = c(getTaskFeatureNames(taskc))
  resultc[i] <- importance_c
  #print(importance)
}
resultc
colnames(resultc) <- getTaskFeatureNames(taskc)
rownames(resultc) <- "importance_c5"
FIPclm <- data.frame(
  importancec= round((t(resultc)),3),
  featurec= getTaskFeatureNames(taskc))

plocolm <- ggplot(data = FIPulm ,aes(x=featureu,y = importance_u, colour = "0"))+ 
  geom_point(mapping=aes(x=featureu, y=importance_u), size=0, stroke =3, shape=21, fill="white") +
  geom_point(mapping=aes(x=FIPc25lm$featurec25, y=FIPc25lm$importance_c25, colour = "0.25"), size=0,stroke = 3, shape=21, fill="white", show.legend = TRUE)+
  geom_point(mapping=aes(x=FIPc5lm$featurec5, y=FIPc5lm$importance_c5, colour = "0.5"), size=0,stroke = 3, shape=21, fill="white", show.legend = TRUE)+
  geom_point(mapping=aes(x=FIPc75lm$featurec75, y=FIPc75lm$importance_c75, colour = "0.75"), size=0,stroke = 3, shape=21, fill="white", show.legend = TRUE)+
  geom_point(mapping=aes(x=FIPclm$featurec, y=FIPclm$importance_c, colour = "0.99"), size=0,stroke = 3, shape=21, fill="white", show.legend = TRUE)+
  coord_flip() +
  theme(plot.subtitle = element_text(size = 10)) +
  labs(y = "Feature Importance MSE", x= "") + 
  theme(text = element_text(size = 14))+
  scale_color_manual(labels = c("0","0.25","0.5","0.75","0.99"), values=c("0" = "white", "0.25" = "lightblue", "0.5"= "blue", "0.75"="darkblue", "0.99" = "black"), name = "Correlation Intensity",breaks=c("0", "0.25", "0.5", "0.75", "0.99"))+
  ggtitle("LOCO on Linear Model")

plocolm
```

```{r arrange03, message = FALSE, fig.height=6, fig.width=12, fig.cap= "Comparison"}
library(ggpubr)
ggarrange(prf, plm, ploco, plocolm , ncol=2, nrow = 2, legend = "bottom", common.legend = TRUE)
``` 



**4) Simulation with high amount of correlated data**

```{r scenariohac, message = FALSE, fig.height=6, fig.width=12, fig.cap="Simulation of 10 correlated features (r=0.99) "}

library(mvtnorm)

set.seed(456)

sigma_sim10 <- diag(1, nrow = 12)
sigma_sim10[which(sigma_sim10 != "1")] = 0.99 
sigma_sim10[11,] <- 0
sigma_sim10[,11] <- 0
sigma_sim10[12,] <- 0
sigma_sim10[,12] <- 0
sigma_sim10[11,11] <- 1
sigma_sim10[12,12] <- 1

data_sim10 <- as.data.frame(rmvnorm(n = 1000, 
                              mean = rep(0, times = 12), 
                              sigma = sigma_sim10))

data_sim10 <- as.data.frame(data_sim10)
data_sim10['random'] <- runif(nrow(data_sim10))

sigma_sim5 <- diag(1, nrow = 7)
sigma_sim5[which(sigma_sim5 != "1")] = 0.99 
sigma_sim5[6,] <- 0
sigma_sim5[,6] <- 0
sigma_sim5[7,] <- 0
sigma_sim5[,7] <- 0
sigma_sim5[6,6] <- 1
sigma_sim5[7,7] <- 1
sigma_sim5

data_sim5 <- as.data.frame(rmvnorm(n = 1000, 
                              mean = rep(0, times = 7), 
                              sigma = sigma_sim5))
head(data_sim5)


data_sim5 <- as.data.frame(data_sim5)
data_sim5['random'] <- runif(nrow(data_sim5))


colnames(data_sim5) <- c("X1", "X2", "X3", "X4","X5", "X6", "X7", "random")
colnames(data_sim10) <- c("X1", "X2", "X3", "X4","X5", "X6", "X7", "X8","X9", "X10", "X11", "X12", "random")

sd_sim5 <- mean(data_sim5$X1 + data_sim5$X2 + data_sim5$X3 + data_sim5$X4+ data_sim5$X5+ data_sim5$X6+ data_sim5$X7)*0.1

sd_sim10 <- mean(data_sim10$X1 + data_sim10$X2 + data_sim10$X3 + data_sim10$X4+ data_sim10$X5+ data_sim10$X6+ data_sim10$X7+ data_sim10$X8+ data_sim10$X9+ data_sim10$X10+ data_sim10$X11+ data_sim10$X12)*0.1

y1sim5 <- data_sim5$X1 + data_sim5$X2 + data_sim5$X3 + data_sim5$X4+ data_sim5$X5+ data_sim5$X6+ data_sim5$X7 + rnorm(n = 1, mean = 0, sd = sd_sim5) 

y1sim10 <- data_sim10$X1 + data_sim10$X2 + data_sim10$X3 + data_sim10$X4+ data_sim10$X5+ data_sim10$X6+ data_sim10$X7+ data_sim10$X8+ data_sim10$X9+ data_sim10$X10+ data_sim10$X11+ data_sim10$X12 + rnorm(n = 1, mean = 0, sd = sd_sim10)
```



```{r PFI07, message = FALSE, fig.height=6, fig.width=12, fig.cap="Simulation of 10 correlated features (r=0.99) "}

library(mlr)
library(iml)
library(ggplot2)
library(randomForest)

rfsim5 <- randomForest(y1sim5~. , data = data_sim5)
rfsim10 <- randomForest(y1sim10~. , data = data_sim10)


modrfsim5 <- Predictor$new(rfsim5, data=data_sim5, y = y1sim5)
modrfsim10 <- Predictor$new(rfsim10, data=data_sim10, y = y1sim10)

imprfsim5 <- FeatureImp$new(modrfsim5, loss = "mse" , compare = "ratio")
imprfsim10 <- FeatureImp$new(modrfsim10, loss = "mse" , compare = "ratio")


imp.datrfsim5 <- imprfsim5$results
imp.datrfsim10 <- imprfsim10$results

prfsim5 <- ggplot(imp.datrfsim5, aes(x=reorder(feature, importance), ymin=importance.05, ymax=importance.95)) +
  geom_errorbar(width=0.3, size=1, color="darkgreen") + 
    geom_point(mapping=aes(x=feature, y=importance), size=3, shape=21, fill="green") +
   coord_flip() +
    theme(plot.subtitle = element_text(size = 10)) +
    labs(y = "Feature Importance MSE", x= "") + 
    theme(text = element_text(size = 16))
prfsim5

prfsim10 <- ggplot(imp.datrfsim10, aes(x=reorder(feature, importance), ymin=importance.05, ymax=importance.95)) +
  geom_errorbar(width=0.3, size=1, color="darkred") +
    geom_point(mapping=aes(x=feature, y=importance), size=3, shape=21, fill="red")+
    coord_flip() +
    theme(plot.subtitle = element_text(size = 10)) +
    labs(y = "Feature Importance MSE", x= "") + 
    theme(text = element_text(size = 16)) 
prfsim10


mean(imp.datrfsim5$importance[1:5])
mean(imp.datrfsim10$importance[1:10])
```

```{r arrange04, message = FALSE, fig.height=6, fig.width=12, fig.cap= "Comparison"}
library(ggpubr)
ggarrange(prfsim5, prfsim10 , ncol=2, legend = "bottom", common.legend = TRUE)
``` 


```{r LOCO07, message = FALSE, fig.height=6, fig.width=12, echo = FALSE, eval=FALSE, fig.cap="LOCO Feature Importance with different kind of correlation intensity on a random forest model"}
library(mlr)
library(iml)
library(ggplot2)
library(randomForest)


res_desc <- makeResampleDesc("Subsample", iters = 5, split = 4/5)
data_sim5 ["y1sim5"] <- y1sim5
taskc = makeRegrTask(data = data_sim5  , target= "y1sim5")
learnerc = makeLearner("regr.randomForest")
featc = getTaskFeatureNames(taskc)
featc
mod.fullc = train(learnerc, taskc)
resc = resample(learner = learnerc, task = taskc, 
               resampling = res_desc,show.info = FALSE)

resultc <- data.frame(matrix(nrow=1,ncol=length(featc))) 

for(i in 1:length(featc)){
  taskfeatc = dropFeatures(taskc, featc[i])
  mod.featc = train(learnerc, dropFeatures(taskc, featc[i]))
  resfeatc = resample(learner = learnerc, task = taskfeatc, resampling = res_desc ,show.info = FALSE);
  importance_c = data.frame(resfeatc$aggr/resc$aggr)
  featurec = c(getTaskFeatureNames(taskc))
  resultc[i] <- importance_c
  #print(importance)
}
resultc
colnames(resultc) <- getTaskFeatureNames(taskc)
rownames(resultc) <- "importance_c"
FIPcsim5 <- data.frame(
  importancec= round((t(resultc)),3),
  featurec= getTaskFeatureNames(taskc))

locosim5 <- ggplot(data = FIPcsim5 ,aes(reorder(x=featurec,importance_c), y= importance_c)) + #### hier fehler 
  geom_point(size=3, shape=21, fill="green") +
  coord_flip() +
  theme(plot.subtitle = element_text(size = 10)) +
  labs(y = "Feature Importance", x= "") + 
  theme(text = element_text(size = 16))

locosim5
```

```{r LOCO07, message = FALSE, fig.height=6, fig.width=12, echo = FALSE, eval=FALSE, fig.cap="LOCO Feature Importance with different kind of correlation intensity on a random forest model"}
library(mlr)
library(iml)
library(ggplot2)
library(randomForest)


res_desc <- makeResampleDesc("Subsample", iters = 5, split = 4/5)
data_sim10 ["y1sim10"] <- y1sim10
taskc = makeRegrTask(data = data_sim10  , target= "y1sim10")
learnerc = makeLearner("regr.randomForest")
featc = getTaskFeatureNames(taskc)
featc
mod.fullc = train(learnerc, taskc)
resc = resample(learner = learnerc, task = taskc, 
               resampling = res_desc,show.info = FALSE)

resultc <- data.frame(matrix(nrow=1,ncol=length(featc))) 

for(i in 1:length(featc)){
  taskfeatc = dropFeatures(taskc, featc[i])
  mod.featc = train(learnerc, dropFeatures(taskc, featc[i]))
  resfeatc = resample(learner = learnerc, task = taskfeatc, resampling = res_desc ,show.info = FALSE);
  importance_c = data.frame(resfeatc$aggr/resc$aggr)
  featurec = c(getTaskFeatureNames(taskc))
  resultc[i] <- importance_c
  #print(importance)
}
resultc
colnames(resultc) <- getTaskFeatureNames(taskc)
rownames(resultc) <- "importance_c"
FIPcsim10 <- data.frame(
  importancec= round((t(resultc)),3),
  featurec= getTaskFeatureNames(taskc))

locosim10 <- ggplot(data = FIPcsim10 ,aes(reorder(x=featurec,importance_c), y= importance_c)) + 
  geom_point(size=3, shape=21, fill="red") +
  coord_flip() +
  theme(plot.subtitle = element_text(size = 10)) +
  labs(y = "Feature Importance", x= "") + 
  theme(text = element_text(size = 16))

locosim10
```

```{r arrange05, message = FALSE, fig.height=6, fig.width=12, fig.cap= "Comparison"}
library(ggpubr)
ggarrange(locosim5, locosim10 , ncol=2, legend = "bottom", common.legend = TRUE)
``` 



### Real Data

* Probably dataset "mtcars", because the features are highly correlated, but should be 
* Just showing the problem on a real data set to show the complex effects of correlated, real data
* But this chapter should be rather small

## Unrealistic data instances

* this chapter should be hold relativly small (less weight than 2nd chapter), because the emphasis is on the chapter "The Effect on Feature Importance by Adding Correlated Features"

### Real Data

## Prevention of Correlation Problems

In summary, when features of a given data set are correlated, feature importance measures like LOCO or PFI can be strongly misleading. Thus, a check for correlated features before a usage of these two methods is recommended or in other words even necessary to have a credible interpretation. 

* Conditional variable importance (Strobl)

