# ALE Intervals, Piece-Wise Constant Models and Categorical Features

*Author: Nikolas Fritz*



As mentioned in the former section the choice of intervals and starting value $z_{0,j}$ have both a certain influence on the estimated ALE - curve. While the main influence of $z_{0,j}$ is canceld out by centering the ALE, the choice of intervals stays crucial. Therefore the next section is dedicated to this topic. 

## How to choose the number and/or length of the intervals

Before investigating the choice of intervals one should be clear about in how far they influence the estimation. On the one hand for a given interval the ALE estimation will be linear due to the expected constant effect within this interval.
Remember that within each interval the local effect within this interval was calculated by the mean total difference of the prediction when shifting the variable of interest from the lower interval boundary to the upper one. This leads by definition to a constant effect within this interval that results in a linear function when integrating over the interval.   
It seems obvious that the ALE estimation (within a grid interval) can only be as good as a linear approximation for the "real" and usually unknown prediction function can be. Therefore it is crucial for a good estimation to have small enough intervals especially in regions where the prediction function is shaky or far from linear (i.e high second derivatives) with respect to the feature of interest.
On the other hand to get stable estimations for a grid interval it is important to have a sufficient high number of datapoints within the interval. This means that the intervals shouldn't become too small so that they would contain only a few data points. Note that this is only true if the other features have an influence on the local effect of the prediction function. If they dont, any data point within the grid interval would lead to the same predictions at the interval boundarys.
Thats why there is a natural trade of between a small interval width and the number of the contained data points. 

  
### State of the art

So the question is how to optimally choose the grid intervals. Should they all be of the same width containing a different number of datapoints? Should they all contain the same or at least a similar number of datapoints, accepting different interval sizes. Or could there even be a better solution in between the two concepts?   
Within the iml-package which is one of two implementations of ALE - plots the choosen method is the second one. The quantiles of the distribution of the feature are used as the grid that defines the intervals. That means the length of the intervals depends on the choosen grid size and the given feature distribution.

In the following section some examples with artificial data sets are provided, that should help to get a better feeling for the different deterministic factors that influence the goodness of the ALE estimation. Within the whole chapter the ALE estimation is conducted via the iml-package implementation.

### ALE Approximations 

In the following section, we only consider two dimensional data sets, of continious features $x_1$ and $x_2$ with a certain correlation. Furthermore we use some exemplary prediction functions which are differentiable such that we can calculate the theoratical ALE (see section \@ref(ale-intro-formula)) and use it to evaluate the goodness of the estimated ALE - curve. As we want to isolate some of the above mentioned influencial factors, we start with some easy examples adding step by step more complexity to the problem. 

### Example 1: additive feature effects

In the first example we assume a uniform distribution for the feature $x_1$ on the interval $[0, 10]$, i.e.
$X_1 \sim U(0,10)$. Furthermore we assume the conditional distribution of the feature $x_2$ given $x_1$ to be also uniform on the interval $[x_1 - 3,~x_1 + 3 ]$, i.e. $X_2 \vert X_1 = x_1 \sim U(x_1 - 3,~x_1 + 3 )$. Sampling 100 data points from this distribution yields the first dataset (see Figure \@ref(fig:DatasetALE1)) .

```{r eval = FALSE, include = FALSE}
prediction_function_1 <- function(x_1, x_2){
  y <-(x_1-4)*(x_1-5)*(x_1-6) + x_2^3
  return(y)
}


sample_uniform <- function(n, FUN = prediction_function_1, a = -5, b = 5, c = 2, d = 2){
  x_1 <- runif(n = n, min = a, max = b)
  x_2 <- runif(n = n, min = x_1 - c, max = x_1 + d)
  y <- FUN(x_1, x_2)
  return(as.data.frame(cbind(x_1, x_2, y)))
}

set.seed(123)
dataset_1 <- sample_uniform(100, prediction_function_1, a = 0, b = 10, c = 3, d = 3)
p <- ggplot2::ggplot(dataset_1, ggplot2::aes(x_1, x_2))
p + ggplot2::geom_point() +
  ggplot2::labs(title = "", subtitle = "")
```

```{r DatasetALE1, fig.cap='(ref:aleDataset1)', out.width='100%', echo=FALSE}

knitr::include_graphics("images/ALE_2_Dataset1_.PNG")
```
(ref:aleDataset1) The correlation is clearly recognizable.

Why we only made assumptions about the conditional distributon of $X_2$ and not on the joint distribution of $(X_1,X_2)$ gets clearer once we take a look on the calculation of the theoratical ALE. Therefore we first asume the prediction function $\hat{f}_1 (x_1, x_2) =  (x_1-4)(x_1-5)(x_1-6) + x_2^3$. Due to the special structure of $f_1$ the partial derivative with respect to $x_1$ is a polynomial of degree 2 which doesn't depend on $x_2$, concretley $\hat{f}^1(x_1,x_2) = 3x_1^2 -30x_1 +74$ (Remember the unusual notation for the j-th partial derivative as $f^{j}$). 
Now we can calculate the theoratical (uncentered) ALE:

$$(1)~~~~\widetilde{ALE}_{\hat{f},1}(x) = \int_{z_{0,1}}^x E_{X_2\vert X_1= Z_1}[\hat{f}^1(x_1,x_2)]~dz~~~=$$
$$(2)~~~~ \int_{z_{0,1}}^x \int p_{X_2\vert X_1 = z }(x_2)\hat{f}^1(z,x_2)~dx_2~dz~~~=$$
$$(3)~~~~ \int_{z_{0,1}}^x \hat{f}^1(z,x_2)\int p_{X_2\vert X_1=z}(x_2)~dx_2~dz~~~=$$

$$(4)~~~~ \int_{z_{0,1}}^x \hat{f}^1(z,x_2)~dz~~~=$$
$$(5)~~~~ \int_{z_{0,1}}^x  3z^2 -30z +74~dz~~~=$$
$$(6)~~~~ [z^3 -15z^2 +74z + c]_{z_{0,1}}^x~~~=$$

$$(7)~~~~ x^3 -15x^2 +74x - z_{0,1} ^ 3 + 15 z_{0,1}^2 - 74z_{0,1}~~~$$
Here $p_{X_2\vert X_1=z}(x_2)$ notates the conditional density of $X_2\vert X_1$ for $x_1 = z$.   
Step (3) makes use of the fact that $\hat{f}^1(x_1,x_2)$ doesn't depend on $x_2$ and in step (4) the integral over the density gives 1.
To get the centered ALE we have to calculate:

$$(8)~~~~ALE_{\hat{f},1}(x) = \widetilde{ALE}_{\hat{f},1}(x) - E[\widetilde{ALE}_{\hat{f},1}(X_1)] ~~~=$$

$$(9)~~~~ x^3 - 15x^2 +74x - z_{0,1} ^ 3 + 15 z_{0,1}^2 - 74z_{0,1}  ~- $$

$$ E[X_1^3 -15X_1^2 +74X_1 - z_{0,1} ^ 3 + 15 z_{0,1}^2 - 74z_{0,1}] ~~~=$$

$$(10)~~~~ x^3 -15x^2 +74x - E[X_1^3 -15X_1^2 +74X_1] ~~~=$$


$$(11)~~~~ x^3 -15x^2 +74x - E[X_1^3] +15E[X_1^2] -74E[X_1] ~~~=$$

$$(12)~~~~ x^3 -15x^2 +74x - 250 +15 \frac{100}{3}) - 74* 5 ~~~=$$

$$(13)~~~~ x^3 -15x^2 +74x - 120~~~.$$

In Step (12) the formula for the kth - moment of the uniform distribution which is given by $m_k = \frac{1}{k+1}\sum_{i=0}^k a^i b^{k-i}$ was used.
Knowing the theoretical ALE-curve we can have a look on the behaviour of the estimated ALE for different grid sizes. Figure \@ref(fig:exampleALE1) shows the theoretical ALE and the estimations for grid sizes 2, 3, 5, and 10. 
```{r eval = FALSE, include = FALSE}

prediction_function_1 <- function(x_1, x_2){
  y <-(x_1-4)*(x_1-5)*(x_1-6) + x_2^3
  return(y)
}


sample_uniform <- function(n, FUN = prediction_function_1, a = -5, b = 5, c = 2, d = 2){
  x_1 <- runif(n = n, min = a, max = b)
  x_2 <- runif(n = n, min = x_1 - c, max = x_1 + d)
  y <- FUN(x_1, x_2)
  return(as.data.frame(cbind(x_1, x_2, y)))
}

set.seed(123)
dataset_1 <- sample_uniform(100, prediction_function_1, a = 0, b = 10, c = 3, d = 3)
p <- ggplot2::ggplot(dataset_1, ggplot2::aes(x_1, x_2))
p + ggplot2::geom_point() +
  ggplot2::labs(title = "", subtitle = "")


expected_ALE_1 <- function(x){

  ALE <- x^3 -15*x^2 +74*x  -120
  return (ALE)
}

fun.0 <- function(x) expected_ALE_1(x)

library(crs)
library(iml)

model = crs(y~., data = dataset_1)
predictor_1 = Predictor$new(model, data = dataset_1, y = "y", predict.fun = function(model, newdata){
  #a<-predict(model, newdata)
  a <- numeric(0)
  for (i in 1:nrow(newdata)){
    a[i]<- prediction_function_1(newdata[i, 1], newdata[i, 2])
  }
  return(a)
})

X_1 = dataset_1[which(names(df) != "y")]


ale.1 = FeatureEffect$new(predictor_1, feature = "x_1", grid.size = 2)
fun.1 <- function(x) ale.1$predict(x)

library(ggplot2)
plot1 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.1, aes(colour = "red")) +
  xlim(c(0,10)) +
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 2",  x = "x _ j", y = "ALE(x _ j)")

ale.2 = FeatureEffect$new(predictor_1, feature = "x_1", grid.size = 3)
fun.2 <- function(x) ale.2$predict(x)

plot2 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.2, aes(colour = "red")) +
  xlim(c(0,10)) +
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 3",  x = "x _ j", y = "ALE(x _ j)")


ale.3 = FeatureEffect$new(predictor_1, feature = "x_1", grid.size = 5)
fun.3 <- function(x) ale.3$predict(x)

plot3 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.3, aes(colour = "red")) +
  xlim(c(0,10)) +
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 5",  x = "x _ j", y = "ALE(x _ j)")

ale.4 = FeatureEffect$new(predictor_1, feature = "x_1", grid.size = 10)
fun.4 <- function(x) ale.4$predict(x)

plot4 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.4, aes(colour = "red")) +
  xlim(c(0,10)) +
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 10",  x = "x _ j", y = "ALE(x _ j)")



library(ggpubr)

ggarrange(plot1, plot2, plot3, plot4, ncol=2, nrow=2, common.legend = TRUE, legend="bottom")

```



```{r exampleALE1, fig.cap='(ref:exampleALE1)', out.width='100%', echo=FALSE}

knitr::include_graphics("images/ALE_2_example1_.PNG")
```
(ref:exampleALE1) Theoretical vs estimated ALE

While the estimated ALE with grid size 2 only shows a linear effect over the whole data range, the estimated ALE with grid size 3 already gives a good approximation to the theoretical ALE in the second interval, where the theoretical ALE has a low curvature. With grid size 5 only the outer intervals show clearly recognizable deviations to the theoretical ALE and with grid size 10 the approximation looks quite reasonable.
As the partial derivative of the prediction function was independent of $x_2$, there was no risk of getting bad estimations due to too few datapoints within an interval. Thats why we take a look on a second example.

### Example 2: multiplicative feature effects

Now we asume the prediction function $\hat{f}_2 (x_1, x_2) =  (x_1-4)(x_1-5)(x_1-6)x_2^3$. In this case the partial derivative with respect to $x_1$ is a polynomial of degree 2 which clearly depends on $x_2$, concretley $\hat{f}^1(x_1,x_2) = (3x_1^2 -30x_1 +74)x_2^3$. The new structure of the partial derivative yields a new calculation for the theoretical uncentered ALE:

$$(1)~~~~\widetilde{ALE}_{\hat{f},1}(x) = \int_{z_{0,1}}^x E_{X_2\vert X_1= Z_1}[\hat{f}^1(x_1,x_2)]~dz~~~=$$

$$(2)~~~~ \int_{z_{0,1}}^x \int p_{X_2\vert X_1=z}(x_2)\hat{f}^1(z,x_2)~dx_2~dz~~~=$$
$$(3)~~~~ \int_{z_{0,1}}^x \int p_{X_2\vert X_1=z}(x_2)(3z^2 -30z +74)x_2^3~dx_2~dz~~~=$$
$$(4)~~~~ \int_{z_{0,1}}^x (3z^2 -30z +74)\int p_{X_2\vert X_1=z}(x_2)x_2^3~dx_2~dz~~~=$$
$$(5)~~~~ \int_{z_{0,1}}^x (3z^2 -30z +74)~E_{X_2\vert X_1 = z}[X_2^3]~dz~~~=$$

$$(6)~~~~ \int_{z_{0,1}}^x (3z^2 -30z +74)(\frac{1}{4}\sum_{i=0}^{k=3}(z-3)^i(z+3)^{k-i})~dz~~~=$$
$$(7)~~~~ \int_{z_{0,1}}^x (3z^2 -30z +74)(z^3 + 9z)~dz~~~=$$
$$(8)~~~~ \int_{z_{0,1}}^x 3 z^5 - 30 z^4 + 101 z^3 - 270 z^2 + 666 z ~dz~~~=$$

$$(9)~~~~ [ \frac{3}{6} z^6 - \frac{30}{5}z^5 + \frac{101}{4}z^4 - 90 z^3 + 333 z^2]_{z_{0,1}}^x~~~$$
Centering yields

$$(10)~~~~ALE_{\hat{f},1}(x) =  \frac{3}{6} x^6 - \frac{30}{5}x^5 + \frac{101}{4}x^4 - 90 x^3 + 333 x^2 - $$

$$E[ \frac{3}{6} X_1^6 - \frac{30}{5}X_1^5 + \frac{101}{4}X_1^4 - 90 X_1^3 + 333 X_1^2] ~~~$$
Again using the formula for the moments of a uniform distribution we finally obtain

$$(11)~~~~ALE_{\hat{f},1}(x) =  \frac{3}{6} x^6 - \frac{30}{5}x^5 + \frac{101}{4}x^4 - 90 x^3 + 333 x^2 - $$

$$(\frac{3}{6}\frac{10^6}{7}-\frac{30}{5}\frac{10^5}{6} +\frac{101}{4}\frac{10^4}{5}-90\frac{10^3}{4}+333\frac{10^2}{3}) ~~~=$$

$$(12)~~~~ALE_{\hat{f},1}(x) =  \frac{3}{6} x^6 - \frac{30}{5}x^5 + \frac{101}{4}x^4 - 90 x^3 + 333 x^2 - 10528.57~~~.$$
Figure \@ref(fig:exampleALE2) shows the behaviour of the ALE with different grid sizes in this setting.


```{r eval = FALSE, include = FALSE}
prediction_function_2 <- function(x_1, x_2){
  y <-(x_1-4)*(x_1-5)*(x_1-6)* x_2^3
  return(y)
}


sample_uniform <- function(n, FUN = prediction_function_2, a = 0, b = 10, c = 3, d = 3){
  x_1 <- runif(n = n, min = a, max = b)
  x_2 <- runif(n = n, min = x_1 - c, max = x_1 + d)
  y <- FUN(x_1, x_2)
  return(as.data.frame(cbind(x_1, x_2, y)))
}

set.seed(123)
dataset_1 <- sample_uniform(100, prediction_function_2, a = 0, b = 10, c = 3, d = 3)
p <- ggplot2::ggplot(dataset_1, ggplot2::aes(x_1, x_2))
p + ggplot2::geom_point() +
  ggplot2::labs(title = "Dataset 1", subtitle = "Uniform dist.")



expected_ALE_2 <- function(x){

  ALE <- 3/6*x^6 -30/5*x^5 +101/4 * x^4 -90*x^3+333*x^2 -10528.57
  return (ALE)
}

fun.0 <- function(x) expected_ALE_2(x)

library(crs)
library(iml)

model = crs(y~., data = dataset_1)
predictor_2 = Predictor$new(model, data = dataset_1, y = "y", predict.fun = function(model, newdata){
  #a<-predict(model, newdata)
  a <- numeric(0)
  for (i in 1:nrow(newdata)){
    a[i]<- prediction_function_2(newdata[i, 1], newdata[i, 2])
  }
  return(a)
})

X_1 = dataset_1[which(names(df) != "y")]


ale.1 = FeatureEffect$new(predictor_2, feature = "x_1", grid.size = 2)
fun.1 <- function(x) ale.1$predict(x)

library(ggplot2)
plot1 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.1, aes(colour = "red")) +
  xlim(c(0,10)) +
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 2",  x = "x _ j", y = "ALE(x _ j)")


ale.2 = FeatureEffect$new(predictor_2, feature = "x_1", grid.size = 5)
fun.2 <- function(x) ale.2$predict(x)

plot2 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.2, aes(colour = "red")) +
  xlim(c(0,10)) +
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 5",  x = "x _ j", y = "ALE(x _ j)")


ale.3 = FeatureEffect$new(predictor_2, feature = "x_1", grid.size = 10)
fun.3 <- function(x) ale.3$predict(x)

plot3 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.3, aes(colour = "red")) +
  xlim(c(0,10)) +
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 10",  x = "x _ j", y = "ALE(x _ j)")

ale.4 = FeatureEffect$new(predictor_2, feature = "x_1", grid.size = 25)
fun.4 <- function(x) ale.4$predict(x)

plot4 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.4, aes(colour = "red")) +
  xlim(c(0,10)) +
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 25",  x = "x _ j", y = "ALE(x _ j)")

ale.5 = FeatureEffect$new(predictor_2, feature = "x_1", grid.size = 50)
fun.5 <- function(x) ale.4$predict(x)

plot5 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.5, aes(colour = "red")) +
  xlim(c(0,10)) +
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 50",  x = "x _ j", y = "ALE(x _ j)")

ale.6 = FeatureEffect$new(predictor_2, feature = "x_1", grid.size = 100)
fun.6 <- function(x) ale.4$predict(x)

plot6 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.6, aes(colour = "red")) +
  xlim(c(0,10)) +
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 100",  x = "x _ j", y = "ALE(x _ j)")





library(ggpubr)

ggarrange(plot1, plot2, plot3, plot4, plot5, plot6 , ncol=2, nrow=3, common.legend = TRUE, legend="bottom")
#ggarrange(plot3, plot4, plot5, plot6 , ncol=2, nrow=2, common.legend = TRUE, legend="bottom")

```




```{r exampleALE2, fig.cap='(ref:exampleALE2)',  out.width='100%', echo=FALSE}

knitr::include_graphics("images/ALE_2_example2_6plots_.PNG")
```
(ref:exampleALE2) Theoretical vs estimated ALE for different grid sizes 

While for grid size 5 and bigger the approximations for the region 0 to 7.5 seem quite reasonable it looks like for the region 7.5 to 10 the approximation is best for grid size 10 and gets worse with higher grid sizes. Zooming in for grid sizes 10 and 25 reveals this effect more clearly (Figure \@ref(fig:exampleALE2a)).


```{r eval = FALSE, include = FALSE}
prediction_function_2 <- function(x_1, x_2){
  y <-(x_1-4)*(x_1-5)*(x_1-6)* x_2^3
  return(y)
}


sample_uniform <- function(n, FUN = prediction_function_2, a = 0, b = 10, c = 3, d = 3){
  x_1 <- runif(n = n, min = a, max = b)
  x_2 <- runif(n = n, min = x_1 - c, max = x_1 + d)
  y <- FUN(x_1, x_2)
  return(as.data.frame(cbind(x_1, x_2, y)))
}

set.seed(123)
dataset_1 <- sample_uniform(100, prediction_function_2, a = 0, b = 10, c = 3, d = 3)
p <- ggplot2::ggplot(dataset_1, ggplot2::aes(x_1, x_2))
p + ggplot2::geom_point() +
  ggplot2::labs(title = "Dataset 1", subtitle = "Uniform dist.")



expected_ALE_2 <- function(x){

  ALE <- 3/6*x^6 -30/5*x^5 +101/4 * x^4 -90*x^3+333*x^2 -10528.57
  return (ALE)
}

fun.0 <- function(x) expected_ALE_2(x)

library(crs)
library(iml)

model = crs(y~., data = dataset_1)
predictor_2 = Predictor$new(model, data = dataset_1, y = "y", predict.fun = function(model, newdata){
  #a<-predict(model, newdata)
  a <- numeric(0)
  for (i in 1:nrow(newdata)){
    a[i]<- prediction_function_2(newdata[i, 1], newdata[i, 2])
  }
  return(a)
})

X_1 = dataset_1[which(names(df) != "y")]


ale.1 = FeatureEffect$new(predictor_2, feature = "x_1", grid.size = 2)
fun.1 <- function(x) ale.1$predict(x)

library(ggplot2)
plot1 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.1, aes(colour = "red")) +
  xlim(c(0,10)) +
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 2",  x = "x _ j", y = "ALE(x _ j)")


ale.2 = FeatureEffect$new(predictor_2, feature = "x_1", grid.size = 5)
fun.2 <- function(x) ale.2$predict(x)

plot2 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.2, aes(colour = "red")) +
  xlim(c(0,10)) +
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 5",  x = "x _ j", y = "ALE(x _ j)")


ale.3 = FeatureEffect$new(predictor_2, feature = "x_1", grid.size = 10)
fun.3 <- function(x) ale.3$predict(x)

plot3 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.3, aes(colour = "red")) +
  xlim(c(0,10)) +
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 10",  x = "x _ j", y = "ALE(x _ j)")

ale.4 = FeatureEffect$new(predictor_2, feature = "x_1", grid.size = 25)
fun.4 <- function(x) ale.4$predict(x)

plot4 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.4, aes(colour = "red")) +
  xlim(c(0,10)) +
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 25",  x = "x _ j", y = "ALE(x _ j)")

ale.5 = FeatureEffect$new(predictor_2, feature = "x_1", grid.size = 50)
fun.5 <- function(x) ale.4$predict(x)

plot5 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.5, aes(colour = "red")) +
  xlim(c(0,10)) +
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 50",  x = "x _ j", y = "ALE(x _ j)")

ale.6 = FeatureEffect$new(predictor_2, feature = "x_1", grid.size = 100)
fun.6 <- function(x) ale.4$predict(x)

plot6 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.6, aes(colour = "red")) +
  xlim(c(0,10)) +
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 100",  x = "x _ j", y = "ALE(x _ j)")





library(ggpubr)

#ggarrange(plot1, plot2, plot3, plot4, plot5, plot6 , ncol=2, nrow=3, common.legend = TRUE, legend="bottom")

plot7 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.3, aes(colour = "red")) +
  xlim(c(6,9)) + ylim(c(-10000, 40000))+
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 10",  x = "x _ j", y = "ALE(x _ j)")




plot8 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.4, aes(colour = "red")) +
  xlim(c(6,9)) + ylim(c(-10000, 40000)) +
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 25",  x = "x _ j", y = "ALE(x _ j)")

ggarrange(plot3, plot4, plot7, plot8 , ncol=2, nrow=2, common.legend = TRUE, legend="bottom")



```



```{r exampleALE2a,fig.cap='(ref:exampleALE2a)',  out.width='100%', echo=FALSE}

knitr::include_graphics("images/ALE_2_example2_zoom_.PNG")
```
(ref:exampleALE2a) ALE-plots for grid sizes 10 and 25 (zoomed in) 

Where does this come from? The structure of the prediction function leads to an increasing effect of $x_2$ on the total differences calculated for the series of intervals. Due to unsufficient many datapoints within the intervals, there is a high probability of under or over estimating this effect. With grid size 25 only 4 data points are used for the estimation. Obviously it's quite probable that the $x_2$ values of those data points are clearly above average in some intervals. If that happens for high $x_1$ - which implies due to the correlation structure high $x_2$ - the total difference will be clearly overestimated as the delta in $x_1$ is multiplied by the average $x_2^3$. As the effect on the intervals is accumulated, the error persists for the whole ALE-curve from that point on.

To get a deeper insight in this dynamic, for the given context ALE - curves for 50 sampled datasets were estimated with grid sizes 10, 25, and 50. For each grid size at each value of $x_1$ the minimal and the maximal ALE estimation was taken as the boundary of the range of estimations. Figure \@ref(fig:exampleALE2b) shows this range exemplarily for gride size 10.

```{r eval = FALSE, include = FALSE}


prediction_function_2 <- function(x_1, x_2){
  y <-(x_1-4)*(x_1-5)*(x_1-6)* x_2^3
  return(y)
}


sample_uniform <- function(n, FUN = prediction_function_2, a = 0, b = 10, c = 3, d = 3){
  x_1 <- runif(n = n, min = a, max = b)
  x_2 <- runif(n = n, min = x_1 - c, max = x_1 + d)
  y <- FUN(x_1, x_2)
  return(as.data.frame(cbind(x_1, x_2, y)))
}


expected_ALE_2 <- function(x){

  ALE <- 3/6*x^6 -30/5*x^5 +101/4 * x^4 -90*x^3+333*x^2 -10528.57
  return (ALE)
}

fun.0 <- function(x) expected_ALE_2(x)

library(crs)
library(iml)
set.seed(123)

dataset_1 <- sample_uniform(100, prediction_function_2, a = 0, b = 10, c = 3, d = 3)


model = crs(y~., data = dataset_1)



library(ggplot2)
plot_v <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) + xlim(c(0,10)) +
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "Data Set 4",  x = "x _ j", y = "ALE(x _ j)")
#plot_v

anz <- 100
df_list<-list()
set.seed(123)
for(j in 3:(anz +2)){
  df_list[[j]] <- sample_uniform(100, prediction_function_2, a = 0, b = 10, c = 3, d = 3)
}
eval_points <- seq(from = 0, to = 10, by = 0.05)
function_list <- list()

for (j in 3:(anz +2)){
  df <- df_list[[j]]
  #df <- sample_uniform(100, prediction_function_2, a = 0, b = 10, c = 3, d = 3)
  #model = crs(y~., data = df)
  predictor_2_1 = Predictor$new(model, data = df, y = "y", predict.fun = function(model, newdata){
    #a<-predict(model, newdata)
    a <- numeric(0)
    for (i in 1:nrow(newdata)){
      a[i]<- prediction_function_2(newdata[i, 1], newdata[i, 2])
    }
    return(a)
  })
  ale.1 = FeatureEffect$new(predictor_2_1, feature = "x_1", grid.size = 10)
  fun.1 <- function(x) ale.1$predict(x)
  function_list[[j]] <- fun.1(eval_points)
}
function_list[[1]] <- eval_points
function_list[[2]] <- fun.0(eval_points)
names(function_list)= c("x", "ALE" )
d10<- data.frame(function_list)
d2 <- d10[,(3:ncol(d10))]
maximum10 <- numeric(0)
minimum10 <- numeric(0)
stdabw10 <-numeric(0)
for (i in 1 : nrow(d10)){
  maximum10[i] <- max(d2[i,], na.rm = TRUE)
  minimum10[i] <- min(d2[i,], na.rm = TRUE)
  stdabw10[i] <- sd(d2[i,], na.rm = TRUE)
}
maximum10[1]<- maximum10[2]
minimum10[1]<- minimum10[2]
maximum10[201]<- maximum10[200]
minimum10[201]<- minimum10[200]

proz_var10 <- numeric(0)
var10<- numeric(0)

for (i in 1 : nrow(d10)){
  proz_var10[i] <- (abs(maximum10[i]-minimum10[i]))/abs(d10$ALE[i])
  var10[i]<- (abs(maximum10[i]-minimum10[i]))

}


function_list <- list()

for (j in 3:(anz +2)){
  df <- df_list[[j]]
  #df <- sample_uniform(100, prediction_function_2, a = 0, b = 10, c = 3, d = 3)
  #model = crs(y~., data = df)
  predictor_2_1 = Predictor$new(model, data = df, y = "y", predict.fun = function(model, newdata){
    #a<-predict(model, newdata)
    a <- numeric(0)
    for (i in 1:nrow(newdata)){
      a[i]<- prediction_function_2(newdata[i, 1], newdata[i, 2])
    }
    return(a)
  })
  ale.1 = FeatureEffect$new(predictor_2_1, feature = "x_1", grid.size = 25)
  fun.1 <- function(x) ale.1$predict(x)
  function_list[[j]] <- fun.1(eval_points)
}
function_list[[1]] <- eval_points
function_list[[2]] <- fun.0(eval_points)
names(function_list)= c("x", "ALE" )
d25<- data.frame(function_list)
d2 <- d25[,(3:ncol(d25))]
maximum25 <- numeric(0)
minimum25 <- numeric(0)
stdabw25 <- numeric(0)
for (i in 1 : nrow(d25)){
  maximum25[i] <- max(d2[i,], na.rm = TRUE)
  minimum25[i] <- min(d2[i,], na.rm = TRUE)
  stdabw25[i] <- sd(d2[i,], na.rm = TRUE)
}
maximum25[1]<- maximum25[2]
minimum25[1]<- minimum25[2]
maximum25[201]<- maximum25[200]
minimum25[201]<- minimum25[200]

proz_var25 <- numeric(0)
var25<- numeric(0)
for (i in 1 : nrow(d25)){
  proz_var25[i] <- (abs(maximum25[i]-minimum25[i]))/abs(d25$ALE[i])
  var25[i]<- (abs(maximum25[i]-minimum25[i]))
}











#plot_v <- plot_v +
#geom_line(aes(x, minimum10, color = "min" ), d10) +
#geom_line(aes(x, maximum10, color = "max"), d10)+
#geom_line(aes(x, var10, color = "delta10"), d10)+
#geom_line(aes(x, var25, color = "delta25"), d10)+
#scale_color_manual("Legende",values = c('min' = 'red','max' = 'red', 'theo' = 'green', 'delta10' = "blue", 'delta25' = "orange"),
#labels = c('min' = "Minimal est. ALE", 'max'= "Maximal est. ALE", 'theo' = "theoretical ALE",'delta10'="Delta grid size 10",'delta25 '="Delta grid size 25"))
#plot_v

#plot(stdabw10,stdabw25)


plot_v_1 <- ggplot() +
  geom_line(aes(x, ALE, color = "theo" ), d10)

plot_v_1 <- plot_v_1 +
  geom_line(aes(x, minimum10, color = "min" ), d10) +
  geom_line(aes(x, maximum10, color = "max"), d10)+
  geom_segment(aes(x = 2.5, y = minimum10[51], xend = 2.5, yend = maximum10[51]))+
  geom_segment(aes(x = 5, y = minimum10[101], xend = 5, yend = maximum10[101]))+
  geom_segment(aes(x = 7.5, y = minimum10[151], xend = 7.5, yend = maximum10[151]))+
  geom_segment(aes(x = 9, y = minimum10[181], xend = 9, yend = maximum10[181]))+
  geom_segment(aes(x = 9.5, y = minimum10[191], xend = 9.5, yend = maximum10[191]))+
  scale_color_manual("Legende",values = c('min' = 'red','max' = 'red', 'theo' = 'green'),
                     labels = c('min' = "Minimal est. ALE", 'max'= "Maximal est. ALE", 'theo' = "theoretical ALE")) +
  labs(title = "",  x = "x", y = "ALE(x)")+
  ylim(c(-15000, 100000))+
  xlim(c(0,10))

plot_v_1

```

```{r exampleALE2b, fig.cap='(ref:exampleALE2b)', out.width='100%', echo=FALSE}

knitr::include_graphics("images/ALE_2_ALErange_10_.PNG")
```
(ref:exampleALE2b) Maximum range of estimation

The vertical lines indicate the absolut delta of the maximal and minimal ALE estimation at x. Plotting these deltas for the gride sizes 10, 25, and 50 yields Figure \@ref(fig:exampleALE2c). 

```{r eval = FALSE, include = FALSE}


prediction_function_2 <- function(x_1, x_2){
  y <-(x_1-4)*(x_1-5)*(x_1-6)* x_2^3
  return(y)
}


sample_uniform <- function(n, FUN = prediction_function_2, a = 0, b = 10, c = 3, d = 3){
  x_1 <- runif(n = n, min = a, max = b)
  x_2 <- runif(n = n, min = x_1 - c, max = x_1 + d)
  y <- FUN(x_1, x_2)
  return(as.data.frame(cbind(x_1, x_2, y)))
}


expected_ALE_2 <- function(x){

  ALE <- 3/6*x^6 -30/5*x^5 +101/4 * x^4 -90*x^3+333*x^2 -10528.57
  return (ALE)
}

fun.0 <- function(x) expected_ALE_2(x)

library(crs)
library(iml)
set.seed(123)

dataset_1 <- sample_uniform(100, prediction_function_2, a = 0, b = 10, c = 3, d = 3)


model = crs(y~., data = dataset_1)



library(ggplot2)
plot_v <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) + xlim(c(0,10)) +
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "Data Set 4",  x = "x _ j", y = "ALE(x _ j)")
#plot_v

anz <- 100
df_list<-list()
set.seed(123)
for(j in 3:(anz +2)){
  df_list[[j]] <- sample_uniform(100, prediction_function_2, a = 0, b = 10, c = 3, d = 3)
}
eval_points <- seq(from = 0, to = 10, by = 0.05)
function_list <- list()

for (j in 3:(anz +2)){
  df <- df_list[[j]]
  #df <- sample_uniform(100, prediction_function_2, a = 0, b = 10, c = 3, d = 3)
  #model = crs(y~., data = df)
  predictor_2_1 = Predictor$new(model, data = df, y = "y", predict.fun = function(model, newdata){
    #a<-predict(model, newdata)
    a <- numeric(0)
    for (i in 1:nrow(newdata)){
      a[i]<- prediction_function_2(newdata[i, 1], newdata[i, 2])
    }
    return(a)
  })
  ale.1 = FeatureEffect$new(predictor_2_1, feature = "x_1", grid.size = 10)
  fun.1 <- function(x) ale.1$predict(x)
  function_list[[j]] <- fun.1(eval_points)
}
function_list[[1]] <- eval_points
function_list[[2]] <- fun.0(eval_points)
names(function_list)= c("x", "ALE" )
d10<- data.frame(function_list)
d2 <- d10[,(3:ncol(d10))]
maximum10 <- numeric(0)
minimum10 <- numeric(0)
stdabw10 <-numeric(0)
for (i in 1 : nrow(d10)){
  maximum10[i] <- max(d2[i,], na.rm = TRUE)
  minimum10[i] <- min(d2[i,], na.rm = TRUE)
  stdabw10[i] <- sd(d2[i,], na.rm = TRUE)
}
maximum10[1]<- maximum10[2]
minimum10[1]<- minimum10[2]
maximum10[201]<- maximum10[200]
minimum10[201]<- minimum10[200]

proz_var10 <- numeric(0)
var10<- numeric(0)

for (i in 1 : nrow(d10)){
  proz_var10[i] <- (abs(maximum10[i]-minimum10[i]))/abs(d10$ALE[i])
  var10[i]<- (abs(maximum10[i]-minimum10[i]))

}


function_list <- list()

for (j in 3:(anz +2)){
  df <- df_list[[j]]
  #df <- sample_uniform(100, prediction_function_2, a = 0, b = 10, c = 3, d = 3)
  #model = crs(y~., data = df)
  predictor_2_1 = Predictor$new(model, data = df, y = "y", predict.fun = function(model, newdata){
    #a<-predict(model, newdata)
    a <- numeric(0)
    for (i in 1:nrow(newdata)){
      a[i]<- prediction_function_2(newdata[i, 1], newdata[i, 2])
    }
    return(a)
  })
  ale.1 = FeatureEffect$new(predictor_2_1, feature = "x_1", grid.size = 25)
  fun.1 <- function(x) ale.1$predict(x)
  function_list[[j]] <- fun.1(eval_points)
}
function_list[[1]] <- eval_points
function_list[[2]] <- fun.0(eval_points)
names(function_list)= c("x", "ALE" )
d25<- data.frame(function_list)
d2 <- d25[,(3:ncol(d25))]
maximum25 <- numeric(0)
minimum25 <- numeric(0)
stdabw25 <- numeric(0)
for (i in 1 : nrow(d25)){
  maximum25[i] <- max(d2[i,], na.rm = TRUE)
  minimum25[i] <- min(d2[i,], na.rm = TRUE)
  stdabw25[i] <- sd(d2[i,], na.rm = TRUE)
}
maximum25[1]<- maximum25[2]
minimum25[1]<- minimum25[2]
maximum25[201]<- maximum25[200]
minimum25[201]<- minimum25[200]

proz_var25 <- numeric(0)
var25<- numeric(0)
for (i in 1 : nrow(d25)){
  proz_var25[i] <- (abs(maximum25[i]-minimum25[i]))/abs(d25$ALE[i])
  var25[i]<- (abs(maximum25[i]-minimum25[i]))
}











plot_v <- plot_v +
  geom_line(aes(x, minimum10, color = "min" ), d10) +
  geom_line(aes(x, maximum10, color = "max"), d10)+
  geom_line(aes(x, var10, color = "delta10"), d10)+
  geom_line(aes(x, var25, color = "delta25"), d10)+
  scale_color_manual("Legende",values = c('min' = 'red','max' = 'red', 'theo' = 'green', 'delta10' = "blue", 'delta25' = "orange"),
                     labels = c('min' = "Minimal est. ALE", 'max'= "Maximal est. ALE", 'theo' = "theoretical ALE",'delta10'="Delta grid size 10",'delta25 '="Delta grid size 25"))
#plot_v

#plot(stdabw10,stdabw25)


plot_v_1 <- ggplot() +
  geom_line(aes(x, ALE, color = "theo" ), d10)

plot_v_1 <- plot_v_1 +
  geom_line(aes(x, minimum10, color = "min" ), d10) +
  geom_line(aes(x, maximum10, color = "max"), d10)+
  geom_segment(aes(x = 2.5, y = minimum10[51], xend = 2.5, yend = maximum10[51]))+
  geom_segment(aes(x = 5, y = minimum10[101], xend = 5, yend = maximum10[101]))+
  geom_segment(aes(x = 7.5, y = minimum10[151], xend = 7.5, yend = maximum10[151]))+
  geom_segment(aes(x = 9, y = minimum10[181], xend = 9, yend = maximum10[181]))+
  geom_segment(aes(x = 9.5, y = minimum10[191], xend = 9.5, yend = maximum10[191]))+
  scale_color_manual("Legende",values = c('min' = 'red','max' = 'red', 'theo' = 'green'),
                     labels = c('min' = "Minimal est. ALE", 'max'= "Maximal est. ALE", 'theo' = "theoretical ALE")) +
  labs(title = "Minimal and maximal est. ALE with grid size 10 for 100 different datasets",  x = "x", y = "ALE(x)")+
  ylim(c(-15000, 100000))+
  xlim(c(0,10))

#plot_v_1

plot_v_2 <- ggplot() +
  geom_line(aes(x, ALE, color = "theo" ), d10)

plot_v_2 <- plot_v_2 +
  geom_line(aes(x, minimum25, color = "min" ), d10) +
  geom_line(aes(x, maximum25, color = "max"), d10)+
  geom_segment(aes(x = 2.5, y = minimum25[51], xend = 2.5, yend = maximum25[51]))+
  geom_segment(aes(x = 5, y = minimum25[101], xend = 5, yend = maximum25[101]))+
  geom_segment(aes(x = 7.5, y = minimum25[151], xend = 7.5, yend = maximum25[151]))+
  geom_segment(aes(x = 9, y = minimum25[181], xend = 9, yend = maximum25[181]))+
  geom_segment(aes(x = 9.5, y = minimum25[191], xend = 9.5, yend = maximum25[191]))+
  scale_color_manual("Legende",values = c('min' = 'red','max' = 'red', 'theo' = 'green'),
                     labels = c('min' = "Minimal est. ALE", 'max'= "Maximal est. ALE", 'theo' = "theoretical ALE")) +
  labs(title = "Minimal and maximal ALE with grid size 25 for 100 datasets",  x = "x", y = "ALE(x)")+
  ylim(c(-15000, 100000))+
  xlim(c(0,10))

#plot_v_2


function_list <- list()

for (j in 3:(anz +2)){
  df <- df_list[[j]]
  #df <- sample_uniform(100, prediction_function_2, a = 0, b = 10, c = 3, d = 3)
  #model = crs(y~., data = df)
  predictor_2_1 = Predictor$new(model, data = df, y = "y", predict.fun = function(model, newdata){
    #a<-predict(model, newdata)
    a <- numeric(0)
    for (i in 1:nrow(newdata)){
      a[i]<- prediction_function_2(newdata[i, 1], newdata[i, 2])
    }
    return(a)
  })
  ale.1 = FeatureEffect$new(predictor_2_1, feature = "x_1", grid.size = 50)
  fun.1 <- function(x) ale.1$predict(x)
  function_list[[j]] <- fun.1(eval_points)
}
function_list[[1]] <- eval_points
function_list[[2]] <- fun.0(eval_points)
names(function_list)= c("x", "ALE" )
d50<- data.frame(function_list)
d2 <- d50[,(3:ncol(d25))]
maximum50 <- numeric(0)
minimum50 <- numeric(0)
stdabw50 <- numeric(0)
for (i in 1 : nrow(d25)){
  maximum50[i] <- max(d2[i,], na.rm = TRUE)
  minimum50[i] <- min(d2[i,], na.rm = TRUE)
  stdabw50[i] <- sd(d2[i,], na.rm = TRUE)
}
maximum50[1]<- maximum50[2]
minimum50[1]<- minimum50[2]
maximum50[201]<- maximum50[200]
minimum50[201]<- minimum50[200]

proz_var50 <- numeric(0)
var50<- numeric(0)
for (i in 1 : nrow(d50)){
  proz_var50[i] <- (abs(maximum50[i]-minimum50[i]))/abs(d50$ALE[i])
  var50[i]<- (abs(maximum50[i]-minimum50[i]))
}


plot_v_4 <- ggplot() +
  geom_line(aes(x, ALE, color = "theo" ), d10)

plot_v_4 <- plot_v_4 +
  geom_line(aes(x, minimum50, color = "min" ), d10) +
  geom_line(aes(x, maximum50, color = "max"), d10)+
  geom_segment(aes(x = 2.5, y = minimum50[51], xend = 2.5, yend = maximum50[51]))+
  geom_segment(aes(x = 5, y = minimum50[101], xend = 5, yend = maximum50[101]))+
  geom_segment(aes(x = 7.5, y = minimum50[151], xend = 7.5, yend = maximum50[151]))+
  geom_segment(aes(x = 9, y = minimum50[181], xend = 9, yend = maximum50[181]))+
  geom_segment(aes(x = 9.5, y = minimum50[191], xend = 9.5, yend = maximum50[191]))+
  scale_color_manual("Legende",values = c('min' = 'red','max' = 'red', 'theo' = 'green'),
                     labels = c('min' = "Minimal est. ALE", 'max'= "Maximal est. ALE", 'theo' = "theoretical ALE")) +
  labs(title = "Minimal and maximal ALE with grid size 50 for 100 datasets",  x = "x", y = "ALE(x)")+
  ylim(c(-15000, 100000))+
  xlim(c(0,10))

#plot_v_4





function_list <- list()

for (j in 3:(anz +2)){
  df <- df_list[[j]]
  #df <- sample_uniform(100, prediction_function_2, a = 0, b = 10, c = 3, d = 3)
  #model = crs(y~., data = df)
  predictor_2_1 = Predictor$new(model, data = df, y = "y", predict.fun = function(model, newdata){
    #a<-predict(model, newdata)
    a <- numeric(0)
    for (i in 1:nrow(newdata)){
      a[i]<- prediction_function_2(newdata[i, 1], newdata[i, 2])
    }
    return(a)
  })
  ale.1 = FeatureEffect$new(predictor_2_1, feature = "x_1", grid.size = 5)
  fun.1 <- function(x) ale.1$predict(x)
  function_list[[j]] <- fun.1(eval_points)
}
function_list[[1]] <- eval_points
function_list[[2]] <- fun.0(eval_points)
names(function_list)= c("x", "ALE" )
d5<- data.frame(function_list)
d2 <- d5[,(3:ncol(d25))]
maximum5 <- numeric(0)
minimum5 <- numeric(0)
stdabw5 <- numeric(0)
for (i in 1 : nrow(d25)){
  maximum5[i] <- max(d2[i,], na.rm = TRUE)
  minimum5[i] <- min(d2[i,], na.rm = TRUE)
  stdabw5[i] <- sd(d2[i,], na.rm = TRUE)
}
maximum5[1]<- maximum5[2]
minimum5[1]<- minimum5[2]
maximum5[201]<- maximum5[200]
minimum5[201]<- minimum5[200]

proz_var5 <- numeric(0)
var5<- numeric(0)
for (i in 1 : nrow(d5)){
  proz_var5[i] <- (abs(maximum5[i]-minimum5[i]))/abs(d5$ALE[i])
  var5[i]<- (abs(maximum5[i]-minimum5[i]))
}





plot_v_3 <- ggplot() +
  geom_line(aes(x, var10, color = "delta10"), d10)+
  geom_line(aes(x, var25, color = "delta25"), d10)+
  geom_line(aes(x, var50, color = "delta50"), d10)+
  scale_color_manual("Legende",values = c('delta10' = "blue", 'delta25' = "orange", 'delta50' = "red"),
                     labels = c('delta10'="Delta grid size 10",'delta25'="Delta grid size 25",'delta50' = "Delta grid size 50"))+
  labs(title = "",  x = "x", y = "Max_ALE(x)-Min_ALE(x)")

plot_v_3
```

```{r exampleALE2c, fig.cap='(ref:exampleALE2c)',  out.width='100%', echo=FALSE}

knitr::include_graphics("images/ALE_2_example2.1_delta_.PNG")

```
(ref:exampleALE2c) Delta of maximal and minimal estimated ALE for different grid sizes

It is clearly recognizable that on the one hand for higher x the variance in the ALE estimation increases for all grid sizes. The expected higher variance of the estimations with higher grid sizes is in particular revealed in the region from $x_1 = 7$ to $x_1 = 10$, because the estimation is quite sensible to the absolute value of $x_2$, which also increases with $x_1$.  



As the the theoretical ALE in this example was quite smooth, grid size 10 gave reasonable estimations. The following example shows problems that occur once the prediction function is quite shaky especially in regions with only few observations.

### Example 3: Unbalanced datasets and shaky prediction functions

In the 3rd example we assume $X_1 \sim N(10,3)$ as well as   
$X_2 \vert X_1 = x_1 \sim U(x_1 - 3, x_1 + 3 )$. 

``` {r eval = FALSE, include = FALSE}
prediction_function_3 <- function(x_1, x_2){
  y <-sin(10*x_1)*x_2
  return(y)
}


sample_normal_uniform <- function(n, FUN = prediction_function_3, a = 5, b = 1, c = 3, d = 3){
  x_1 <- rnorm(n = n, mean = a, sd = b)
  x_2 <- runif(n = n, min = x_1 - c, max = x_1 + d)
  y <- FUN(x_1, x_2)
  return(as.data.frame(cbind(x_1, x_2, y)))
}

set.seed(123)
dataset_1 <- sample_normal_uniform(1000, prediction_function_3, a = 10, b = 3, c = 3, d = 3)
p <- ggplot2::ggplot(dataset_1, ggplot2::aes(x_1, x_2))
p + ggplot2::geom_point() +
  ggplot2::labs(title = "", subtitle = "")


```

```{r datasetALE2,fig.cap='(ref:datasetALE2)',  out.width='100%', echo=FALSE}

knitr::include_graphics("images/ALE_2_dataset2_.PNG")
```
(ref:datasetALE2) Mixture of normal and uniform distributed features

For this example the sample size was 1000 (see figure \@ref(fig:datasetALE2)). As expected the correlation is clearly recognizable. This time only few data points lay in the outer regions, i.e. between 0 and 2.5 and 17.5 and 20, while there is a high concentration of data around the mean at 10.  
Furthermore we look at the prediction function $\hat{f}_3(x_1,x_2) = sin(10x_1)~x_2$. The calculation of the theoretical uncentered ALE (as before) yields $~~~~\widetilde{ALE}_{\hat{f},1}(x) = x~sin(x) + \frac{1}{10}cos(10x)$. For centering the expectation of the uncentered ALE, i.e. $E[\widetilde{ALE}_{\hat{f},1}(X_1)]$, was estimated by Monte-Carlo integration to be almost zero. 
As well as the prediction function, the theoretical ALE has lots of extreme points. This leads to some troubles especially for low grid sizes.
Figure \@ref(fig:example33gs) shows the estimated and the theoretical ALE for three different grid sizes. 

``` {r eval = FALSE, include = FALSE}
prediction_function_3 <- function(x_1, x_2){
  y <-sin(10*x_1)*x_2
  return(y)
}


sample_normal_uniform <- function(n, FUN = prediction_function_3, a = 10, b = 1, c = 3, d = 3){
  x_1 <- rnorm(n = n, mean = a, sd = b)
  x_2 <- runif(n = n, min = x_1 - c, max = x_1 + d)
  y <- FUN(x_1, x_2)
  return(as.data.frame(cbind(x_1, x_2, y)))
}

ew <- numeric(0)
for (i in 1:100){
  x <- numeric(0)
  x <- rnorm(100000, mean = 5, sd = 3)
  ew[i] = mean(x*sin(10*x)+ 1/10*cos(10*x))
  print(i)
}

sd(ew)
mean(ew)
EW <- mean(ew)
EW

set.seed(123)
dataset_2 <- sample_normal_uniform(1000, prediction_function_3, a = 10, b = 3, c = 3, d = 3)
p <- ggplot2::ggplot(dataset_2, ggplot2::aes(x_1, x_2))
#p + ggplot2::geom_point() +
ggplot2::labs(title = "Dataset 2", subtitle = "Normal distr.")



expected_ALE_3 <- function(x){

  ALE <- x*sin(10*x)+ 1/10*cos(10*x) - EW
  return (ALE)
}

fun.0 <- function(x) expected_ALE_3(x)

library(crs)
library(iml)

model = crs(y~., data = dataset_2)
predictor_3 = Predictor$new(model, data = dataset_2, y = "y", predict.fun = function(model, newdata){
  #a<-predict(model, newdata)
  a <- numeric(0)
  for (i in 1:nrow(newdata)){
    a[i]<- prediction_function_3(newdata[i, 1], newdata[i, 2])
  }
  return(a)
})

X_1 = dataset_2[which(names(df) != "y")]


ale.1 = FeatureEffect$new(predictor_3, feature = "x_1", grid.size = 100)
fun.1 <- function(x) ale.1$predict(x)

library(ggplot2)
plot1 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.1, aes(colour = "red")) +
  xlim(c(0,20)) +
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 100",  x = "x _ j", y = "ALE(x _ j)")
#plot1


ale.2 = FeatureEffect$new(predictor_3, feature = "x_1", grid.size = 1000)
fun.2 <- function(x) ale.2$predict(x)


plot2 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.2, aes(colour = "red")) +
  xlim(c(0,20)) +
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 1000",  x = "x _ j", y = "ALE(x _ j)")
#plot2


ale.3 = FeatureEffect$new(predictor_3, feature = "x_1", grid.size = 20)
fun.3 <- function(x) ale.3$predict(x)

library(ggplot2)
plot3 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.3, aes(colour = "red")) +
  xlim(c(0,20)) +
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 20",  x = "x _ j", y = "ALE(x _ j)")
#plot3


plot0 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  xlim(c(0,20)) +
  scale_colour_manual("Legende", values = c("green"), labels = c("theoretical ALE")) +
  labs(title = "",  x = "x _ j", y = "ALE(x _ j)")
#plot0



library(ggpubr)
ggarrange(plot3, plot1, plot2, ncol=1, nrow=3, common.legend = TRUE, legend="bottom")

```


```{r example33gs, fig.cap='(ref:example33gs)',  out.width='100%', echo=FALSE}

knitr::include_graphics("images/ALE_2_example3_3gs_.PNG")
```
(ref:example33gs) Theoretical vs. estimated ALE

For grid size 20 the local behaviour of the theoretical ALE is absolutley not recognizable. Only one peak left of the mean was estimated resonable, which is due to the high data intensity in this region. For the rest of the plot the grid intervals contain two or more peaks. Within each of them the ALE is estimated linear and therefore the true effect smoothed out.  
Increasing the grid size to 100 one nicley sees how the approximation becomes quite reasonable in the inner region, i.e. between 6 and 14, while in the outer region, where the intervalls still are too long the ALE continues to be estimated wrong. 
The more one increases the grid size the wider the inner region of good estimation becomes. Anyway still at grid size 1000 which implies only one data point per interval,  the estimations near the boundaries stay bad, as there are simply not sufficient observations to show the fine structure of the prediction function. As this was a constructed example, the latter shouldn't be overrated, as in real data situations it is quite improbable that a learner reslults in a that granular prediciton function within regions with such few data points. 
While in figure \@ref(fig:example33gs) apearentlly both grid sizes (100 and 1000)  result in equally good ALE estimations in the inner region, zooming in reveals that this isn't the case.

``` {r eval = FALSE, include = FALSE}
prediction_function_3 <- function(x_1, x_2){
  y <-sin(10*x_1)*x_2
  return(y)
}


sample_normal_uniform <- function(n, FUN = prediction_function_3, a = 10, b = 1, c = 3, d = 3){
  x_1 <- rnorm(n = n, mean = a, sd = b)
  x_2 <- runif(n = n, min = x_1 - c, max = x_1 + d)
  y <- FUN(x_1, x_2)
  return(as.data.frame(cbind(x_1, x_2, y)))
}

ew <- numeric(0)
for (i in 1:100){
  x <- numeric(0)
  x <- rnorm(100000, mean = 5, sd = 3)
  ew[i] = mean(x*sin(10*x)+ 1/10*cos(10*x))
  print(i)
}

sd(ew)
mean(ew)
EW <- mean(ew)
EW

set.seed(123)
dataset_2 <- sample_normal_uniform(1000, prediction_function_3, a = 10, b = 3, c = 3, d = 3)
p <- ggplot2::ggplot(dataset_2, ggplot2::aes(x_1, x_2))
#p + ggplot2::geom_point() +
ggplot2::labs(title = "Dataset 2", subtitle = "Normal distr.")



expected_ALE_3 <- function(x){

  ALE <- x*sin(10*x)+ 1/10*cos(10*x) - EW
  return (ALE)
}

fun.0 <- function(x) expected_ALE_3(x)

library(crs)
library(iml)

model = crs(y~., data = dataset_2)
predictor_3 = Predictor$new(model, data = dataset_2, y = "y", predict.fun = function(model, newdata){
  #a<-predict(model, newdata)
  a <- numeric(0)
  for (i in 1:nrow(newdata)){
    a[i]<- prediction_function_3(newdata[i, 1], newdata[i, 2])
  }
  return(a)
})

X_1 = dataset_2[which(names(df) != "y")]


ale.1 = FeatureEffect$new(predictor_3, feature = "x_1", grid.size = 100)
fun.1 <- function(x) ale.1$predict(x)

library(ggplot2)
plot1 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.1, aes(colour = "red")) +
  xlim(c(0,20)) +
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 100",  x = "x _ j", y = "ALE(x _ j)")
#plot1


ale.2 = FeatureEffect$new(predictor_3, feature = "x_1", grid.size = 1000)
fun.2 <- function(x) ale.2$predict(x)


plot2 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.2, aes(colour = "red")) +
  xlim(c(0,20)) +
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 1000",  x = "x _ j", y = "ALE(x _ j)")
#plot2


ale.3 = FeatureEffect$new(predictor_3, feature = "x_1", grid.size = 20)
fun.3 <- function(x) ale.3$predict(x)

library(ggplot2)
plot3 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.3, aes(colour = "red")) +
  xlim(c(0,20)) +
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 20",  x = "x _ j", y = "ALE(x _ j)")
#plot3


plot0 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  xlim(c(0,20)) +
  scale_colour_manual("Legende", values = c("green"), labels = c("theoretical ALE")) +
  labs(title = "",  x = "x _ j", y = "ALE(x _ j)")
#plot0



library(ggpubr)
#ggarrange(plot3, plot1, plot2, ncol=1, nrow=3, common.legend = TRUE, legend="bottom")


plot_zoom_1<- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.1, aes(colour = "red")) +
  xlim(c(9.5,10.5)) +
  ylim(c(-5,5))+
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 100",  x = "x _ j", y = "ALE(x _ j)")
#plot_zoom_1

plot_zoom_2<- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.0, aes(colour = "green")) +
  stat_function(fun = fun.2, aes(colour = "red")) +
  xlim(c(9.5,10.5)) +
  ylim(c(-5,5))+
  scale_colour_manual("Legende", values = c("green", "red"), labels = c("theoretical ALE", "estimated ALE")) +
  labs(title = "grid.size = 1000",  x = "x _ j", y = "ALE(x _ j)")
#plot_zoom_2


ggarrange(plot_zoom_1, plot_zoom_2, ncol=1, nrow=2, common.legend = TRUE, legend="bottom")

```


```{r example3zoom, fig.cap='(ref:example3zoom)',  out.width='100%', echo=FALSE}

knitr::include_graphics("images/ALE_2_example3_zoom_.PNG")
```
(ref:example3zoom) Zooming in reveals the bias for grid size 1000 

Figure \@ref(fig:example3zoom) shows a very small part around the mean. 
As expected the estimations for grid size 100 are a little closer to the theoretical ALE as again the true effect of the second feature, which still affects the prediction, is better estimated within each interval (10 observations vs 1 observation).

At the end of this section we have seen a good example for the natural trade off between small intervals on the one hand and sufficient data to get a good and stable estimation on the other hand. The optimal choice of the number /size of intervals thereby higly depends on the given prediction function and the data. This can be taken as main message of the section. The next section shall provide the reader with the understanding of in how far additional problems can occur in the context of piece-wise constant models.       



## Problems with piece-wise constant models

Piece-wise constant models such as for example decision trees and random forests don't have continous prediction functions, which implies they are not differentiable. Thus the concept of theoretical ALE doesn't make any sense in this context as the partial derivative doesn't exist. Still it is possible to estimate the ALE as the "jump" will result in a more or less steep linear part, depending on the interval size of the interval containing the step. It is intuitive that the goodness of the estimation highly dependes on if one manages to place the intervals quite narrow around the steps.
As the following examples will show, problems can occur due to "wrong" interval sizes or unluckily distributed data in the region of the steps.


###Example 4: Simple step function

Throughout this section we assume $X_1$ to be uniformily distributed on the interval $[0,10]$, i.e. $X_1 \sim U(0,10)$ as well as $X_2$ given $X_1$ uniformily on the interval $[max(x_1 - 3, 0),~min( x_1 + 3, 10]$,   
i.e. $X_2 \vert X_1 = x_1 \sim U(max(x_1 - 3, 0),~min( x_1 + 3, 10) )$. That means all the data is distributed within the 10 times 10 square.
In the the first example we take a look on a simple prediction function to get a good understanding of the basic problem with piece-wise constant models. We assume a prediction function that independently of $x_2$ predicts 0, except if $x_1$ falls into a certain small interval arround 5. In this case it predicts 10. Concretely $f(x_1, x_2) =    1_{[4.9,~5.1]}(x_1) * 10$.   


``` {r eval = FALSE, include = FALSE}
sample_uniform_sq <- function(n, FUN = prediction_function_4_1, a = 3, b = 3){
  x_1 <- runif(n = n, min = 0, max = 10)
  x_2 <- runif(n = n, min = pmax(x_1 - a, 0), max = pmin(x_1 + b, 10))
  y <- numeric(0)
  for (i in 1:n){
    y[i] <- FUN(x_1[i], x_2[i])
  }
  return(as.data.frame(cbind(x_1, x_2, y)))
}


library (ggplot2)
prediction_function_4_4 <- function(x_1, x_2){
  if(x_1 < 4.9){
    y = 0
  }else if(x_1> 5.1){
    y = 0
  } else {
    y =10
  }
  return(y)
}
set.seed(1)
dataset_3_3 <- sample_uniform_sq(100, prediction_function_4_4, a = 3, b = 3)

p <- ggplot2::ggplot(dataset_3_3, ggplot2::aes(x_1, x_2))
p + ggplot2::geom_point() +
  ggplot2::labs(title = "")+
  geom_segment(aes(x = 4.9, y = 0, xend = 4.9, yend = 9))+
  geom_segment(aes(x = 5.1, y = 0, xend = 5.1, yend = 9))+
  annotate("text", x = 5, y = 9.5, label = "f   =   10", color = "red") +
  annotate("text", x = 5 - 10/3, y = 5+10/3, label = "f = 0", color = "red") +
  annotate("text", x = 5 + 10/3, y = 5-10/3, label = "f = 0", color = "red")




library(crs)
library(iml)

model = crs(y~., data = dataset_3_3)
predictor_4_4 = Predictor$new(model, data = dataset_3_3, y = "y", predict.fun = function(model, newdata){
  #a<-predict(model, newdata)
  a <- numeric(0)
  for (i in 1:nrow(newdata)){
    a[i]<- prediction_function_4_4(newdata[i, 1], newdata[i, 2])
  }
  return(a)
})

ale.1 = FeatureEffect$new(predictor_4_4, feature = "x_1", grid.size = 20)
fun.1 <- function(x) ale.1$predict(x)


plot_20 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.1, aes(colour = "red")) +
  xlim(c(0,10)) +
  ylim(c(-10,10)) +
  scale_colour_manual("Legende", values = c("red"), labels = c( "estimated ALE")) +
  labs(title = "grid.size = 20",  x = "x _ j", y = "ALE(x _ j)")


ale.2 = FeatureEffect$new(predictor_4_4, feature = "x_1", grid.size = 30)
fun.2 <- function(x) ale.2$predict(x)


plot_30 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.2, aes(colour = "red")) +
  xlim(c(0,10)) +
  ylim(c(-10,10)) +
  scale_colour_manual("Legende", values = c("red"), labels = c( "estimated ALE")) +
  labs(title = "grid.size = 30",  x = "x _ j", y = "ALE(x _ j)")

ale.3 = FeatureEffect$new(predictor_4_4, feature = "x_1", grid.size = 50)
fun.3 <- function(x) ale.3$predict(x)


plot_50 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.3, aes(colour = "red")) +
  xlim(c(0,10)) +
  ylim(c(-10,10)) +
  scale_colour_manual("Legende", values = c("red"), labels = c( "estimated ALE")) +
  labs(title = "grid.size = 50",  x = "x _ j", y = "ALE(x _ j)")

ale.4 = FeatureEffect$new(predictor_4_4, feature = "x_1", grid.size = 100)
fun.4 <- function(x) ale.4$predict(x)


plot_100 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.4, aes(colour = "red")) +
  xlim(c(0,10)) +
  ylim(c(-10,10)) +
  scale_colour_manual("Legende", values = c("red"), labels = c( "estimated ALE")) +
  labs(title = "grid.size = 100",  x = "x _ j", y = "ALE(x _ j)")

ggpubr::ggarrange(plot_20, plot_30,plot_50, plot_100, ncol=2, nrow=2, common.legend = TRUE, legend="bottom")

```

Figure  \@ref(fig:pwcexample4datasetpredf) shows a sampled data set of 100 datapoints and a sketch of the prediction function. 

```{r pwcexample4datasetpredf, fig.cap='(ref:pwcexample4datasetpredf)', ,  out.width='100%', echo=FALSE}

knitr::include_graphics("images/ALE_2_pwc_example4_dataset_predf_.PNG")
```
(ref:pwcexample4datasetpredf) Prediction Function 1

As mentioned above a good estimation of the ALE would result into quite steep linear parts, one around 4.9 and a second inverse one around 5.1. The problem now is that the ALE estimation won't catch those jumps as long as both jumps lay within the same interval. The reason is that all the points within this interval would be moved to the interval boundaries which lay outside the area, where the prediction function predicts 10. This leads to an estimation of the local effect as zero.
Figure \@ref(fig:pwcexample44plots) shows the estimates for different grid sizes 20, 30, 50 and 100. 

``` {r eval = FALSE, include = FALSE}

library(crs)
library(iml)

model = crs(y~., data = dataset_3_3)
predictor_4_4 = Predictor$new(model, data = dataset_3_3, y = "y", predict.fun = function(model, newdata){
  #a<-predict(model, newdata)
  a <- numeric(0)
  for (i in 1:nrow(newdata)){
    a[i]<- prediction_function_4_4(newdata[i, 1], newdata[i, 2])
  }
  return(a)
})

ale.1 = FeatureEffect$new(predictor_4_4, feature = "x_1", grid.size = 20)
fun.1 <- function(x) ale.1$predict(x)


plot_20 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.1, aes(colour = "red")) +
  xlim(c(0,10)) +
  ylim(c(-10,10)) +
  scale_colour_manual("Legende", values = c("red"), labels = c( "estimated ALE")) +
  labs(title = "grid.size = 20",  x = "x _ j", y = "ALE(x _ j)")


ale.2 = FeatureEffect$new(predictor_4_4, feature = "x_1", grid.size = 30)
fun.2 <- function(x) ale.2$predict(x)


plot_30 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.2, aes(colour = "red")) +
  xlim(c(0,10)) +
  ylim(c(-10,10)) +
  scale_colour_manual("Legende", values = c("red"), labels = c( "estimated ALE")) +
  labs(title = "grid.size = 30",  x = "x _ j", y = "ALE(x _ j)")

ale.3 = FeatureEffect$new(predictor_4_4, feature = "x_1", grid.size = 50)
fun.3 <- function(x) ale.3$predict(x)


plot_50 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.3, aes(colour = "red")) +
  xlim(c(0,10)) +
  ylim(c(-10,10)) +
  scale_colour_manual("Legende", values = c("red"), labels = c( "estimated ALE")) +
  labs(title = "grid.size = 50",  x = "x _ j", y = "ALE(x _ j)")

ale.4 = FeatureEffect$new(predictor_4_4, feature = "x_1", grid.size = 100)
fun.4 <- function(x) ale.4$predict(x)


plot_100 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.4, aes(colour = "red")) +
  xlim(c(0,10)) +
  ylim(c(-10,10)) +
  scale_colour_manual("Legende", values = c("red"), labels = c( "estimated ALE")) +
  labs(title = "grid.size = 100",  x = "x _ j", y = "ALE(x _ j)")

ggpubr::ggarrange(plot_20, plot_30,plot_50, plot_100, ncol=2, nrow=2, common.legend = TRUE, legend="bottom")



```




```{r pwcexample44plots, fig.cap='(ref:pwcexample44plots)',  out.width='100%', echo=FALSE}

knitr::include_graphics("images/ALE_2_pwc_example4_4plots_.PNG")
```
(ref:pwcexample44plots) Behaviour of ALE estimation with increasing grid size

As expected the ALE estimations with grid size 20 and 30 are not sensitive to the effect. Increasing the grid size ensures that some interval boundaries fall into the interval $[4.9, ~5.1]$ which exposes the step of the prediction function.
Having a second look on the data situation in this example, one notes that only 2 datapoints fall to the interval $[4.9, ~5.1]$. Grid size 50 implies for 100 datapoints 2 datapoints per interval. That means that we even got lucky in this example that the 2 data points didn't fall into the same grid interval. Otherwise the effect would have remained hidden even at grid size 50. The following example shows how unluckily distributed datapoints can lead to bad ALE estimations.  


###Example 5: Two dimensional step functions and unluckily distributed data 

We assume the same data distribution as in the former example. Furthermore well take a look on two prediciton functions, one independent of $x_2$ defined as $f(x_1, x_2) =  1 +  1_{[0,~\frac{10}{3}]}(x_1)+ 1_{[\frac{10}{3},~\frac{20}{3}]}(x_1)$. The second also depends on $x_2$ and is defined as 
$f(x_1, x_2) =   3~(1_{[\frac{10}{3},~\frac{20}{3}]}(x_1)~*~1_{[\frac{10}{3},~\frac{20}{3}]}(x_2)) +    2~(1_{[0,~\frac{10}{3}]}(x_1)~*~(1_{[0,~\frac{10}{3}]}(x_2)+1_{[\frac{20}{3}, ~10]}(x_2)) +    ~(1_{[\frac{20}{3},~10]}(x_1)~*~(1_{[0,~\frac{10}{3}]}(x_2)+1_{[\frac{20}{3}, ~10]}(x_2))$ .
Both on the first sigth a little unhandy become quite easy to understand looking at the sketches below (see figures \@ref(fig:pwcexample5predf1) and \@ref(fig:pwcexample5predf2)). 

``` {r eval = FALSE, include = FALSE}
prediction_function_4_1 <- function(x_1, x_2){
  if(x_1 < 10/3){
    y = 2
  }else if(x_1> 20/3){
    y = 1
  } else {
    y =3
  }
  return(y)
}



sample_uniform_sq <- function(n, FUN = prediction_function_4_1, a = 3, b = 3){
  x_1 <- runif(n = n, min = 0, max = 10)
  x_2 <- runif(n = n, min = pmax(x_1 - a, 0), max = pmin(x_1 + b, 10))
  y <- numeric(0)
  for (i in 1:n){
    y[i] <- FUN(x_1[i], x_2[i])
  }
  return(as.data.frame(cbind(x_1, x_2, y)))
}
set.seed(123)
dataset_3 <- sample_uniform_sq(100, prediction_function_4_1, a = 3, b = 3)



p <- ggplot2::ggplot(dataset_3, ggplot2::aes(x_1, x_2))


p + ggplot2::geom_point() +
  ggplot2::labs(title = "", subtitle = "")+
  geom_segment(aes(x = 10/3, y = 0, xend = 10/3, yend = 10))+
  geom_segment(aes(x = 20/3, y = 0, xend = 20/3, yend = 10))+
  geom_segment(aes(y = 10/3, x = 0, yend = 10/3, xend = 10))+
  geom_segment(aes(y = 20/3, x = 0, yend = 20/3, xend = 10))+
  annotate("text", x = 5, y = 5, label = "f = 3", color = "red") +
  annotate("text", x = 5 - 10/3, y = 5, label = "f = 2", color = "red") +
  annotate("text", x = 5 + 10/3, y = 5, label = "f = 1", color = "red") +
  annotate("text", x = 5, y = 5-10/3, label = "f = 3", color = "red") +
  annotate("text", x = 5 - 10/3, y = 5-10/3, label = "f = 2", color = "red") +
  annotate("text", x = 5 + 10/3, y = 5-10/3, label = "f = 1", color = "red")  +
  annotate("text", x = 5, y = 5+10/3, label = "f = 3", color = "red") +
  annotate("text", x = 5 - 10/3, y = 5+10/3, label = "f = 2", color = "red") +
  annotate("text", x = 5 + 10/3, y = 5+10/3, label = "f = 1", color = "red")



```

```{r pwcexample5predf1, fig.cap='(ref:pwcexample5predf1)', out.width='100%', echo=FALSE}

knitr::include_graphics("images/ALE_2_pwc_example5_predf_1_.PNG")

```
(ref:pwcexample5predf1) Prediction function 2

``` {r eval = FALSE, include = FALSE}
prediction_function_4_1 <- function(x_1, x_2){
  if(x_1 < 10/3){
    y = 2
  }else if(x_1> 20/3){
    y = 1
  } else {
    y =3
  }
  return(y)
}



sample_uniform_sq <- function(n, FUN = prediction_function_4_1, a = 3, b = 3){
  x_1 <- runif(n = n, min = 0, max = 10)
  x_2 <- runif(n = n, min = pmax(x_1 - a, 0), max = pmin(x_1 + b, 10))
  y <- numeric(0)
  for (i in 1:n){
    y[i] <- FUN(x_1[i], x_2[i])
  }
  return(as.data.frame(cbind(x_1, x_2, y)))
}
set.seed(123)
dataset_3 <- sample_uniform_sq(100, prediction_function_4_1, a = 3, b = 3)



p <- ggplot2::ggplot(dataset_3, ggplot2::aes(x_1, x_2))


p + ggplot2::geom_point() +
  ggplot2::labs(title = "", subtitle = "")+
  geom_segment(aes(x = 10/3, y = 0, xend = 10/3, yend = 10))+
  geom_segment(aes(x = 20/3, y = 0, xend = 20/3, yend = 10))+
  geom_segment(aes(y = 10/3, x = 0, yend = 10/3, xend = 10))+
  geom_segment(aes(y = 20/3, x = 0, yend = 20/3, xend = 10))+
  annotate("text", x = 5, y = 5, label = "f = 3", color = "red") +
  annotate("text", x = 5 - 10/3, y = 5, label = "f = 0", color = "red") +
  annotate("text", x = 5 + 10/3, y = 5, label = "f = 0", color = "red") +
  annotate("text", x = 5, y = 5-10/3, label = "f = 0", color = "red") +
  annotate("text", x = 5 - 10/3, y = 5-10/3, label = "f = 2", color = "red") +
  annotate("text", x = 5 + 10/3, y = 5-10/3, label = "f = 1", color = "red")  +
  annotate("text", x = 5, y = 5+10/3, label = "f = 0", color = "red") +
  annotate("text", x = 5 - 10/3, y = 5+10/3, label = "f = 2", color = "red") +
  annotate("text", x = 5 + 10/3, y = 5+10/3, label = "f = 1", color = "red")


```

```{r pwcexample5predf2, fig.cap='(ref:pwcexample5predf2)', out.width='100%', echo=FALSE}

knitr::include_graphics("images/ALE_2_pwc_example5_predf_2_.PNG")
```
(ref:pwcexample5predf2) Prediction function 3

``` {r eval = FALSE, include = FALSE}


prediction_function_4_1 <- function(x_1, x_2){
  if(x_1 < 10/3){
    y = 2
  }else if(x_1> 20/3){
    y = 1
  } else {
    y =3
  }
  return(y)
}


sample_uniform_sq <- function(n, FUN = prediction_function_4_1, a = 3, b = 3){
  x_1 <- runif(n = n, min = 0, max = 10)
  x_2 <- runif(n = n, min = pmax(x_1 - a, 0), max = pmin(x_1 + b, 10))
  y <- numeric(0)
  for (i in 1:n){
    y[i] <- FUN(x_1[i], x_2[i])
  }
  return(as.data.frame(cbind(x_1, x_2, y)))
}

dataset_3 <- sample_uniform_sq(100, prediction_function_4_1, a = 3, b = 3)


library(crs)
library(iml)

model = crs(y~., data = dataset_3)
predictor_4_1 = Predictor$new(model, data = dataset_3, y = "y", predict.fun = function(model, newdata){
  #a<-predict(model, newdata)
  a <- numeric(0)
  for (i in 1:nrow(newdata)){
    a[i]<- prediction_function_4_1(newdata[i, 1], newdata[i, 2])
  }
  return(a)
})

X_1 = dataset_3[which(names(df) != "y")]


ale.1 = FeatureEffect$new(predictor_4_1, feature = "x_1", grid.size = 5)
fun.1 <- function(x) ale.1$predict(x)

library(ggplot2)
plot1 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.1, aes(colour = "red")) +
  xlim(c(0,10)) +
  ylim(c(-2,2)) +
  scale_colour_manual("Legende", values = c("red"), labels = c( "estimated ALE")) +
  labs(title = "grid.size = 5",  x = "x _ j", y = "ALE(x _ j)")
#plot1



prediction_function_4_2 <- function(x_1, x_2){
  if(x_1 < 10/3){
    if(x_2 < 10/3){
      y = 2
    }else if(x_2> 20/3){
      y = 2
    }else{
      y = 0
    }

  }else if(x_1> 20/3){
    if(x_2 < 10/3){
      y = 1
    }else if(x_2> 20/3){
      y = 1
    }else{
      y = 0
    }
  } else {
    if(x_2 < 10/3){
      y = 0
    }else if(x_2> 20/3){
      y = 0
    }else{
      y = 3
    }
  }
  return(y)
}


set.seed(123)
dataset_3 <- sample_uniform_sq(100, prediction_function_4_2, a = 3, b = 3)
p <- ggplot2::ggplot(dataset_3, ggplot2::aes(x_1, x_2))
#p + ggplot2::geom_point() +
ggplot2::labs(title = "Dataset 3", subtitle = "Uniform ")


library(crs)
library(iml)

model = crs(y~., data = dataset_3)
predictor_4_2 = Predictor$new(model, data = dataset_3, y = "y", predict.fun = function(model, newdata){
  #a<-predict(model, newdata)
  a <- numeric(0)
  for (i in 1:nrow(newdata)){
    a[i]<- prediction_function_4_2(newdata[i, 1], newdata[i, 2])
  }
  return(a)
})

X_1 = dataset_3[which(names(df) != "y")]


ale.2 = FeatureEffect$new(predictor_4_2, feature = "x_1", grid.size = 5)
fun.2 <- function(x) ale.2$predict(x)

library(ggplot2)
plot2 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.2, aes(colour = "red")) +
  xlim(c(0,10)) +
  ylim(c(-2,2)) +
  scale_colour_manual("Legende", values = c("red"), labels = c( "estimated ALE")) +
  labs(title = "grid.size = 5",  x = "x _ j", y = "ALE(x _ j)")

#plot2
#plot1

gs5<-ggpubr::ggarrange(plot1, plot2, ncol=2, nrow=1, common.legend = TRUE, legend="bottom")

#gs5

ale.1 = FeatureEffect$new(predictor_4_1, feature = "x_1", grid.size = 10)
fun.1 <- function(x) ale.1$predict(x)

library(ggplot2)
plot1_10 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.1, aes(colour = "red")) +
  xlim(c(0,10)) +
  ylim(c(-2,2)) +
  scale_colour_manual("Legende", values = c("red"), labels = c( "estimated ALE")) +
  labs(title = "grid.size = 10",  x = "x _ j", y = "ALE(x _ j)")
#plot1_10
ale.2 = FeatureEffect$new(predictor_4_2, feature = "x_1", grid.size = 10)
fun.2 <- function(x) ale.2$predict(x)

library(ggplot2)
plot2_10 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.2, aes(colour = "red")) +
  xlim(c(0,10)) +
  ylim(c(-2,2)) +
  scale_colour_manual("Legende", values = c("red"), labels = c( "estimated ALE")) +
  labs(title = "grid.size = 10",  x = "x _ j", y = "ALE(x _ j)")

gs10<-ggpubr::ggarrange(plot1_10, plot2_10, ncol=2, nrow=1, common.legend = TRUE, legend="bottom")

#gs10
ale.1 = FeatureEffect$new(predictor_4_1, feature = "x_1", grid.size = 20)
fun.1 <- function(x) ale.1$predict(x)

library(ggplot2)
plot1_20 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.1, aes(colour = "red")) +
  xlim(c(0,10)) +
  ylim(c(-2,2)) +
  scale_colour_manual("Legende", values = c("red"), labels = c( "estimated ALE")) +
  labs(title = "grid.size = 20",  x = "x _ j", y = "ALE(x _ j)")
#plot1_20
ale.2 = FeatureEffect$new(predictor_4_2, feature = "x_1", grid.size = 20)
fun.2 <- function(x) ale.2$predict(x)

library(ggplot2)
plot2_20 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.2, aes(colour = "red")) +
  xlim(c(0,10)) +
  ylim(c(-2,2)) +
  scale_colour_manual("Legende", values = c("red"), labels = c( "estimated ALE")) +
  labs(title = "grid.size = 20",  x = "x _ j", y = "ALE(x _ j)")


gs20<-ggpubr::ggarrange(plot1_20, plot2_20, ncol=2, nrow=1, common.legend = TRUE, legend="bottom")
#gs20

ale.1 = FeatureEffect$new(predictor_4_1, feature = "x_1", grid.size = 50)
fun.1 <- function(x) ale.1$predict(x)

library(ggplot2)
plot1_50 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.1, aes(colour = "red")) +
  xlim(c(0,10)) +
  ylim(c(-2,2)) +
  scale_colour_manual("Legende", values = c("red"), labels = c( "estimated ALE")) +
  labs(title = "grid.size = 50",  x = "x _ j", y = "ALE(x _ j)")
#plot1_50
ale.2 = FeatureEffect$new(predictor_4_2, feature = "x_1", grid.size = 50)
fun.2 <- function(x) ale.2$predict(x)

library(ggplot2)
plot2_50 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.2, aes(colour = "red")) +
  xlim(c(0,10)) +
  ylim(c(-2,2)) +
  scale_colour_manual("Legende", values = c("red"), labels = c( "estimated ALE")) +
  labs(title = "grid.size = 50",  x = "x _ j", y = "ALE(x _ j)")


gs50<-ggpubr::ggarrange(plot1_50, plot2_50, ncol=2, nrow=1, common.legend = TRUE, legend="bottom")
#gs50


gs_all1<-ggpubr::ggarrange(gs5,gs10,  ncol=1, nrow=2, common.legend = TRUE, legend="bottom")
gs_all1
gs_all2<-ggpubr::ggarrange(gs20,gs50,  ncol=1, nrow=2, common.legend = TRUE, legend="bottom")
#gs_all2


```

In the following the ALE was estimated for increasing grid sizes. In figure \@ref(fig:pwcexample5gs510) starting with grid size 5 on the left side we see the behaviour of the first prediction function on the right side for the second one.

```{r pwcexample5gs510, fig.cap='(ref:pwcexample5gs510)', out.width='100%', echo=FALSE}

knitr::include_graphics("images/ALE_2_pwc_example5_gs5_10_.PNG")
```
(ref:pwcexample5gs510) Behaviour of ALE estimations for prediction function 2 (leftside) and 3 (rightside)

For grid size 5 both estimations recognize the step but estimate it relativley flat, which is not very suprising as the interval length should be around 2. For the first prediction function we see a total increase within the second grid interval of 1 and a total decrease in the fourth one of 2. This reflects the behaviour of the prediction function, so the only problem is the low grid size. For the second prediction functions the total changes are estimated to be much lower. This is due to the areas of 0 prediction which clearly influence the mean change in prediction, as some data points change from 0 to 3, but others from 2 to 0 within the second grid interval, as well as from 3 to 0 and from 0 to 1 within the fourth grid interval. So the absolut effect of $x_1$ is relativized by the influence of $x_2$, which is intended by the concept of ALE.
At grid size 10 the estimations for both prediction functions look quite similar. The steps become steeper as the grid intervals shrink to half their length. The estimated change in prediction for the second prediction function now is even bigger than for the first one. Due to the correlation now more (relatively more) datapoints are shifted from prediction 0 to 3 and 3 to 0 respectivly, which leads to the sligthly higher estimation of the effect. 

``` {r eval = FALSE, include = FALSE}

prediction_function_4_1 <- function(x_1, x_2){
  if(x_1 < 10/3){
    y = 2
  }else if(x_1> 20/3){
    y = 1
  } else {
    y =3
  }
  return(y)
}


sample_uniform_sq <- function(n, FUN = prediction_function_4_1, a = 3, b = 3){
  x_1 <- runif(n = n, min = 0, max = 10)
  x_2 <- runif(n = n, min = pmax(x_1 - a, 0), max = pmin(x_1 + b, 10))
  y <- numeric(0)
  for (i in 1:n){
    y[i] <- FUN(x_1[i], x_2[i])
  }
  return(as.data.frame(cbind(x_1, x_2, y)))
}

dataset_3 <- sample_uniform_sq(100, prediction_function_4_1, a = 3, b = 3)


library(crs)
library(iml)

model = crs(y~., data = dataset_3)
predictor_4_1 = Predictor$new(model, data = dataset_3, y = "y", predict.fun = function(model, newdata){
  #a<-predict(model, newdata)
  a <- numeric(0)
  for (i in 1:nrow(newdata)){
    a[i]<- prediction_function_4_1(newdata[i, 1], newdata[i, 2])
  }
  return(a)
})

X_1 = dataset_3[which(names(df) != "y")]


ale.1 = FeatureEffect$new(predictor_4_1, feature = "x_1", grid.size = 5)
fun.1 <- function(x) ale.1$predict(x)

library(ggplot2)
plot1 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.1, aes(colour = "red")) +
  xlim(c(0,10)) +
  ylim(c(-2,2)) +
  scale_colour_manual("Legende", values = c("red"), labels = c( "estimated ALE")) +
  labs(title = "grid.size = 5",  x = "x _ j", y = "ALE(x _ j)")
#plot1



prediction_function_4_2 <- function(x_1, x_2){
  if(x_1 < 10/3){
    if(x_2 < 10/3){
      y = 2
    }else if(x_2> 20/3){
      y = 2
    }else{
      y = 0
    }

  }else if(x_1> 20/3){
    if(x_2 < 10/3){
      y = 1
    }else if(x_2> 20/3){
      y = 1
    }else{
      y = 0
    }
  } else {
    if(x_2 < 10/3){
      y = 0
    }else if(x_2> 20/3){
      y = 0
    }else{
      y = 3
    }
  }
  return(y)
}


set.seed(123)
dataset_3 <- sample_uniform_sq(100, prediction_function_4_2, a = 3, b = 3)
p <- ggplot2::ggplot(dataset_3, ggplot2::aes(x_1, x_2))
#p + ggplot2::geom_point() +
  ggplot2::labs(title = "Dataset 3", subtitle = "Uniform ")


library(crs)
library(iml)

model = crs(y~., data = dataset_3)
predictor_4_2 = Predictor$new(model, data = dataset_3, y = "y", predict.fun = function(model, newdata){
  #a<-predict(model, newdata)
  a <- numeric(0)
  for (i in 1:nrow(newdata)){
    a[i]<- prediction_function_4_2(newdata[i, 1], newdata[i, 2])
  }
  return(a)
})

X_1 = dataset_3[which(names(df) != "y")]


ale.2 = FeatureEffect$new(predictor_4_2, feature = "x_1", grid.size = 5)
fun.2 <- function(x) ale.2$predict(x)

library(ggplot2)
plot2 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.2, aes(colour = "red")) +
  xlim(c(0,10)) +
  ylim(c(-2,2)) +
  scale_colour_manual("Legende", values = c("red"), labels = c( "estimated ALE")) +
  labs(title = "grid.size = 5",  x = "x _ j", y = "ALE(x _ j)")

#plot2
#plot1

gs5<-ggpubr::ggarrange(plot1, plot2, ncol=2, nrow=1, common.legend = TRUE, legend="bottom")

#gs5

ale.1 = FeatureEffect$new(predictor_4_1, feature = "x_1", grid.size = 10)
fun.1 <- function(x) ale.1$predict(x)

library(ggplot2)
plot1_10 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.1, aes(colour = "red")) +
  xlim(c(0,10)) +
  ylim(c(-2,2)) +
  scale_colour_manual("Legende", values = c("red"), labels = c( "estimated ALE")) +
  labs(title = "grid.size = 10",  x = "x _ j", y = "ALE(x _ j)")
#plot1_10
ale.2 = FeatureEffect$new(predictor_4_2, feature = "x_1", grid.size = 10)
fun.2 <- function(x) ale.2$predict(x)

library(ggplot2)
plot2_10 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.2, aes(colour = "red")) +
  xlim(c(0,10)) +
  ylim(c(-2,2)) +
  scale_colour_manual("Legende", values = c("red"), labels = c( "estimated ALE")) +
  labs(title = "grid.size = 10",  x = "x _ j", y = "ALE(x _ j)")

gs10<-ggpubr::ggarrange(plot1_10, plot2_10, ncol=2, nrow=1, common.legend = TRUE, legend="bottom")

#gs10
ale.1 = FeatureEffect$new(predictor_4_1, feature = "x_1", grid.size = 20)
fun.1 <- function(x) ale.1$predict(x)

library(ggplot2)
plot1_20 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.1, aes(colour = "red")) +
  xlim(c(0,10)) +
  ylim(c(-2,2)) +
  scale_colour_manual("Legende", values = c("red"), labels = c( "estimated ALE")) +
  labs(title = "grid.size = 20",  x = "x _ j", y = "ALE(x _ j)")
#plot1_20
ale.2 = FeatureEffect$new(predictor_4_2, feature = "x_1", grid.size = 20)
fun.2 <- function(x) ale.2$predict(x)

library(ggplot2)
plot2_20 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.2, aes(colour = "red")) +
  xlim(c(0,10)) +
  ylim(c(-2,2)) +
  scale_colour_manual("Legende", values = c("red"), labels = c( "estimated ALE")) +
  labs(title = "grid.size = 20",  x = "x _ j", y = "ALE(x _ j)")


gs20<-ggpubr::ggarrange(plot1_20, plot2_20, ncol=2, nrow=1, common.legend = TRUE, legend="bottom")
#gs20

ale.1 = FeatureEffect$new(predictor_4_1, feature = "x_1", grid.size = 50)
fun.1 <- function(x) ale.1$predict(x)

library(ggplot2)
plot1_50 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.1, aes(colour = "red")) +
  xlim(c(0,10)) +
  ylim(c(-2,2)) +
  scale_colour_manual("Legende", values = c("red"), labels = c( "estimated ALE")) +
  labs(title = "grid.size = 50",  x = "x _ j", y = "ALE(x _ j)")
#plot1_50
ale.2 = FeatureEffect$new(predictor_4_2, feature = "x_1", grid.size = 50)
fun.2 <- function(x) ale.2$predict(x)

library(ggplot2)
plot2_50 <- ggplot(data = data.frame(x = 0), mapping = aes(x = x)) +
  stat_function(fun = fun.2, aes(colour = "red")) +
  xlim(c(0,10)) +
  ylim(c(-2,2)) +
  scale_colour_manual("Legende", values = c("red"), labels = c( "estimated ALE")) +
  labs(title = "grid.size = 50",  x = "x _ j", y = "ALE(x _ j)")


gs50<-ggpubr::ggarrange(plot1_50, plot2_50, ncol=2, nrow=1, common.legend = TRUE, legend="bottom")
#gs50


gs_all1<-ggpubr::ggarrange(gs5,gs10,  ncol=1, nrow=2, common.legend = TRUE, legend="bottom")
#gs_all1
gs_all2<-ggpubr::ggarrange(gs20,gs50,  ncol=1, nrow=2, common.legend = TRUE, legend="bottom")
gs_all2

```

```{r pwcexample5gs2050, fig.cap='(ref:pwcexample5gs2050)', out.width='100%', echo=FALSE}

knitr::include_graphics("images/ALE_2_pwc_example5_gs20_50_.PNG")

```
(ref:pwcexample5gs2050) Behaviour of ALE estimations for prediction function 2 (leftside) and 3 (rightside)

Increasing the grid size first to 20 and then to 50 reveals the whole danger of this situation. While prediction function 2 seems to be estimated quite stable (the absolute changes stay to be 1 and -2, while the steps become steeper and steeper), the estimation for prediction function 3 changes its behaviour. At grid size 20 the left step grows to be 3, at grid size 50 the second step to be -3. Centering leads to quite radical upwards and downwards shifts of the whole plot. 
To understand this we'll have a look on the data points that are used to estimate the steps. 
``` {r eval = FALSE, include = FALSE}
prediction_function_4_1 <- function(x_1, x_2){
  if(x_1 < 10/3){
    y = 2
  }else if(x_1> 20/3){
    y = 1
  } else {
    y =3
  }
  return(y)
}



sample_uniform_sq <- function(n, FUN = prediction_function_4_1, a = 3, b = 3){
  x_1 <- runif(n = n, min = 0, max = 10)
  x_2 <- runif(n = n, min = pmax(x_1 - a, 0), max = pmin(x_1 + b, 10))
  y <- numeric(0)
  for (i in 1:n){
    y[i] <- FUN(x_1[i], x_2[i])
  }
  return(as.data.frame(cbind(x_1, x_2, y)))
}
set.seed(123)
dataset_3 <- sample_uniform_sq(100, prediction_function_4_1, a = 3, b = 3)



p <- ggplot2::ggplot(dataset_3, ggplot2::aes(x_1, x_2))




p + ggplot2::geom_point() +
  ggplot2::labs(title = "", subtitle = "")+
  geom_segment(aes(x = 10/3, y = 0, xend = 10/3, yend = 10))+
  geom_segment(aes(x = 20/3, y = 0, xend = 20/3, yend = 10))+
  geom_segment(aes(y = 10/3, x = 0, yend = 10/3, xend = 10))+
  geom_segment(aes(y = 20/3, x = 0, yend = 20/3, xend = 10))+
  annotate("text", x = 5, y = 5, label = "f = 3", color = "red") +
  annotate("text", x = 5 - 10/3, y = 5, label = "f = 0", color = "red") +
  annotate("text", x = 5 + 10/3, y = 5, label = "f = 0", color = "red") +
  annotate("text", x = 5, y = 5-10/3, label = "f = 0", color = "red") +
  annotate("text", x = 5 - 10/3, y = 5-10/3, label = "f = 2", color = "red") +
  annotate("text", x = 5 + 10/3, y = 5-10/3, label = "f = 1", color = "red")  +
  annotate("text", x = 5, y = 5+10/3, label = "f = 0", color = "red") +
  annotate("text", x = 5 - 10/3, y = 5+10/3, label = "f = 2", color = "red") +
  annotate("text", x = 5 + 10/3, y = 5+10/3, label = "f = 1", color = "red") +
  geom_point(size = 5,data =dataset_3[order(dataset_3$x_1)[31],] , pch = 1, color = "red")+
  geom_point(size = 5,data =dataset_3[order(dataset_3$x_1)[32],] , pch = 1, color = "red")+
  geom_point(size = 5,data =dataset_3[order(dataset_3$x_1)[33],] , pch = 1, color = "red")+
  geom_point(size = 5,data =dataset_3[order(dataset_3$x_1)[34],] , pch = 1, color = "red")+
  geom_point(size = 5,data =dataset_3[order(dataset_3$x_1)[35],] , pch = 1, color = "red")


```


```{r ALE2pwcexample5criticalpoints2, fig.cap='(ref:ALE2pwcexample5criticalpoints2)',   out.width='100%', echo=FALSE}

knitr::include_graphics("images/ALE_2_pwc_example5_critical_points2_.PNG")

```
(ref:ALE2pwcexample5criticalpoints2) Points that are used to estimate the step at grid size 20

For grid size 20, 5 data points are used to estimate the total change in prediction. As figure \@ref(fig:ALE2pwcexample5criticalpoints2)  shows, coincidencially 5 data points with $x_2$-values between $\frac{10}{3}$ and $\frac{20}{3}$ fall into the step interval. This is why the mean difference of the prediction is estimated to be 3. 


``` {r eval = FALSE, include = FALSE}
prediction_function_4_1 <- function(x_1, x_2){
  if(x_1 < 10/3){
    y = 2
  }else if(x_1 > 20/3){
    y = 1
  } else {
    y = 3
  }
  return(y)
}



sample_uniform_sq <- function(n, FUN = prediction_function_4_1, a = 3, b = 3){
  x_1 <- runif(n = n, min = 0, max = 10)
  x_2 <- runif(n = n, min = pmax(x_1 - a, 0), max = pmin(x_1 + b, 10))
  y <- numeric(0)
  for (i in 1:n){
    y[i] <- FUN(x_1[i], x_2[i])
  }
  return(as.data.frame(cbind(x_1, x_2, y)))
}
set.seed(123)
dataset_3 <- sample_uniform_sq(100, prediction_function_4_1, a = 3, b = 3)



p <- ggplot2::ggplot(dataset_3, ggplot2::aes(x_1, x_2))



p + ggplot2::geom_point() +
  ggplot2::labs(title = "", subtitle = "")+
  geom_segment(aes(x = 10/3, y = 0, xend = 10/3, yend = 10))+
  geom_segment(aes(x = 20/3, y = 0, xend = 20/3, yend = 10))+
  geom_segment(aes(y = 10/3, x = 0, yend = 10/3, xend = 10))+
  geom_segment(aes(y = 20/3, x = 0, yend = 20/3, xend = 10))+
  annotate("text", x = 5, y = 5, label = "f = 3", color = "red") +
  annotate("text", x = 5 - 10/3, y = 5, label = "f = 0", color = "red") +
  annotate("text", x = 5 + 10/3, y = 5, label = "f = 0", color = "red") +
  annotate("text", x = 5, y = 5-10/3, label = "f = 0", color = "red") +
  annotate("text", x = 5 - 10/3, y = 5-10/3, label = "f = 2", color = "red") +
  annotate("text", x = 5 + 10/3, y = 5-10/3, label = "f = 1", color = "red")  +
  annotate("text", x = 5, y = 5+10/3, label = "f = 0", color = "red") +
  annotate("text", x = 5 - 10/3, y = 5+10/3, label = "f = 2", color = "red") +
  annotate("text", x = 5 + 10/3, y = 5+10/3, label = "f = 1", color = "red") +
  geom_point(size = 5,data =dataset_3[order(dataset_3$x_1)[67],] , pch = 1, color = "red")+
  geom_point(size = 5,data =dataset_3[order(dataset_3$x_1)[68],] , pch = 1, color = "red")

```


```{r ALE2pwcexample5criticalpoints, fig.cap='(ref:ALE2pwcexample5criticalpoints)',  out.width='100%', echo=FALSE}

knitr::include_graphics("images/ALE_2_pwc_example5_critical_points_.PNG")
```
(ref:ALE2pwcexample5criticalpoints) Points that are used to estimate the step at grid size 50

Analogiuously for grid size 50 only two datapoints are used. As again both fall into the same $x_2$ - range, the estimation of the mean change of prediction in this grid interval is -3 now. Looking at figure \@ref(fig:ALE2pwcexample5criticalpoints) it becomes clear that only a little higher $x_2$ value of the upper data point would have lead to an estimation of -1 instead. This shows how sensitve the ALE estimation in the context of piece-wise constant models is. While there could be arguments for the heigth of the steps in the first 3 estimations, the last estimation clearly displays a false image. Here the interpretation would be that there is no main effect of feature 1 changing from less than $\frac{10}{3}$ to higher than $\frac{20}{3}$, which is obviously wrong. 

### Outlook 

We have seen that ALE estimations in the context of piece-wise constant models are even more critical due to sharp changes of the prediction at the steps. On the one hand one needs the intervals to be within the steps to recognize them and at the same time quite narrow arround them to catch the steepness of the step. Notice that in real world examples one cannot know if there is a step or if the flat linear aprroximation is true. On the other hand the data distribution around the steps has a strong influence on the ALE which leads to highly instable estimations. In this context different methods of interval selection, maybe even adaptive, data driven methods should be investigated.   




## Categorical Features

So far we were only interested in ALE-estimations for a numerical feature of interest. In real data situations categorical features often play an important role. That's why it would be nice to expand the concept of ALE so that it can also be applied to categorical features. In the original paper by [@Apley2016] this concept was not described but still a first method implemented. [@molnar2019] adapted the method for the iml-package. The following section brievly describes the implemented method as well as the interpretation of ALE-plots for categorical features. It also shows some specific problems. 



### Ordering the features

One of the biggest and crucial differences of categorical and numerical features in the context of ALE is that categorical features usually don't have a natural order. As the concept of ALE is based on accumulating the local effects in a certain direction, an order of the feature is indispensable. Sometimes the categorical feature is an ordinal feature which comes with an natural order. In this case the natural order should be used. If there is no natural order, the first essential step to calculate the ALE is to order the feature. Therefore different methods are conceivable. The iml-package implementation tries to order the feature with respect to the similarity of the other features. As we'll see in the next subsection for the estimation of the ALE the datapoints of a category will be shifted to the neighbour categories (neighbour categories only exist if the feature is ordered). To stay with the original idea of ALE and try to avoid extrapolation, ordering the feature with respect to the similarity of the other features seems reasonable. Within the iml-package in a first step the distance of each pair of categories (of the feature of interest) is calculated with respect to every other feature. This results in $\frac{c~(c-1)~(f-1)}{2}$ distances, where f is the total number of features while c is the number of categories of the feature of interest. To calculate these distances for numerical features the Kolmogorov-Smirnov distance is used. It is defined as the maximal absolut difference of two distribution functions, which are estimated from the data within the two compared categories. For categorical features one simply sums up the absolute differences of the relative frequencys of the categories.
Finally the distance between two categories is calculated as the sum of their distances with respect to all features. 
Once the distance between all categories is calculated, multidimensional scaling is used to reduce the distance matrix to a one dimensional distance messure [@molnar2019].


### Estimation of the ALE

Once the features are ordered (no matter if as proposed by [@Apley2016] and [@molnar2019] or in a different order) it's still not clear how to estimate the ALE. The partioning of the axis into intervals doesn't make sense anymore as the categories themselves kind of partion the range of the feature in a natural manner. But there are no "values" in between of them and at the same time the datapoints fall exactly on them. A continious ALE wouldn't make sense at all, as there are no possibilities of changing the feature value if not from one category to another category. Thats why the idea is to estimate exactly these expected changes in prediction if one category is changed to its neighbour category. Therefore for each pair of neighbour categories the expected change is estimated by shifting the data from the lower to the upper category and vice verse and calculating the mean difference of the prediction. This mean difference is taken to be the expected effect between these two categories. How these changes are accumulated and how the ALE-plot looks, becomes clearer once looking at an example.    

### Example of ALE with categorical feature  

For the following example the munich rent dataset, which consists of a sample of 2053 appartments from the data collected for the preparation of the Munich rent index 2003, was used. For our purposes we restricted the data to the variables rentm (Net rent per square meter in EUR (numeric)), size (Floor area in square meters (numeric)), rooms (Number of rooms (numeric)), year (Year of construction (numeric)) and area (Urban district where the apartment is located (Factor with 25 levels)). In a first step a model (Support Vector Machine for Regression) was fitted to predict the variable rentm. Now the ALE for the feature area, which is a categorical variable, was estimated with the iml-package. Figure \@ref(fig:ALE2catfullmod) shows the result.   


``` {r eval = FALSE, include = FALSE}
data(package = "catdata",rent)


library(crs)
library(iml)
library(mlr)
library(ggplot2)
data <- rent[,c(2,3,4,5,6)]
#data <-data[,c(1,4,5)]



data$area <- factor(data$area, order = FALSE)
example_cat_task <- makeRegrTask(id = "example_cat", data = data,
                                 target = "rentm")
example_cat.ksvm <- makeLearner("regr.ksvm")
#example_cat.kknn <- makeLearner("regr.kknn")

model_2 <- train(example_cat.ksvm, example_cat_task)
#data$area <- factor(data$area, order = TRUE)
model_2 = Predictor$new(model_2, data = data)

ale = FeatureEffect$new(model_2, feature = "area")
a <- ale$plot() +
  labs(title = "Model with features: size, rooms, year, area ")

a


```

```{r ALE2catfullmod,  fig.cap='(ref:ALE2catfullmod)', out.width='100%', echo=FALSE}

knitr::include_graphics("images/ALE_2_example_cat_.PNG")

```
(ref:ALE2catfullmod) ALE for the variable area (categorical) 

As described previously in a first step the categories (each number stands for one district) were ordered on basis of their similarity  w.r.t. the other features. Notice that the first bar at category one only reflects the centering. The uncentered ALE wouldn't show an effect for this category as it is kind of the starting point.
Now the mean difference of prediction between category 1 and 8 was calculated. Therefore datapoints from category 1 were shifted to category 8, letting the rest of the features untouched and vice versa. The total effect was estimated as the mean difference in prediction for these datapoints. It is shown as the delta of the category 1 bar and the category 8 bar. Without centering it would be seen at the category 8 bar. Now the same difference is estimated for the change from category 8 to category 2 and is shown as the delta of their bars.
This procedure continoues till finally the change from category 23 to category 22 results in the last delta between their bars.


### Interpretation

The interpretation of the ALE-plot for categorical features is unfortunatley quite difficult. The deltas between two adjacent bars surely can be interpreted as the change between the corresponding categories. Once looking on deltas of two categories with one or more other categories in between this changes. The delta is not anylonger the change of prediction between the two categories but the estimated change of prediction for shifting throug all the categories in between in exactly the given order. The reason is that the estimated delta is not path independent. For example the delta between categories 1 and 2 in the example above was calculated using datapoints of category 1, 8, and 2. If they were direct neigbhours, the datapoints of category 8 wouldn't be involved in the estimation at all. This problem clearly grows the further two categories are ordered. Having this in mind the absolut values of the bars shouldn't be interpreted at all. Furthermore this is another argument for ordering the the categories in a reasonable manner, while it stays arguable what "reasonable" in this context means. 

### Changes of the ALE due to diffrent orders

The last two graffics show how much the ALE for categorical features depends on the underlying order of the features. For the first one the same learner was fitted on a restricted feature space containing only the variables year and area.  

``` {r eval = FALSE, include = FALSE}
data(package = "catdata",rent)


library(crs)
library(iml)
library(mlr)
library(ggplot2)
data <- rent[,c(2,3,4,5,6)]
#data <-data[,c(1,4,5)]



data$area <- factor(data$area, order = FALSE)
example_cat_task <- makeRegrTask(id = "example_cat", data = data,
                                 target = "rentm")
example_cat.ksvm <- makeLearner("regr.ksvm")
#example_cat.kknn <- makeLearner("regr.kknn")

model_2 <- train(example_cat.ksvm, example_cat_task)
#data$area <- factor(data$area, order = TRUE)
model_2 = Predictor$new(model_2, data = data)

ale = FeatureEffect$new(model_2, feature = "area")
a <- ale$plot() +
  labs(title = "Model with features: size, rooms, year, area ")

a

data <- rent[,c(2,5,6)]
data$area <- factor(data$area, order = FALSE)
example_cat_task <- makeRegrTask(id = "example_cat", data = data,
                                 target = "rentm")
example_cat.ksvm <- makeLearner("regr.ksvm")
#example_cat.kknn <- makeLearner("regr.kknn")

model_2 <- train(example_cat.ksvm, example_cat_task)
#data$area <- factor(data$area, order = TRUE)
model_2 = Predictor$new(model_2, data = data)

ale = FeatureEffect$new(model_2, feature = "area")
b <- ale$plot()+
  labs(title = "Model with features: year, area ")



library(ggpubr)
ggarrange(a,b, ncol=1, nrow=2, common.legend = TRUE, legend="bottom")


```


```{r ALE2catrestrmod, fig.cap='(ref:ALE2catrestrmod)', out.width='100%', echo=FALSE}

knitr::include_graphics("images/ALE_2_cat_different_features_.PNG")

```
(ref:ALE2catrestrmod) ALE-plot of the full model vs ALE-plot of the restricted model

As the comparison of the ALE-plots of figure \@ref(fig:ALE2catrestrmod) shows, the similarity based order changes as it is only calculated on basis of the variable year (instead of size, room, year). As the underlying model is now a different one, changes of the ALE are not suprising. Still the comparison is quite difficult due to the new order. 

The second ALE-plot of figure \@ref(fig:ALE2catdifford) is again based on the full model. This time the area was taken as ordered factor, such that the similarity based order wasn't calculated. The resulting ALE takes the district enumeration as order and proceeds accordingly.  

``` {r eval = FALSE, include = FALSE}
data(package = "catdata",rent)


library(crs)
library(iml)
library(mlr)
library(ggplot2)
data <- rent[,c(2,3,4,5,6)]

data$area <- factor(data$area, order = FALSE)
example_cat_task <- makeRegrTask(id = "example_cat", data = data,
                                 target = "rentm")
example_cat.ksvm <- makeLearner("regr.ksvm")
#example_cat.kknn <- makeLearner("regr.kknn")

model_2 <- train(example_cat.ksvm, example_cat_task)
#data$area <- factor(data$area, order = TRUE)
model_2 = Predictor$new(model_2, data = data)

ale = FeatureEffect$new(model_2, feature = "area")
a <- ale$plot() +
  labs(title = "Model with features: size, rooms, year, area; ordered by similarity ")

data$area <- factor(data$area, order = FALSE)
example_cat_task <- makeRegrTask(id = "example_cat", data = data,
                                 target = "rentm")
example_cat.ksvm <- makeLearner("regr.ksvm")
#example_cat.kknn <- makeLearner("regr.kknn")

model_2 <- train(example_cat.ksvm, example_cat_task)
data$area <- factor(data$area, order = TRUE)
model_2 = Predictor$new(model_2, data = data)

ale2 = FeatureEffect$new(model_2, feature = "area")
b <- ale2$plot() +
  labs(title = "Model with features: size, rooms, year, area; ordered by district number")
ggarrange(a,b, ncol=1, nrow=2, common.legend = TRUE, legend="bottom")
```



```{r ALE2catdifford, fig.cap='(ref:ALE2catdifford)', out.width='100%', echo=FALSE}

knitr::include_graphics("images/ALE_2_cat_different_orders_.PNG")

``` 
(ref:ALE2catdifford) Two ALE-plots for different orderes of the category area

 
Although the underlaying model is exactly the same, the ALE changes completely. Not only the order of the features changed but also the delta between some not adjacent categories. For example we see a decrease from category 1 to 12 instead of an increase, as in the ALE-plot with similarity based order. This underlines how carefull one should be when interpreting ALE-plots for categorical features.
 
### Conclusion

We have seen how sensitve the ALE (for categorical features) is to different orders of the category. Due to the lack of theoretical foundations concerning the implemented order method, further investigations are highly recomended. The interpretation of the ALE should be done quite carefully.  

