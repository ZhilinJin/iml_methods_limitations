# ALE Intervals, Piece-Wise Constant Models and Categorical Features

*Author: Nikolas Fritz*


As mentioned in the former section the choice of intervals an starting value $z_{0,j}$ have both a certain influence on the estimated ALE - curve. While the main influence of $z_{0,j}$ is canceld out by centering the ALE the choice of intervals stays crucial. Therefore the next section is dedicated to this topic. 
## How to choose the number and/or length of the intervals

Before investigating the choice of intervals one should be clear about in how far they influence the estimation. On the one hand for a given interval the ALE estimation will be linear due to the expected constant effect within this interval.
Remember that within each interval the local effect within this interval was calculated by the mean total difference of the prediction when shifting the variable of interest from the lower interval boundary to the upper one. This leads by definition to a constant effect within this interval that results in a linear function when integrating over the interval.   
It seems obvious that the ALE estimation (within a grid interval) can only be as good as a linear approximation for the "real" and usually unknown prediction function (TODO: check that!) can be. Therefore it is crucial for a good estimation to have small enough intervals especially in regions where the prediction function is shaky or far from linear (i.e high second derivatives) with respect to the feature of interest.
On the other hand to get stable estimations for a grid interval its important to have a sufficient high number of datapoints within the interval. This means that the intervals shouldn't become too small so that they would contain only a few data points. (Note that this is only true if the other features have an influence on the local effect of the prediction function. If they dont, any data point within the grid interval would lead to the same predictions at the interval boundarys)
Thats why there is a natural trade of between a small interval width and the number of the contained data points. 

  
### State of the art
So the question is how to optimally choose the grid intervals. Should they all be of the same width containing a different number of datapoints? Should they all contain the same or at least a similar number of datapoints, accepting different interval sizes. Or could there even be a better solution in between the two concepts?
Within the iml-package which is one of two implementations of ALE-plots the choosen method is the second one. The quantiles of the distribution of the feature are used as the grid that defines the intervals. That means the length of the intervals depends on the choosen grid size and the given feature distribution.

In the following section some examples with artificial data sets are provided, that should help to get a better feeling for the different deterministic factors that influnce the goodness of the ALE estimation. Within the whole chapter the ALE estimation is conducted via the iml-package implementation.

### ALE Approximations 

In the following section, we only consider two dimensional data sets, of continious features $x_1$ and $x_2$ with a certain correlation. Furthermore we use some exemplary prediction functions which are differentiable such that we can calculate the theoratical ALE (see section XX formula XX) and use it to evaluate the goodness of the estimated ALE-curve. As we want to isolate some of the above mentioned influencial factors, we start with some easy examples adding step by step more complexity to the problem. 

### Example 1

In the first example we assume a uniform distribution for the feature $x_1$ on the interval $[0, 10]$, i.e. $X_1 \sim U(0,10) $. Furthermore we assume the conditional distribution of the feature $x_2$ given $x_1$ to be also uniform on the interval $[x_1 - 3, x_1 + 3 ]$, i.e. $X_2 \vert X_1 = x_1 \sim U(x_1 - 3, x_1 + 3 ) $. Sampling 100 data points from this distribution yields the first dataset.

```{r eval = FALSE}
prediction_function_1 <- function(x_1, x_2){
  y <-(x_1-4)*(x_1-5)*(x_1-6) + x_2^3
  return(y)
}


sample_uniform <- function(n, FUN = prediction_function_1, a = -5, b = 5, c = 2, d = 2){
  x_1 <- runif(n = n, min = a, max = b)
  x_2 <- runif(n = n, min = x_1 - c, max = x_1 + d)
  y <- FUN(x_1, x_2)
  return(as.data.frame(cbind(x_1, x_2, y)))
}

set.seed(123)
dataset_1 <- sample_uniform(100, prediction_function_1, a = 0, b = 10, c = 3, d = 3)
p <- ggplot2::ggplot(dataset_1, ggplot2::aes(x_1, x_2))
p + ggplot2::geom_point() +
  ggplot2::labs(title = "Dataset 1", subtitle = "Uniform dist.")

```

```{r DatasetALE1,  out.width='100%', echo=FALSE}

knitr::include_graphics("images/ALE_2_Dataset1.PNG")
```
(ref:DatasetALE1)The correlation is clearly recognizable.
