<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Introduction | Limitations of Interpretable Machine Learning Methods</title>
  <meta name="description" content="Situations in which PDP, ALE, LIME, LOCO and feature importance fail." />
  <meta name="generator" content="bookdown 0.14 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Introduction | Limitations of Interpretable Machine Learning Methods" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Situations in which PDP, ALE, LIME, LOCO and feature importance fail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Introduction | Limitations of Interpretable Machine Learning Methods" />
  
  <meta name="twitter:description" content="Situations in which PDP, ALE, LIME, LOCO and feature importance fail." />
  

<meta name="author" content="" />


<meta name="date" content="2019-10-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Limitations of ML Interpretability</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#statistical-modeling-the-two-approaches"><i class="fa fa-check"></i><b>1.1</b> Statistical Modeling: The Two Approaches</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#importance-of-interpretability"><i class="fa fa-check"></i><b>1.2</b> Importance of Interpretability</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#interpretable-machine-learning"><i class="fa fa-check"></i><b>1.3</b> Interpretable Machine Learning</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#outline-of-the-booklet"><i class="fa fa-check"></i><b>1.4</b> Outline of the booklet</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html"><a href="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html"><i class="fa fa-check"></i><b>2</b> Introduction to Partial Dependence Plots (PDP) and Individual Conditional Expectation (ICE)</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html"><a href="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html#partial-dependence-plots-pdp"><i class="fa fa-check"></i><b>2.1</b> Partial Dependence Plots (PDP)</a><ul>
<li class="chapter" data-level="2.1.1" data-path="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html"><a href="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html#advantages-and-limitations-of-partial-dependence-plots"><i class="fa fa-check"></i><b>2.1.1</b> Advantages and Limitations of Partial Dependence Plots</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html"><a href="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html#individual-conditional-expectation-curves"><i class="fa fa-check"></i><b>2.2</b> Individual Conditional Expectation Curves</a><ul>
<li class="chapter" data-level="2.2.1" data-path="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html"><a href="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html#centered-ice-plot"><i class="fa fa-check"></i><b>2.2.1</b> Centered ICE Plot</a></li>
<li class="chapter" data-level="2.2.2" data-path="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html"><a href="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html#derivative-ice-plot"><i class="fa fa-check"></i><b>2.2.2</b> Derivative ICE Plot</a></li>
<li class="chapter" data-level="2.2.3" data-path="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html"><a href="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html#advantages-and-limitations-of-ice-plots"><i class="fa fa-check"></i><b>2.2.3</b> Advantages and Limitations of ICE Plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="pdp-and-correlated-features.html"><a href="pdp-and-correlated-features.html"><i class="fa fa-check"></i><b>3</b> PDP and Correlated Features</a><ul>
<li class="chapter" data-level="3.1" data-path="pdp-and-correlated-features.html"><a href="pdp-and-correlated-features.html#ProblemDescription"><i class="fa fa-check"></i><b>3.1</b> Problem Description</a><ul>
<li class="chapter" data-level="3.1.1" data-path="pdp-and-correlated-features.html"><a href="pdp-and-correlated-features.html#what-is-the-issue-with-dependent-features"><i class="fa fa-check"></i><b>3.1.1</b> What is the issue with dependent features?</a></li>
<li class="chapter" data-level="3.1.2" data-path="pdp-and-correlated-features.html"><a href="pdp-and-correlated-features.html#what-is-the-issue-with-extrapolation"><i class="fa fa-check"></i><b>3.1.2</b> What is the issue with extrapolation?</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="pdp-and-correlated-features.html"><a href="pdp-and-correlated-features.html#RealData"><i class="fa fa-check"></i><b>3.2</b> Dependent Features: Bike Sharing Dataset</a><ul>
<li class="chapter" data-level="3.2.1" data-path="pdp-and-correlated-features.html"><a href="pdp-and-correlated-features.html#dependency-between-numerical-features"><i class="fa fa-check"></i><b>3.2.1</b> Dependency between Numerical Features</a></li>
<li class="chapter" data-level="3.2.2" data-path="pdp-and-correlated-features.html"><a href="pdp-and-correlated-features.html#dependency-between-categorical-features"><i class="fa fa-check"></i><b>3.2.2</b> Dependency between Categorical Features</a></li>
<li class="chapter" data-level="3.2.3" data-path="pdp-and-correlated-features.html"><a href="pdp-and-correlated-features.html#NumCat"><i class="fa fa-check"></i><b>3.2.3</b> Dependency between Numerical and Categorical Features</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="pdp-and-correlated-features.html"><a href="pdp-and-correlated-features.html#SimulatedData"><i class="fa fa-check"></i><b>3.3</b> Dependent Features: Simulated Data</a><ul>
<li class="chapter" data-level="3.3.1" data-path="pdp-and-correlated-features.html"><a href="pdp-and-correlated-features.html#simulation-settings-numerical-features"><i class="fa fa-check"></i><b>3.3.1</b> Simulation Settings: Numerical Features</a></li>
<li class="chapter" data-level="3.3.2" data-path="pdp-and-correlated-features.html"><a href="pdp-and-correlated-features.html#simulation-of-setting-1-linear-dependence"><i class="fa fa-check"></i><b>3.3.2</b> Simulation of Setting 1: Linear Dependence</a></li>
<li class="chapter" data-level="3.3.3" data-path="pdp-and-correlated-features.html"><a href="pdp-and-correlated-features.html#simulation-of-setting-2-nonlinear-dependence"><i class="fa fa-check"></i><b>3.3.3</b> Simulation of Setting 2: Nonlinear Dependence</a></li>
<li class="chapter" data-level="3.3.4" data-path="pdp-and-correlated-features.html"><a href="pdp-and-correlated-features.html#simulation-of-setting-3-missing-informative-feature-x_3"><i class="fa fa-check"></i><b>3.3.4</b> Simulation of Setting 3: Missing informative feature <span class="math inline">\(x_3\)</span></a></li>
<li class="chapter" data-level="3.3.5" data-path="pdp-and-correlated-features.html"><a href="pdp-and-correlated-features.html#simulation-settings-categorical-features"><i class="fa fa-check"></i><b>3.3.5</b> Simulation Settings: Categorical Features</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="pdp-and-correlated-features.html"><a href="pdp-and-correlated-features.html#ExtrapolationProblem"><i class="fa fa-check"></i><b>3.4</b> Extrapolation Problem: Simulation</a><ul>
<li class="chapter" data-level="3.4.1" data-path="pdp-and-correlated-features.html"><a href="pdp-and-correlated-features.html#ExtrapolationProblemEstablished"><i class="fa fa-check"></i><b>3.4.1</b> Simulation based on established learners</a></li>
<li class="chapter" data-level="3.4.2" data-path="pdp-and-correlated-features.html"><a href="pdp-and-correlated-features.html#ExtrapolationProblemPrediction"><i class="fa fa-check"></i><b>3.4.2</b> Simulation based on own prediction function</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="pdp-and-correlated-features.html"><a href="pdp-and-correlated-features.html#summary"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="pdp-and-causal-interpretation.html"><a href="pdp-and-causal-interpretation.html"><i class="fa fa-check"></i><b>4</b> PDP and Causal Interpretation</a><ul>
<li class="chapter" data-level="4.1" data-path="pdp-and-causal-interpretation.html"><a href="pdp-and-causal-interpretation.html#introduction-1"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="pdp-and-causal-interpretation.html"><a href="pdp-and-causal-interpretation.html#a-brief-look-at-pdp-problems"><i class="fa fa-check"></i><b>4.2</b> A brief look at PDP problems</a></li>
<li class="chapter" data-level="4.3" data-path="pdp-and-causal-interpretation.html"><a href="pdp-and-causal-interpretation.html#causal-interpretability-interventions-and-directed-acyclical-graphs"><i class="fa fa-check"></i><b>4.3</b> Causal Interpretability: Interventions and Directed Acyclical Graphs</a></li>
<li class="chapter" data-level="4.4" data-path="pdp-and-causal-interpretation.html"><a href="pdp-and-causal-interpretation.html#scenarios"><i class="fa fa-check"></i><b>4.4</b> Scenarios</a></li>
<li class="chapter" data-level="4.5" data-path="pdp-and-causal-interpretation.html"><a href="pdp-and-causal-interpretation.html#theoretical-comparison"><i class="fa fa-check"></i><b>4.5</b> Theoretical Comparison</a></li>
<li class="chapter" data-level="4.6" data-path="pdp-and-causal-interpretation.html"><a href="pdp-and-causal-interpretation.html#conclusion"><i class="fa fa-check"></i><b>4.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="introduction-to-accumulated-local-effects-ale.html"><a href="introduction-to-accumulated-local-effects-ale.html"><i class="fa fa-check"></i><b>5</b> Introduction to Accumulated Local Effects (ALE)</a><ul>
<li class="chapter" data-level="5.1" data-path="introduction-to-accumulated-local-effects-ale.html"><a href="introduction-to-accumulated-local-effects-ale.html#motivation"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="introduction-to-accumulated-local-effects-ale.html"><a href="introduction-to-accumulated-local-effects-ale.html#ale-intro-formula"><i class="fa fa-check"></i><b>5.2</b> The Theoretical Formula</a><ul>
<li class="chapter" data-level="5.2.1" data-path="introduction-to-accumulated-local-effects-ale.html"><a href="introduction-to-accumulated-local-effects-ale.html#centering"><i class="fa fa-check"></i><b>5.2.1</b> Centering</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="introduction-to-accumulated-local-effects-ale.html"><a href="introduction-to-accumulated-local-effects-ale.html#estimation-formula"><i class="fa fa-check"></i><b>5.3</b> Estimation Formula</a><ul>
<li class="chapter" data-level="5.3.1" data-path="introduction-to-accumulated-local-effects-ale.html"><a href="introduction-to-accumulated-local-effects-ale.html#implementation-formula"><i class="fa fa-check"></i><b>5.3.1</b> Implementation Formula</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="introduction-to-accumulated-local-effects-ale.html"><a href="introduction-to-accumulated-local-effects-ale.html#ale-intro-interpret"><i class="fa fa-check"></i><b>5.4</b> Intuition and Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="comparison-of-ale-and-pdp.html"><a href="comparison-of-ale-and-pdp.html"><i class="fa fa-check"></i><b>6</b> Comparison of ALE and PDP</a><ul>
<li class="chapter" data-level="6.1" data-path="comparison-of-ale-and-pdp.html"><a href="comparison-of-ale-and-pdp.html#comparison-one-feature"><i class="fa fa-check"></i><b>6.1</b> Comparison one feature</a><ul>
<li class="chapter" data-level="6.1.1" data-path="comparison-of-ale-and-pdp.html"><a href="comparison-of-ale-and-pdp.html#example-1-multiplicative-prediction-function"><i class="fa fa-check"></i><b>6.1.1</b> Example 1: Multiplicative prediction function</a></li>
<li class="chapter" data-level="6.1.2" data-path="comparison-of-ale-and-pdp.html"><a href="comparison-of-ale-and-pdp.html#example-2-additive-prediction-function"><i class="fa fa-check"></i><b>6.1.2</b> Example 2: Additive prediction function</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="comparison-of-ale-and-pdp.html"><a href="comparison-of-ale-and-pdp.html#comparison-two-features"><i class="fa fa-check"></i><b>6.2</b> Comparison two features</a><ul>
<li class="chapter" data-level="6.2.1" data-path="comparison-of-ale-and-pdp.html"><a href="comparison-of-ale-and-pdp.html#the-2d-ale"><i class="fa fa-check"></i><b>6.2.1</b> The 2D ALE</a></li>
<li class="chapter" data-level="6.2.2" data-path="comparison-of-ale-and-pdp.html"><a href="comparison-of-ale-and-pdp.html#d-ale-vs-2d-pdp"><i class="fa fa-check"></i><b>6.2.2</b> 2D ALE vs 2D PDP</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="comparison-of-ale-and-pdp.html"><a href="comparison-of-ale-and-pdp.html#runtime-comparison"><i class="fa fa-check"></i><b>6.3</b> Runtime comparison</a><ul>
<li class="chapter" data-level="6.3.1" data-path="comparison-of-ale-and-pdp.html"><a href="comparison-of-ale-and-pdp.html#one-numerical-feature-of-interest"><i class="fa fa-check"></i><b>6.3.1</b> One numerical feature of interest</a></li>
<li class="chapter" data-level="6.3.2" data-path="comparison-of-ale-and-pdp.html"><a href="comparison-of-ale-and-pdp.html#two-numerical-features-of-interest"><i class="fa fa-check"></i><b>6.3.2</b> Two numerical features of interest</a></li>
<li class="chapter" data-level="6.3.3" data-path="comparison-of-ale-and-pdp.html"><a href="comparison-of-ale-and-pdp.html#one-categorial-feature-of-interest"><i class="fa fa-check"></i><b>6.3.3</b> One categorial feature of interest</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="comparison-of-ale-and-pdp.html"><a href="comparison-of-ale-and-pdp.html#comparison-for-unevenly-distributed-data---example-4-munich-rents"><i class="fa fa-check"></i><b>6.4</b> Comparison for unevenly distributed data - Example 4: Munich rents</a></li>
<li class="chapter" data-level="6.5" data-path="comparison-of-ale-and-pdp.html"><a href="comparison-of-ale-and-pdp.html#ale-2d-example-calculation"><i class="fa fa-check"></i><b>6.5</b> Calculation of theoretical 2D ALE example</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ale-intervals-piece-wise-constant-models-and-categorical-features.html"><a href="ale-intervals-piece-wise-constant-models-and-categorical-features.html"><i class="fa fa-check"></i><b>7</b> ALE Intervals, Piece-Wise Constant Models and Categorical Features</a><ul>
<li class="chapter" data-level="7.1" data-path="ale-intervals-piece-wise-constant-models-and-categorical-features.html"><a href="ale-intervals-piece-wise-constant-models-and-categorical-features.html#how-to-choose-the-number-andor-length-of-the-intervals"><i class="fa fa-check"></i><b>7.1</b> How to choose the number and/or length of the intervals</a><ul>
<li class="chapter" data-level="7.1.1" data-path="ale-intervals-piece-wise-constant-models-and-categorical-features.html"><a href="ale-intervals-piece-wise-constant-models-and-categorical-features.html#state-of-the-art"><i class="fa fa-check"></i><b>7.1.1</b> State of the art</a></li>
<li class="chapter" data-level="7.1.2" data-path="ale-intervals-piece-wise-constant-models-and-categorical-features.html"><a href="ale-intervals-piece-wise-constant-models-and-categorical-features.html#ale-approximations"><i class="fa fa-check"></i><b>7.1.2</b> ALE Approximations</a></li>
<li class="chapter" data-level="7.1.3" data-path="ale-intervals-piece-wise-constant-models-and-categorical-features.html"><a href="ale-intervals-piece-wise-constant-models-and-categorical-features.html#example-1-additive-feature-effects"><i class="fa fa-check"></i><b>7.1.3</b> Example 1: additive feature effects</a></li>
<li class="chapter" data-level="7.1.4" data-path="ale-intervals-piece-wise-constant-models-and-categorical-features.html"><a href="ale-intervals-piece-wise-constant-models-and-categorical-features.html#example-2-multiplicative-feature-effects"><i class="fa fa-check"></i><b>7.1.4</b> Example 2: multiplicative feature effects</a></li>
<li class="chapter" data-level="7.1.5" data-path="ale-intervals-piece-wise-constant-models-and-categorical-features.html"><a href="ale-intervals-piece-wise-constant-models-and-categorical-features.html#example-3-unbalanced-datasets-and-shaky-prediction-functions"><i class="fa fa-check"></i><b>7.1.5</b> Example 3: Unbalanced datasets and shaky prediction functions</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="ale-intervals-piece-wise-constant-models-and-categorical-features.html"><a href="ale-intervals-piece-wise-constant-models-and-categorical-features.html#problems-with-piece-wise-constant-models"><i class="fa fa-check"></i><b>7.2</b> Problems with piece-wise constant models</a><ul>
<li class="chapter" data-level="7.2.1" data-path="ale-intervals-piece-wise-constant-models-and-categorical-features.html"><a href="ale-intervals-piece-wise-constant-models-and-categorical-features.html#outlook"><i class="fa fa-check"></i><b>7.2.1</b> Outlook</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ale-intervals-piece-wise-constant-models-and-categorical-features.html"><a href="ale-intervals-piece-wise-constant-models-and-categorical-features.html#categorical-features"><i class="fa fa-check"></i><b>7.3</b> Categorical Features</a><ul>
<li class="chapter" data-level="7.3.1" data-path="ale-intervals-piece-wise-constant-models-and-categorical-features.html"><a href="ale-intervals-piece-wise-constant-models-and-categorical-features.html#ordering-the-features"><i class="fa fa-check"></i><b>7.3.1</b> Ordering the features</a></li>
<li class="chapter" data-level="7.3.2" data-path="ale-intervals-piece-wise-constant-models-and-categorical-features.html"><a href="ale-intervals-piece-wise-constant-models-and-categorical-features.html#estimation-of-the-ale"><i class="fa fa-check"></i><b>7.3.2</b> Estimation of the ALE</a></li>
<li class="chapter" data-level="7.3.3" data-path="ale-intervals-piece-wise-constant-models-and-categorical-features.html"><a href="ale-intervals-piece-wise-constant-models-and-categorical-features.html#example-of-ale-with-categorical-feature"><i class="fa fa-check"></i><b>7.3.3</b> Example of ALE with categorical feature</a></li>
<li class="chapter" data-level="7.3.4" data-path="ale-intervals-piece-wise-constant-models-and-categorical-features.html"><a href="ale-intervals-piece-wise-constant-models-and-categorical-features.html#interpretation"><i class="fa fa-check"></i><b>7.3.4</b> Interpretation</a></li>
<li class="chapter" data-level="7.3.5" data-path="ale-intervals-piece-wise-constant-models-and-categorical-features.html"><a href="ale-intervals-piece-wise-constant-models-and-categorical-features.html#changes-of-the-ale-due-to-diffrent-orders"><i class="fa fa-check"></i><b>7.3.5</b> Changes of the ALE due to diffrent orders</a></li>
<li class="chapter" data-level="7.3.6" data-path="ale-intervals-piece-wise-constant-models-and-categorical-features.html"><a href="ale-intervals-piece-wise-constant-models-and-categorical-features.html#conclusion-1"><i class="fa fa-check"></i><b>7.3.6</b> Conclusion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="introduction-to-feature-importance.html"><a href="introduction-to-feature-importance.html"><i class="fa fa-check"></i><b>8</b> Introduction to Feature Importance</a><ul>
<li class="chapter" data-level="8.1" data-path="introduction-to-feature-importance.html"><a href="introduction-to-feature-importance.html#permutation-feature-importance-pfi"><i class="fa fa-check"></i><b>8.1</b> Permutation Feature Importance (PFI)</a></li>
<li class="chapter" data-level="8.2" data-path="introduction-to-feature-importance.html"><a href="introduction-to-feature-importance.html#leave-one-covariate-out-loco"><i class="fa fa-check"></i><b>8.2</b> Leave-One-Covariate-Out (LOCO)</a></li>
<li class="chapter" data-level="8.3" data-path="introduction-to-feature-importance.html"><a href="introduction-to-feature-importance.html#interpretability-of-feature-importance-and-its-limitations"><i class="fa fa-check"></i><b>8.3</b> Interpretability of Feature Importance and its Limitations</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="pfi-loco-and-correlated-features.html"><a href="pfi-loco-and-correlated-features.html"><i class="fa fa-check"></i><b>9</b> PFI, LOCO and Correlated Features</a><ul>
<li class="chapter" data-level="9.1" data-path="pfi-loco-and-correlated-features.html"><a href="pfi-loco-and-correlated-features.html#effect-on-feature-importance-by-adding-correlated-features"><i class="fa fa-check"></i><b>9.1</b> Effect on Feature Importance by Adding Correlated Features</a><ul>
<li class="chapter" data-level="9.1.1" data-path="pfi-loco-and-correlated-features.html"><a href="pfi-loco-and-correlated-features.html#simulation"><i class="fa fa-check"></i><b>9.1.1</b> Simulation</a></li>
<li class="chapter" data-level="9.1.2" data-path="pfi-loco-and-correlated-features.html"><a href="pfi-loco-and-correlated-features.html#real-data"><i class="fa fa-check"></i><b>9.1.2</b> Real Data</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="pfi-loco-and-correlated-features.html"><a href="pfi-loco-and-correlated-features.html#alternative-measures-dealing-with-correlated-features"><i class="fa fa-check"></i><b>9.2</b> Alternative Measures Dealing with Correlated Features</a></li>
<li class="chapter" data-level="9.3" data-path="pfi-loco-and-correlated-features.html"><a href="pfi-loco-and-correlated-features.html#summary-1"><i class="fa fa-check"></i><b>9.3</b> Summary</a></li>
<li class="chapter" data-level="9.4" data-path="pfi-loco-and-correlated-features.html"><a href="pfi-loco-and-correlated-features.html#note-to-the-reader"><i class="fa fa-check"></i><b>9.4</b> Note to the reader</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="partial-and-individual-permutation-feature-importance.html"><a href="partial-and-individual-permutation-feature-importance.html"><i class="fa fa-check"></i><b>10</b> Partial and Individual Permutation Feature Importance</a><ul>
<li class="chapter" data-level="10.1" data-path="partial-and-individual-permutation-feature-importance.html"><a href="partial-and-individual-permutation-feature-importance.html#ch2"><i class="fa fa-check"></i><b>10.1</b> Preliminaries on Partial and Individual Conditional Importance</a></li>
<li class="chapter" data-level="10.2" data-path="partial-and-individual-permutation-feature-importance.html"><a href="partial-and-individual-permutation-feature-importance.html#ch3"><i class="fa fa-check"></i><b>10.2</b> Simulations: A cookbook for using with PI and ICI</a><ul>
<li class="chapter" data-level="10.2.1" data-path="partial-and-individual-permutation-feature-importance.html"><a href="partial-and-individual-permutation-feature-importance.html#ch31"><i class="fa fa-check"></i><b>10.2.1</b> Detect Interactions</a></li>
<li class="chapter" data-level="10.2.2" data-path="partial-and-individual-permutation-feature-importance.html"><a href="partial-and-individual-permutation-feature-importance.html#ch32"><i class="fa fa-check"></i><b>10.2.2</b> Explain Interactions</a></li>
<li class="chapter" data-level="10.2.3" data-path="partial-and-individual-permutation-feature-importance.html"><a href="partial-and-individual-permutation-feature-importance.html#ch323"><i class="fa fa-check"></i><b>10.2.3</b> Stress Methods in a Non-Linear Relationship Setting</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="partial-and-individual-permutation-feature-importance.html"><a href="partial-and-individual-permutation-feature-importance.html#ch4"><i class="fa fa-check"></i><b>10.3</b> Real Data Application: Boston Housing</a></li>
<li class="chapter" data-level="10.4" data-path="partial-and-individual-permutation-feature-importance.html"><a href="partial-and-individual-permutation-feature-importance.html#ch5"><i class="fa fa-check"></i><b>10.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="pfi-training-vs-test-data.html"><a href="pfi-training-vs-test-data.html"><i class="fa fa-check"></i><b>11</b> PFI: Training vs. Test Data</a><ul>
<li class="chapter" data-level="11.1" data-path="pfi-training-vs-test-data.html"><a href="pfi-training-vs-test-data.html#introduction-to-test-vs-training-data"><i class="fa fa-check"></i><b>11.1</b> Introduction to Test vs Training Data</a></li>
<li class="chapter" data-level="11.2" data-path="pfi-training-vs-test-data.html"><a href="pfi-training-vs-test-data.html#theoretical-discussion-for-test-and-training-data"><i class="fa fa-check"></i><b>11.2</b> Theoretical Discussion for Test and Training Data</a></li>
<li class="chapter" data-level="11.3" data-path="pfi-training-vs-test-data.html"><a href="pfi-training-vs-test-data.html#reaction-to-model-behavior"><i class="fa fa-check"></i><b>11.3</b> Reaction to model behavior</a><ul>
<li class="chapter" data-level="11.3.1" data-path="pfi-training-vs-test-data.html"><a href="pfi-training-vs-test-data.html#gradient-boosting-machine"><i class="fa fa-check"></i><b>11.3.1</b> Gradient Boosting Machine</a></li>
<li class="chapter" data-level="11.3.2" data-path="pfi-training-vs-test-data.html"><a href="pfi-training-vs-test-data.html#data-sets-used-for-calculations"><i class="fa fa-check"></i><b>11.3.2</b> Data sets used for calculations</a></li>
<li class="chapter" data-level="11.3.3" data-path="pfi-training-vs-test-data.html"><a href="pfi-training-vs-test-data.html#results"><i class="fa fa-check"></i><b>11.3.3</b> Results</a></li>
<li class="chapter" data-level="11.3.4" data-path="pfi-training-vs-test-data.html"><a href="pfi-training-vs-test-data.html#interpretation-of-the-results"><i class="fa fa-check"></i><b>11.3.4</b> Interpretation of the results</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="pfi-training-vs-test-data.html"><a href="pfi-training-vs-test-data.html#summary-2"><i class="fa fa-check"></i><b>11.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="introduction-to-local-interpretable-model-agnostic-explanations-lime.html"><a href="introduction-to-local-interpretable-model-agnostic-explanations-lime.html"><i class="fa fa-check"></i><b>12</b> Introduction to Local Interpretable Model-Agnostic Explanations (LIME)</a><ul>
<li class="chapter" data-level="12.1" data-path="introduction-to-local-interpretable-model-agnostic-explanations-lime.html"><a href="introduction-to-local-interpretable-model-agnostic-explanations-lime.html#local-surrogate-models-and-lime"><i class="fa fa-check"></i><b>12.1</b> Local Surrogate Models and LIME</a></li>
<li class="chapter" data-level="12.2" data-path="introduction-to-local-interpretable-model-agnostic-explanations-lime.html"><a href="introduction-to-local-interpretable-model-agnostic-explanations-lime.html#how-lime-works-in-detail"><i class="fa fa-check"></i><b>12.2</b> How LIME works in detail</a><ul>
<li class="chapter" data-level="12.2.1" data-path="introduction-to-local-interpretable-model-agnostic-explanations-lime.html"><a href="introduction-to-local-interpretable-model-agnostic-explanations-lime.html#neighbourhood"><i class="fa fa-check"></i><b>12.2.1</b> Neighbourhood</a></li>
<li class="chapter" data-level="12.2.2" data-path="introduction-to-local-interpretable-model-agnostic-explanations-lime.html"><a href="introduction-to-local-interpretable-model-agnostic-explanations-lime.html#what-makes-a-good-explainer"><i class="fa fa-check"></i><b>12.2.2</b> What makes a good explainer?</a></li>
<li class="chapter" data-level="12.2.3" data-path="introduction-to-local-interpretable-model-agnostic-explanations-lime.html"><a href="introduction-to-local-interpretable-model-agnostic-explanations-lime.html#sampling-and-perturbation"><i class="fa fa-check"></i><b>12.2.3</b> Sampling and perturbation</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="introduction-to-local-interpretable-model-agnostic-explanations-lime.html"><a href="introduction-to-local-interpretable-model-agnostic-explanations-lime.html#example"><i class="fa fa-check"></i><b>12.3</b> Example</a></li>
<li class="chapter" data-level="12.4" data-path="introduction-to-local-interpretable-model-agnostic-explanations-lime.html"><a href="introduction-to-local-interpretable-model-agnostic-explanations-lime.html#outlook-1"><i class="fa fa-check"></i><b>12.4</b> Outlook</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="id1.html"><a href="id1.html"><i class="fa fa-check"></i><b>13</b> LIME and Neighbourhood</a><ul>
<li class="chapter" data-level="13.1" data-path="id1.html"><a href="id1.html#id2"><i class="fa fa-check"></i><b>13.1</b> The Neighbourhood in LIME in more detail</a></li>
<li class="chapter" data-level="13.2" data-path="id1.html"><a href="id1.html#id3"><i class="fa fa-check"></i><b>13.2</b> The problem in a one-dimensional setting</a></li>
<li class="chapter" data-level="13.3" data-path="id1.html"><a href="id1.html#id4"><i class="fa fa-check"></i><b>13.3</b> The problem in more complex settings</a><ul>
<li class="chapter" data-level="13.3.1" data-path="id1.html"><a href="id1.html#id41"><i class="fa fa-check"></i><b>13.3.1</b> Simulated data</a></li>
<li class="chapter" data-level="13.3.2" data-path="id1.html"><a href="id1.html#id42"><i class="fa fa-check"></i><b>13.3.2</b> Real data</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="id1.html"><a href="id1.html#id5"><i class="fa fa-check"></i><b>13.4</b> Discussion and outlook</a></li>
<li class="chapter" data-level="13.5" data-path="id1.html"><a href="id1.html#id6"><i class="fa fa-check"></i><b>13.5</b> Note to the reader</a><ul>
<li class="chapter" data-level="13.5.1" data-path="id1.html"><a href="id1.html#id61"><i class="fa fa-check"></i><b>13.5.1</b> Packages used</a></li>
<li class="chapter" data-level="13.5.2" data-path="id1.html"><a href="id1.html#id62"><i class="fa fa-check"></i><b>13.5.2</b> How we used the lime R package and why</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="lime-and-sampling.html"><a href="lime-and-sampling.html"><i class="fa fa-check"></i><b>14</b> LIME and Sampling</a><ul>
<li class="chapter" data-level="14.1" data-path="lime-and-sampling.html"><a href="lime-and-sampling.html#understanding-sampling-in-lime"><i class="fa fa-check"></i><b>14.1</b> Understanding sampling in LIME</a><ul>
<li class="chapter" data-level="14.1.1" data-path="lime-and-sampling.html"><a href="lime-and-sampling.html#formula"><i class="fa fa-check"></i><b>14.1.1</b> Formula</a></li>
<li class="chapter" data-level="14.1.2" data-path="lime-and-sampling.html"><a href="lime-and-sampling.html#sampling-strategies"><i class="fa fa-check"></i><b>14.1.2</b> Sampling strategies</a></li>
<li class="chapter" data-level="14.1.3" data-path="lime-and-sampling.html"><a href="lime-and-sampling.html#visualization-of-a-basic-example"><i class="fa fa-check"></i><b>14.1.3</b> Visualization of a basic example</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="lime-and-sampling.html"><a href="lime-and-sampling.html#sketching-problems-of-sampling"><i class="fa fa-check"></i><b>14.2</b> Sketching Problems of Sampling</a></li>
<li class="chapter" data-level="14.3" data-path="lime-and-sampling.html"><a href="lime-and-sampling.html#real-world-problems-with-lime"><i class="fa fa-check"></i><b>14.3</b> Real World Problems with LIME</a><ul>
<li class="chapter" data-level="14.3.1" data-path="lime-and-sampling.html"><a href="lime-and-sampling.html#boston-housing-data"><i class="fa fa-check"></i><b>14.3.1</b> Boston Housing Data</a></li>
<li class="chapter" data-level="14.3.2" data-path="lime-and-sampling.html"><a href="lime-and-sampling.html#rental-bikes-data"><i class="fa fa-check"></i><b>14.3.2</b> Rental Bikes Data</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="lime-and-sampling.html"><a href="lime-and-sampling.html#experiments-regarding-sampling-stability"><i class="fa fa-check"></i><b>14.4</b> Experiments regarding Sampling stability</a><ul>
<li class="chapter" data-level="14.4.1" data-path="lime-and-sampling.html"><a href="lime-and-sampling.html#influence-of-feature-dimension"><i class="fa fa-check"></i><b>14.4.1</b> Influence of feature dimension</a></li>
<li class="chapter" data-level="14.4.2" data-path="lime-and-sampling.html"><a href="lime-and-sampling.html#influence-of-sample-size"><i class="fa fa-check"></i><b>14.4.2</b> Influence of sample size</a></li>
<li class="chapter" data-level="14.4.3" data-path="lime-and-sampling.html"><a href="lime-and-sampling.html#influence-of-black-box"><i class="fa fa-check"></i><b>14.4.3</b> Influence of black-box</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="lime-and-sampling.html"><a href="lime-and-sampling.html#outlook-2"><i class="fa fa-check"></i><b>14.5</b> Outlook</a></li>
<li class="chapter" data-level="14.6" data-path="lime-and-sampling.html"><a href="lime-and-sampling.html#conclusion-2"><i class="fa fa-check"></i><b>14.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>15</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Limitations of Interpretable Machine Learning Methods</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Introduction</h1>
<p><em>Author: Emanuel Renkl</em></p>
<div id="statistical-modeling-the-two-approaches" class="section level2">
<h2><span class="header-section-number">1.1</span> Statistical Modeling: The Two Approaches</h2>
<!-- * Data Modeling Approach
* Algorithmic Modeling Approach
* Historical Development
* Machine Learning
* Information Criterion -->
<p>In statistics there are two approaches to reach conclusions from data (see <span class="citation">Breiman (<a href="#ref-breiman2001" role="doc-biblioref">2001</a><a href="#ref-breiman2001" role="doc-biblioref">b</a>)</span>). First, the data modeling approach, where one assumes that the data are generated by a given stochastic data model. More specifically, a proposed model associates the input variables, random noise and parameters with the response variables. Typical models are for instance the linear and logistic regression model. These models allow to predict what the responses are going to be to future input variables and give information on how the response variables and input variables are associated, i.e. they are interpretable. Second, the algorithmic modeling approach that uses algorithmic models and treats the underlying data mechanism as unknown. More precisely, the goal is to find an algorithm that operates on the input variables to predict the response variables. Algorithms that are used are for instance random forests and neural nets. These algorithms allow to predict what the responses are going to be to future input variables, but do not give information on how the response variables and input variables are associated. Put differently, these algorithms produce black box models, because they do not provide any direct explanation for their predictions, i.e. they are not interpretable.</p>
<p>Within the statistics community the data modeling approach was prevalently dominant for a long time (<span class="citation">Breiman (<a href="#ref-breiman2001" role="doc-biblioref">2001</a><a href="#ref-breiman2001" role="doc-biblioref">b</a>)</span>). However, especially in the last decade the increasing availability of enormous amounts of complex and unstructured data as well as the increase in processing power of computers served as a breeding ground for a strong shift to the algorithmic modeling approach, primarily for two reasons.<br />
First, the data modeling approach is not applicable to exciting problems like text, speech and image recognition (<span class="citation">Breiman (<a href="#ref-breiman2001" role="doc-biblioref">2001</a><a href="#ref-breiman2001" role="doc-biblioref">b</a>)</span>). Second, for complex prediction problems new algorithms such as random forests and neural nets outperform classical models in prediction accuracy as they can model complex relationships in the data (<span class="citation">Breiman (<a href="#ref-breiman2001" role="doc-biblioref">2001</a><a href="#ref-breiman2001" role="doc-biblioref">b</a>)</span>).
For these reasons, more and more researchers switched from the data modeling approach to the algorithmic modeling approach that is much more common under the name machine learning.</p>
<p>But what about interpretability? As we learned in the first paragraph machine learning algorithms are black box models that do not provide any direct explanation for their predictions. Hence, the questions arises whether we need to know why an algorithm makes a certain prediction? To get a better feeling for this question it’s helpful to understand how algorithms learn to make predictions and for which tasks machine learning is used.</p>
</div>
<div id="importance-of-interpretability" class="section level2">
<h2><span class="header-section-number">1.2</span> Importance of Interpretability</h2>
<!-- * How do algorithms learn
* Use cases in academia
* Uses cases in industry
* Summary -->
<p>Algorithms learn to make predictions from training data. Thus, algorithms also pick up biases of the training data and hence may not be robust under certain circumstances, e.g. they perform well on a test set, but not in the real-world. Such behavior can lead to undesired outcomes.<br />
For instance, consider a simple husky versus wolf classifier that misclassifies some huskies as wolves (see <span class="citation">M. T. Ribeiro, Singh, and Guestrin (<a href="#ref-ribeiro2016should" role="doc-biblioref">2016</a><a href="#ref-ribeiro2016should" role="doc-biblioref">b</a>)</span>). Since the machine learning model does not give any information on how the response and input variables are associated, we do not know why it classified a husky as a wolf. However, interpretability might be useful to debug the algorithm and see if this problem is persistent or not. Using methods that make machine learning algorithms interpretable and that we will discuss later in the book, we would find that the misclassification was due to the snow on the image. The algorithm learned to use snow as a feature for classifying images as wolf. This might make sense in the training dataset, but not in the real-world. Thus, in this example interpretability helps us to understand how the algorithm gets to the result and hence, we know in which cases the robustness of the algorithm is not given. In the following we want to derive the importance of interpretability by focusing on academic and industrial settings.</p>
<p>Broadly speaking in academia machine learning is used to draw conclusions from data. However, off the shelf machine learning algorithms only give predictions without explanations. Thus, they answer only the “what”, but not the “why” of a certain question and hence, do not allow for actual scientific findings. Especially, in areas such as life sciences and social sciences, that aim to identify causal relationships between input and response variables interpretability is key to scientific discoveries.<br />
For example, in a medical study researchers applying machine learning found that patients with pneumonia who have a history of asthma have a lower risk of dying from pneumonia than the general population (<span class="citation">Caruana et al. (<a href="#ref-caruana2015intelligible" role="doc-biblioref">2015</a>)</span>). This is of course counterintuitive. However it was a true pattern in the training data: pneumonia patients with a history of asthma were usually admitted not only to the hospital but also directly to the Intensive Care Unit. The aggressive care received by asthmatic pneumonia patients was so effective that it lowered their risk of dying from pneumonia compared to the general population. However, since the prognosis for these patients was above average, models trained on this data erroneously find that asthma reduces the risk, while asthmatics actually have a much higher risk if they are not hospitalized. Hence, in this example blind trust in the machine learning algorithm would yield misleading results.
Thus, interpretability is necessary in research to help identify causal relationships and increase the reliability and robustness of machine learning algorithms. Especially in areas outside of statistics the adoption of machine learning would be facilitated by making these models interpretable by adding explanations to their predictions.</p>
<p>In industry, machine learning is a standard component of almost any digital product offered by the big tech companies. From Amazons Alexa, or Netflixs movie recommendation system to the search algorithm from Google and Facebooks feed. These companies use machine learning to improve their products and business models. However, their machine learning algorithms are also built on training data collected from their users.
Thus, in the age af data leaks à la Cambridge Analytica people want to understand for what purposes their data is collected and how the algorithms work that keep them on the streaming platforms or urge them to buy additional products and spend more time on social media.<br />
Thus, in the digital world interpretability of machine learning models would yield to a broader understanding of machine learning in the society and make the technology more trustworthy and fair.
Switching to the analog world we see a far slower adoption of machine learning sytsems at scale. This is due to the fact that decions made by machine learning systems in the real world can have far more severe consequences than in the digital world.<br />
For instance, if the wrong movie is suggested to us it does not really matter, but if a machine learning system that is deployed to a self driving car does not recognize a cyclist it might take the wrong decision with real lifes at stake (see <span class="citation">Molnar (<a href="#ref-molnar2019" role="doc-biblioref">2019</a>)</span>). We need to be sure that the machine learning system is flawless, because driving over a cyclist is pretty bad. For example, an explanation might show that the most important feature is to recognize the two wheels of a bicycle, and this explanation helps you to think about certain edge cases, such as bicycles with side pockets that partially cover the wheels. Self-driving cars are just one example where machines are taking over decisions in the real world that were previously taken by humans and can involve severe and sometimes irreversible consequences. Interpretability helps to ensure the reliability and robusntess of these systems and thus makes them safer.</p>
<p>To conclude, adding interpretability to machine learning algorithms is necessary in both academic and industrial applications. While we distinguished between academia and industry settings the general points, causality, robustness &amp; reliability, trust and fairness are of course valid in both worlds.<br />
However, for academia interpretability is especially key to identify causal relationships and increase the reliability and robustness of scientific discoveries made by the help of machine learning algorithms.<br />
In industrial settings establishing trust in and fairness of machine learning systems matters most in low-risk environments whereas robustness and reliability is key to high-risk environments where machines take over decisions that have far reaching consequences.</p>
<p>Now that we established the importance of interpretability, how do we put this into practice? A restriction to machine learning models that are considered interpretable due to their simple structure, such as short decision trees or sparse linear models has the drawback that better performing models are excluded a priori from model selection. Hence, do we trade of prediction versus information and go back to more simple models? - No!<br />
We seperate the explanations from the machine learning model and apply interpretable methods that analyze the model after the model is trained.</p>
</div>
<div id="interpretable-machine-learning" class="section level2">
<h2><span class="header-section-number">1.3</span> Interpretable Machine Learning</h2>
<!-- * Definition of post-hoc, model-agnostic interpretation methods
* Overview of methods
* Examples -->
<p>As discussed in the previous chapter, most machine learning algorithms produce black box models, because they do not provide any direct explanation for their predictions. However, we do not want to restrict ourselves to models that are considered interpretable due to their simple structure and thus trade prediction accuracy for interpretability.
Instead, we make machine learning models interpretable by applying methods that analyze the model after the model is trained, i.e. we establish post-hoc interpretability. Moreover, we are separating the explanations from the machine learning model, i.e we focus on so called model-agnostic interpretation methods.<br />
Post-hoc, model-agnostic explanation systems have several advantages (<span class="citation">M. T. Ribeiro, Singh, and Guestrin (<a href="#ref-ribeiro2016model" role="doc-biblioref">2016</a><a href="#ref-ribeiro2016model" role="doc-biblioref">a</a>)</span>). First, since we seperate the underlying machine learning model and its interpretation developers can work with any model as the interpretation method is independent of the model. Thus we establish model flexibility. Second, since the interpretation is independent of the underlying machine learning model, the form of the interpretation also becomes independent. For instance in some cases it might be useful to have a linear formula, in other cases a graphic with feature importances is more appropriate. Thus, we establish explanation flexibility.</p>
<p>So what do these explanation systems do? - As discussed before, interpretation methods for machine learning algorithms ensure causal relationships, robustness &amp; reliability and establish trust and fairness. More specifically, they do so by shedding light on the following issues (see <span class="citation">Molnar (<a href="#ref-molnar2019" role="doc-biblioref">2019</a>)</span>):</p>
<ul>
<li>Algorithm transparency - How does the algorithm create the model?</li>
<li>Global, kolistic model interpretability - How does the trained model make predictions?</li>
<li>Global model interpretability on a modular level - How do parts of the model affect predictions?</li>
<li>Local interpretability for a single prediction - Why did the model make a certain prediction for an instance?</li>
<li>Local interpretability for a group of predictions - Why did the model make specific predictions for a group of instances?</li>
</ul>
<p>Now that we learned that post-hoc and model-agnostic methods ensure model as well as explanantion flexibility and in which ways explanation systems ensure causal relationships, robustness &amp; reliability and establish trust and fairness, we can move on to the outline of the booklet and discuss specific interpretation methods and their limitations.</p>
</div>
<div id="outline-of-the-booklet" class="section level2">
<h2><span class="header-section-number">1.4</span> Outline of the booklet</h2>
<!-- * Methods (Model agnostic)
  * Partial Dependence Plots (PDP)
  * Accumulated Local Effects (ALE)
  * Permutation Feature Importance (PMI)
  * Leave-One-Covariate Out (LOCO)
  * Local Interpretable Model-Agnostic Explanations (LIME)
* Limitations 
  * if a model models interactions (e.g. when a random forest is used)
  * if features strongly correlate with each other
  * if the model does not correctly model causal relationships
  * if parameters of the interpretation method are not set correctly -->
<p>This booklet introduces and investigates the limitations of current post-hoc and model agnostic approaches in interpretable machine learning, such as Partial Dependence Plots (PDP), Accumulated Local Effects (ALE), Permutation Feature Importance (PMI), Leave-One-Covariate Out (LOCO) and Local Interpretable Model-Agnostic Explanations (LIME). All of these methods can be used to explain the behavior and predictions of trained machine learning models. However, their reliability and compactness deteriorates when models use a high number of features, have strong feature interactions and complex feature main effects among others. In the following the methods mentioned are introduced and the outline of the booklet is given.</p>
<p>To start with, PDP and ALE are methods that enable a better understanding of the relationship between the outcome and feature variables of a machine learning model.
Common to both methods is that they reduce the prediction function to a function that depends on only one or two features (<span class="citation">Molnar (<a href="#ref-molnar2019" role="doc-biblioref">2019</a>)</span>). Both methods reduce the function by averaging the effects of the other features, but they differ in how the averages are calculated.<br />
PDP for example visualize whether the relationship between the outcome and a feature variable is for instance linear, monotonic or nonlinear and hence allow for a straightforward interpretation of the marginal effect of a certain feature on the predicted outcome of a model (<span class="citation">Friedman (<a href="#ref-friedman2001greedy" role="doc-biblioref">2001</a>)</span>). However, this holds only true as long as the feature in question is not correlated with any other features of the model. ALE plots are basically a faster and unbiased alternative to PDP, because they can interpret models containing correlated variables correctly (<span class="citation">Molnar (<a href="#ref-molnar2019" role="doc-biblioref">2019</a>)</span>). Chapter 1.0 of the booklet gives a short introduction to PDP. Next, PDP and its limitations when features are correlated and the case of feature interactions are investigated in chapter 1.1 and 1.2, respectively. Finally, chapter 1.3 discusses if PDP allow for causal interpretations. Chapter 2.0 gives then a short introduction to ALE. ALE and PDP are compared in detail in chapter 2.1. The choice of intervals, problems with pice-wise constant models and categorial features in the context of ALE are investigated in chapter 2.2.</p>
<p>Yet, PDP and ALE do not provide any insights to what extent a feature contributes to the predictive power of a model - in the following defined as feature importance. PFI and LOCO are two methods that allow to compute and visualize the importance of a certain feature for a machine learning model.
PFI by <span class="citation">Breiman (<a href="#ref-breiman2001random" role="doc-biblioref">2001</a><a href="#ref-breiman2001random" role="doc-biblioref">a</a>)</span> measures the importance of a feature by calculating the increase in the model’s prediction error after permuting the feature. Leave-One-Covariate-Out (LOCO) by <span class="citation">Lei et al. (<a href="#ref-lei2018distribution" role="doc-biblioref">2018</a>)</span>, requires to refit the model as the approach is based on leaving features out instead of permuting them (<span class="citation">Casalicchio, Molnar, and Bischl (<a href="#ref-casalicchio2018visualizing" role="doc-biblioref">2018</a>)</span>). Chapter 3.0 gives a short introduction to PFI and LOCO and gives rise to its limitations. Chapter 3.1 discusses both methods in the case of correlated features. Then partial and individual PFI are discussed in chapter 3.2 and the issue whether feature importanceshould be computed on training or test data is discussed in chapter 3.3.</p>
<p>Finally, LIME is a method that explains individual predictions of a black box machine learning model by locally approximating the prediction using a less complex and interpretable model (<span class="citation">Molnar (<a href="#ref-molnar2019" role="doc-biblioref">2019</a>)</span>). These simplifying models are referred to as surrogate models. Consider for instance a neural network that is used for a classification task. The neural network itself is of course not interpretable, but certain decision boundaries could for example be explained reasonably well by a logistic regression which in fact yields interpretable coefficients. To refer to the first paragraph of the introduction, we use the data modeling approach to explain the algorithmic modeling approach in this example. Chapter 4.0 gives a short introduction to LIME. Chapter 4.1 sheds light on the effect of the neighbourhood on LIME’s explanantion for tabular data. Chapter 4.2 deals with the sampling step in LIME and the resulting side effects in terms of feature weight stability of the surrogate model.</p>
<p>Now that we shortly introduced the different methods we can move to the respective chapters of the booklet which discuss the methods and their limitations in more detail and by the help of practical examples.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-breiman2001random">
<p>Breiman, Leo. 2001a. “Random Forests.” <em>Machine Learning</em> 45 (1): 5–32.</p>
</div>
<div id="ref-breiman2001">
<p>Breiman, Leo. 2001b. “Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).” <em>Statist. Sci.</em> 16 (3): 199–231. <a href="https://doi.org/10.1214/ss/1009213726">https://doi.org/10.1214/ss/1009213726</a>.</p>
</div>
<div id="ref-caruana2015intelligible">
<p>Caruana, Rich, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. 2015. “Intelligible Models for Healthcare: Predicting Pneumonia Risk and Hospital 30-Day Readmission.” In <em>Proceedings of the 21th Acm Sigkdd International Conference on Knowledge Discovery and Data Mining</em>, 1721–30. ACM.</p>
</div>
<div id="ref-casalicchio2018visualizing">
<p>Casalicchio, Giuseppe, Christoph Molnar, and Bernd Bischl. 2018. “Visualizing the Feature Importance for Black Box Models.” In <em>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</em>, 655–70. Springer.</p>
</div>
<div id="ref-friedman2001greedy">
<p>Friedman, Jerome H. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” <em>Annals of Statistics</em>, 1189–1232.</p>
</div>
<div id="ref-lei2018distribution">
<p>Lei, Jing, Max G’Sell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman. 2018. “Distribution-Free Predictive Inference for Regression.” <em>Journal of the American Statistical Association</em> 113 (523): 1094–1111.</p>
</div>
<div id="ref-molnar2019">
<p>Molnar, Christoph. 2019. <em>Interpretable Machine Learning: A Guide for Making Black Box Models Explainable</em>.</p>
</div>
<div id="ref-ribeiro2016model">
<p>Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016a. “Model-Agnostic Interpretability of Machine Learning.” <em>arXiv Preprint arXiv:1606.05386</em>.</p>
</div>
<div id="ref-ribeiro2016should">
<p>Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016b. “Why Should I Trust You?: Explaining the Predictions of Any Classifier.” In <em>Proceedings of the 22nd Acm Sigkdd International Conference on Knowledge Discovery and Data Mining</em>, 1135–44. ACM.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="introduction-to-partial-dependence-plots-pdp-and-individual-conditional-expectation-ice.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/compstat-lmu/iml_methods_limitations/edit/master/00-introduction.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub", "book.mobi"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
