# Introduction (work in progress)


## Statistical Modeling: The Two Approaches
* Data Modeling Approach
* Algorithmic Modeling Approach
* Historical Development
* Machine Learning
* Information Criterion

In statistics there are two approaches to reach conclusions from Data. First, the data modeling approach, where one assumes that the data are generated by a given stochastic data model. More specifically, a proposed model associates the input variables, random noise and parameters with the response variables. Typical models are for instance the linear and logistic regression model. These models allow to predict what the responses are going to be to future input variables and give information on how the response variables and input variables are associated, i.e. they are interpretable.  
Second, the algorithmic modeling approach that uses algorithmic models and treats the underlying data mechanism as unknown. More precisely, the goal is to find an algorithm that operates on the input variables to predict the response variables. Algorithms that are used are for instance random forests and neural nets. These algorithms allow to predict what the responses are going to be to future input variables, but do not give information on how the response variables and input variables are associated. Put differently, these algorithms produce black box models, because they do not provide any direct explanation for their predictions.

Within the statistics community the data modeling approach was prevalently dominant for a long time. However, especially in the last decade the increasing availability of enormous amounts of complex and unstructured data as well as the increase in processing power of computers served as a breeding ground for a strong shift to the algorithmic modeling approach, primarily for two reasons.  
First, the data modeling approach is not applicable to exciting problems like text, speech and image recognition. Second, for complex prediction problems new algorithms such as random forests and neural nets outperform classical models in prediction accuracy as they can model complex relationships in the data. 
For these reasons, more and more researchers switched from the data modeling approach to the algorithmic modeling approach that is much more common under the name machine learning.  
But what about the interpretability? As we learned in the first paragraph machine learning algorithms are black box models that do not provide any direct explanation for their predictions. Hence, the questions arises whether we need to know why an algorithm makes a certain prediction? To get a better feeling for this question it's helpful to understand how algorithms learn to make predictions and for which tasks machine learning is used in industry and academia.


## Importance of Interpretability
* How do algorithms learn
* Use cases in academia
* Uses cases in industry
* Summary

Algorithms learn to make predictions from training data. By default algorithms also pick up biases of the training data and hence may not robust under certain circumstances, i.e. they perform well on a test set, but not in the real world. This can lead to undesired outcomes.  
For instance, consider a simple husky versus wolf classifier that misclassifies some huskies as wolves. Since the model does not give information on how the response variables and input variables are associated, we do not know why it classified a husky as a wolve. Using methods that make machine learning algorithms interpretable and that we will discuss later in the book, we would find that the misclassification was due to the snow on the image. The classifier learned to use snow as a feature for classifying images as “wolf”. This might make sense in the training dataset, but not in the real-world. In this example. interpretability helps us to understand how the algorithm gets to the result and we can think how appropriate the algorithm is to solve the problem or think of cases where the robustness of the algorithm is not given.  
In the following we want to investigate if there is need for interpretability focusing on academic and indsutrial settings.

In academia machine learning is used to draw conclusions from data, very broadly speaking. Opaque machine learning models do not allow for scientific findings if the model only gives predictions without explanations. what is answerd but not the why? Why helps you learn more about the problem and scientific ...  Especially for the adoption of machine learning outside of statistics, in areas such as life sciences and social sciences, that aim to identify causal relationships interpretability and information is important. example 
that usually restrict model selection to interpretable models such as linear regression models [23]. This often relies on an intuitive notion of interpretability, leading to an avoidance of ”black boxes” such as tree ensembles and neural networks [5]. For instance, xyz. To facilitate adoption of machine learning and scientific results by using it as to why certain predictions or behaviors are created by machines, interpretability and explanations are crucial. 
“HasAsthama(x)⇒LowerRisk(x)”, i.e., that patients with pneumonia who havea history of asthma have lower risk of dying from pneumo-nia than the general population. Needless to say, this ruleis counterintuitive. But it reflected a true pattern in thetraining data: patients with a history of asthma who pre-sented with pneumonia usually were admitted not only tothe hospital but directly to the ICU (Intensive Care Unit).The good news is that the aggressive care received by asth-matic pneumonia patients was so effective that it loweredtheir risk of dying from pneumonia compared to the generalpopulation. The bad news is that because the prognosis forthese patients is better than average, models trained on thedata incorrectly learn that asthma lowers risk, when in factasthmatics have much higher risk (if not hospitalized).
Causality: Check that only causal relationships are picked up.
Reliability or Robustness: Ensuring that small changes in the input do not lead to large changes in the prediction.

In industry, machine learning is a standard component of almost any product offered by the big tech companies. From Amazons Alexa, or Netflix movie recommendation system, to Google search algorithm and Facebooks feed. Machine learning is everywhere, at least in the digital world.   
However, this trend will accelerate and also be adopted in the analog world. Catchword self-driving cards. This is just one very striking example where machines are taking over decisions in the real world that were previously taken by humans and require safety measures and testing.
The difference in the analog world is that the consequences of decisons taken by machine learning systems are far more severe then in the analog world. For instance, if the wrong movie is suggested to us it does not really matter, but if an machine leanrning system does not recognize a cyclist for a car it might take the wrong decision with real lifes at stake. You want to be 100% sure that the abstraction the system has learned is error-free, because running over cyclists is quite bad. For instance, an explanation might reveal that the most important learned feature is to recognize the two wheels of a bicycle, and this explanation helps you think about edge cases like bicycles with side bags that partially cover the wheels.
Moreover,  It might happen that the machine learning model you have trained for automatic approval or rejection of credit applications discriminates against a minority. Your main goal is to grant loans only to people who will eventually repay them. The incompleteness of the problem formulation in this case lies in the fact that you not only want to minimize loan defaults, but are also obliged not to discriminate on the basis of certain demographics. This is an additional constraint that is part of your problem formulation (granting loans in a low-risk and compliant way) that is not covered by the loss function the machine learning model was optimized for.
Fairness: Ensuring that predictions are unbiased and do not implicitly or explicitly discriminate against protected groups. An interpretable model can tell you why it has decided that a certain person should not get a loan, and it becomes easier for a human to judge whether the decision is based on a learned demographic (e.g. racial) bias.
Privacy: Ensuring that sensitive information in the data is protected.
Trust: It is easier for humans to trust a system that explains its decisions compared to a black box.

To conclude interpretability of machine learning would be quite good. For academia the information criterion/ interpretability is necessary to draw conclusions and find causal relationships. Referring to the example for industrial applications the necessity of an information /interpretability criterion depends on the environment where the machine learning system is used and where next to prediction also constrains need to be . Is it low-risk or high-risk?  
So how do we do it? A restriction to structurally simpler models has the drawback that better performing models are often excluded a priori from model selection. Do we trade of prediction vs information and go back to old models? No! We use the data modeling approach to understand the algorithmic modeling approach and thus combine the best out of both worlds. An interpretation for an erroneous prediction helps to understand the cause of the error. It delivers a direction for how to fix the system.
useful debugging tool for detecting bias and robustness in machine learning models.  

## Interpretable Machine Learning
* Definition
* Example
* Unterteilung (model agnostic etc.), Importance
* Überleitung zu Limitations


## Outline of the booklet
* Methods
* Limitations 

This project explains the limitations of current approaches in interpretable machine learning, such as partial dependence plots (PDP, Accumulated Local Effects (ALE), permutation feature importance, leave-one-covariate out (LOCO) and local interpretable model-agnostic explanations (LIME). All of those methods can be used to explain the behavior and predictions of trained machine learning models. The interpretation methods might not work well in the following cases:

* if a model models interactions (e.g. when a random forest is used)
* if features strongly correlate with each other
* if the model does not correctly model causal relationships
* if parameters of the interpretation method are not set correctly
