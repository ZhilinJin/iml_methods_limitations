# Introduction (work in progress)

## Statistical Modeling: The Two Approaches (first draft, citations to be inserted)
* Data Modeling Approach
* Algorithmic Modeling Approach
* Historical Development
* Machine Learning
* Information Criterion

In statistics there are two approaches to reach conclusions from Data. First, the data modeling approach, where one assumes that the data are generated by a given stochastic data model. More specifically, a proposed model associates the input variables, random noise and parameters with the response variables. Typical models are for instance the linear and logistic regression model. These models allow to predict what the responses are going to be to future input variables and give information on how the response variables and input variables are associated, i.e. they are interpretable.  
Second, the algorithmic modeling approach that uses algorithmic models and treats the underlying data mechanism as unknown. More precisely, the goal is to find an algorithm that operates on the input variables to predict the response variables. Algorithms that are used are for instance random forests and neural nets. These algorithms allow to predict what the responses are going to be to future input variables, but do not give information on how the response variables and input variables are associated. Put differently, these algorithms produce black box models, because they do not provide any direct explanation for their predictions.

Within the statistics community the data modeling approach was prevalently dominant for a long time. However, especially in the last decade the increasing availability of enormous amounts of complex and unstructured data as well as the increase in processing power of computers served as a breeding ground for a strong shift to the algorithmic modeling approach, primarily for two reasons.  
First, the data modeling approach is not applicable to exciting problems like text, speech and image recognition. Second, for complex prediction problems new algorithms such as random forests and neural nets outperform classical models in prediction accuracy as they can model complex relationships in the data. 
For these reasons, more and more researchers switched from the data modeling approach to the algorithmic modeling approach that is much more common under the name machine learning.  

But what about the interpretability? As we learned in the first paragraph machine learning algorithms are black box models that do not provide any direct explanation for their predictions. Hence, the questions arises whether we need to know why an algorithm makes a certain prediction? To get a better feeling for this question it's helpful to understand how algorithms learn to make predictions and for which tasks machine learning is used in industry and academia.


## Importance of Interpretability (first draft, citations to be inserted)
* How do algorithms learn
* Use cases in academia
* Uses cases in industry
* Summary
 
Algorithms learn to make predictions from training data. Thus, algorithms also pick up biases of the training data and hence may not be robust under certain circumstances, e.g. they perform well on a test set, but not in the real-world. This can lead to undesired outcomes.  
For instance, consider a simple husky versus wolf classifier that misclassifies some huskies as wolves. Since the machine learning model does not give any information on how the response and input variables are associated, we do not know why it classified a husky as a wolf. Using methods that make machine learning algorithms interpretable and that we will discuss later in the book, we would find that the misclassification was due to the snow on the image. The algorithm learned to use snow as a feature for classifying images as wolf. This might make sense in the training dataset, but not in the real-world. Thus, in this example interpretability helps us to understand how the algorithm gets to the result and hence, we know in which cases the robustness of the algorithm is not given.  
In the following we want to derive the importance of interpretability by focusing on academic and industrial settings.

Broadly speaking in academia machine learning is used to draw conclusions from data. However, off the shelf machine learning algorithms only give predictions without explanations. Thus, they answer only the "what", but not the "why" of a certain question and hence, do not allow for actual scientific findings. Especially, in areas such as life sciences and social sciences, that aim to identify causal relationships between input and response variables interpretability is key to scientific discoveries.  
For example, in a medical study researchers applying machine learning found that patients with pneumonia who have a history of asthma have a lower risk of dying from pneumonia than the general population. This is of course counterintuitive. However it was a true pattern in the training data: pneumonia patients with a history of asthma were usually admitted not only to the hospital but also directly to the Intensive Care Unit. The aggressive care received by asthmatic pneumonia patients was so effective that it lowered their risk of dying from pneumonia compared to the general population. However, since the prognosis for these patients was above average, models trained on this data erroneously find that asthma reduces the risk, while asthmatics actually have a much higher risk if they are not hospitalized. Hence, in this example blind trust in the machine learning algorithm would yield misleading results. 
Thus, interpretability is necessary in research to help identify causal relationships and increase the reliability and robustness of machine learning algorithms. Especially in areas outside of statistics the adoption of machine learning would be facilitated by making these models interpretable by adding explanations to their predictions.  

In industry, machine learning is a standard component of almost any digital product offered by the big tech companies. From Amazons Alexa, or Netflixs movie recommendation system to the search algorithm from Google and Facebooks feed. These companies use machine learning to improve their products and business models. However, their machine learning algorithms are also built on training data collected from their users.
Thus, in the age af data leaks Ã  la Cambridge Analytica people want to understand for what purposes their data is collected and how the algorithms work that keep them on the streaming platforms or urge them to buy additional products and spend more time on social media.  
Thus, in the digital world interpretability of machine learning models would yield to a broader understanding of machine learning in the society and make the technology more trustworthy and fair.
Switching to the analog world we see a far slower adoption of machine learning sytsems at scale. This is due to the fact that decions made by machine learning systems in the real world can have far more severe consequences than in the digital world.  
For instance, if the wrong movie is suggested to us it does not really matter, but if a machine learning system that is deployed to a self driving car does not recognize a cyclist it might take the wrong decision with real lifes at stake. We need to be sure that the machine learning system is flawless, because driving over a cyclist is pretty bad. For example, an explanation might show that the most important feature is to recognize the two wheels of a bicycle, and this explanation helps you to think about certain edge cases, such as bicycles with side pockets that partially cover the wheels. Self-driving cars are just one example where machines are taking over decisions in the real world that were previously taken by humans and can involve severe and sometimes irreversible consequences. Interpretability helps to ensure the reliability and robusntess of these systems and thus makes them safer. 

To conclude, adding interpretability to machine learning algorithms is necessary in both academic and industrial applications. While we distinguished between academia and industry settings the general points, causality, robustness & reliability, trust and fairness are of course valid in both worlds.  
However, for academia interpretability is especially key to identify causal relationships and increase the reliability and robustness of scientific discoveries made by the help of machine learning algorithms.  
In industrial settings establishing trust in and fairness of machine learning systems matters most in low-risk environments whereas robustness and reliability is key to high-risk environments where machines take over decisions that have far reaching consequences.  

Now that we established the importance of interpretability, how do we put this into practice? A restriction to structurally simpler models has the drawback that better performing models are often excluded a priori from model selection. Hence, do we trade of prediction versus information and go back to the old models? - No!  
We use the data modeling approach to understand the algorithmic modeling approach and thus combine the best out of both approaches. 

## Interpretable Machine Learning (work in progress)
* Definition
* State of research
* Overview of methods
* Examples


## Outline of the booklet (work in progress)
* Methods
* Limitations 

This booklet investigates the limitations of current approaches in interpretable machine learning, such as partial dependence plots (PDP, Accumulated Local Effects (ALE), permutation feature importance, leave-one-covariate out (LOCO) and local interpretable model-agnostic explanations (LIME). All of these methods can be used to explain the behavior and predictions of trained machine learning models. The interpretation methods might not work well in the following cases:

* if a model models interactions (e.g. when a random forest is used)
* if features strongly correlate with each other
* if the model does not correctly model causal relationships
* if parameters of the interpretation method are not set correctly

