# PDP and Causal Interpretation {#pdp-causal}

*Author: Thommy Dassen*

```{r include=FALSE}
library(dplyr) # for group by
library(Ecdat) # icecream datasets
library(ggdag) # for structural causal graphs
library(ggpubr) # ggarrange
library(iml)  # pdp
library(randomForest)
```

## Introduction
In this chapter an attempt will be made to evaluate the merits of a causal interpretation of Partial Dependance Plots (PDPs). In order to do so, Directed Acyclical Graphs (DAGs) are used to visualize various data generating mechanisms. Judea Pearl's framework of do-calculus is used to intervene on simulated data. The goal is to compare the PDP with the expected value under an intervention. We will see scenarios in which the PDP gives the same result as an intervention and scenarios in which the limitations of the PDP as a tool for causal interpretation become clear.  

To interpret causally means to interpret one state (the effect) to be the result of another (the cause), with the cause being (partly) responsible for the effect, and the effect being partially dependent on the cause. [@zhaohastie] formulated three elements that are needed to ensure that the PDP coincides with the intervention effect:  
1. A good predictive model which closely approximates the real relationship.  
2. Domain knowledge to ensure the causal structure makes sense and the backdoor criterion, explained below, is met.  
3. A visualization tool like a PDP (or an Individual Conditional Expectation plot)  

The first condition is an important one, because there is a big difference between being able to causally interpret an effect for the *model* and using it as a causal interpretation for the real world. The second condition will make clear when PDPs are  the same, and when they are different from interventions on the data. In this chapter we will systematically analyze a number of scenarios in order to see under which conditions PDPs can be causally interpreted or not. 

## A brief look at PDP problems  

Before we have a look at various scenarios and settings involving interventions, let's look at two examples of other problems that can occur when using a PDP. The first example shows the need for domain knowledge. Let's say we have a dataset containing data on the amount of ice-cream consumption per capita and the temperature outside. Intuitively, we know in this case that there will be a relationship between the two: People eat more ice-cream when temperatures are high than when temperatures are low. Just looking at the data, however, the direction of the relationship is not as evident. Below are two PDPs, showing the effect of one variable on the other.
```{r echo=FALSE, message=FALSE, warning=FALSE ,}
data("Icecream")
 rf.icecream = randomForest(cons ~ temp
                         , data = Icecream, importance = T)
  mod = Predictor$new(rf.icecream, data = Icecream)
  pdp.obj = FeatureEffect$new(mod, feature = c("temp"))  
  p1 <- plot(pdp.obj) 
  
    rf.icecream_reverse = randomForest(temp ~ cons
                             , data = Icecream, importance = T)
  mod_reverse = Predictor$new(rf.icecream_reverse, data = Icecream)
  pdp.obj_reverse = FeatureEffect$new(mod_reverse, feature = c("cons"))  
  p2 <- plot(pdp.obj_reverse) 
  
```

```{r  echo=FALSE, Figure1, out.width='100%', fig.align= 'left', fig.cap="The PDP on the left shows temperature causing ice-cream consumption. The PDP on the right shows ice-cream consumption causing temperatures. Which one is correct?"}
knitr::include_graphics('images/ice_cream.jpeg', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

Clearly it would be wrong to interpret the second plot causally. While it would be ok to say that higher consumption of icecream is associated with higher temperatures, no one would ever say that higher icecream consumption *causes* higher temperatures. This example illustrates the need for domain knowledge about the causal structure between variables in order to be able to interpret PDPs causally. Blindly plotting a PDP and interpreting it causally is problematic. Not all cases are as clear-cut as the one here: Think for instance of the link between smoking and depression. Does smoking cause depression, or depression cause smoking? Potentially they cause and reinforce each other. Directionality is only one aspect that can be solved by domain knowledge. We will see later domain knowledge is also a necessity in order to determine whether the so-called backdoor criterion is met.

Another problem with PDPs was shown by [@scholbeck]. Assume we have data that is distributed as follows:

$$ Y \leftarrow  X_{1}^2 - 15X_{1}X_2 + \epsilon $$
$$ X_1 \sim \mathcal{U}(-1, 1), \ \ \ \ \ X_2 \sim \mathcal{U}(-1, 1), \ \ \ \ \ \epsilon \sim \mathcal{N}(0,0.1), \ \ \ \ \ N \leftarrow 1000    $$
Training a Random Forest on this data leads to Figure \@ref(fig:Figure2) below. The problem should be immediately clear. Looking at the PDP, one would assume that $X_1$ has virtually no impact on $Y$. The ICE curves, however, show that the averaging effect of the PDP completely obfuscates the true effect, which is highly positive for some observations while being highly negative for others. In this example too it would be misguided to simply interpret the PDP causally and state that $X_1$ does not have any impact on $Y$ whatsoever. This would be correct for the average effect, but evidently not true on the individual level.

```{r eval=FALSE, include=FALSE}
set.seed(13)
N <- 1000
e <- rnorm(N, 0, 0.1)
x1 <- runif(N, -1, 1)
x2 <- runif(N, -1, 1)
y <- x1^2 - 15*x1*x2 + e

df <- data.frame(x1, x2, y)
rf.test = randomForest(y ~ . , data = df, importance = T, ntree = 500)
mod = Predictor$new(rf.test, data = df)

pdp.obj = FeatureEffect$new(mod, feature = c("x1"), method = "pdp+ice")  
plot(pdp.obj)

```
```{r  echo=FALSE, Figure2, out.width='100%', fig.align= 'left', fig.cap="The average effect of the PDP (yellow line) hides the heterogeneity of the individual effects "}
knitr::include_graphics('images/avg_vs_individual.jpeg', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

## Causal Interpretability: Interventions and Directed Acyclical Graphs

In the rest of this chapter, we will use the concept of interventions and show causal structures visually. When we make an intervention, it means we fix the value of a variable [@pearl1993]. We change every instance of a particular variable to a specific value chosen by us. This can be done to see the effect a variable has when at a specific value. When we intervene, values of a variable no longer vary in response to other variables. This usually means that the values for other variables change as well as a result. 
The difference between a variable $X$ taking a value $x$ naturally and having a fixed value $X=x$ is reflected in the notation. The latter is denoted by $do(X=x)$. As such, $P(Y=y|X=x)$ is the probability that $Y=y$ conditional on $X=x$. $P(Y=y|do(X=x))$ is then the population distribution of $Y$ if the value of $X$ was fixed at $x$ for the entire population.

Causal structures will be visualized with the use of Direct Acyclical Graphs. A DAG is a representation of relationships between variables in graphical form. Each variable is represented as a *node* and the lines between these nodes, or *edges*, show the direction of the causal relationship through arrowheads. In addition to being directed, these graphs are per definition acyclical. This means that a relationship $X \rightarrow Y \rightarrow Z \rightarrow X$ can not be represented as a DAG. Several examples of DAGs follow in the rest of the chapter, as each scenario starts with on. With regards to interventions, in a graphical sense this simply means removing edges from direct parents to the variable.

In order to know when a causal interpretation makes sense, more is needed than only a representation of a DAG and knowledge of how to do an intervention. An important formula introduced by  [@pearl1993] adresses exactly this problem: The back-door adjustment formula. This formula stipulates that the causal effect of $X_S$ on $Y$ can be identified if the causal relationship between the variables can be visualized in a graph and $X_C$, the complementary set to $X_S$, adheres to what he called the back-door criterion. The back-door adjustment formula is:

$$P(Y|do(X_S = x_S)) = \int P(Y |X_S = x_S, X_C = x_C) dP(x_C)$$
As [@zhaohastie] pointed out, this formula is basically the same as the formula for the partial
dependence of $g$ on a subset of variables $X_S$ given output $g(x)$:

$$ g_S(x_S) = \mathbb E_{x_C}[g(x_S, X_C)] = \int g(x_S, x_C)dP(x_C)  $$
If we take the expectation of Pearl's adjustment formula we get:
$$ E[Y |do(X_S = x_S)] = \int E[Y |X_S = x_S, X_C = x_C] dP(x_C) $$
These last two formulas are the same, if $C$ is the complement of $S$. 

[@pearl1993] defined a back-door criterion that needs to be fulfilled in order for the adjustment formula to be valid. It states that:

1. No node in $X_C$ can be a descendant of $X_S$ in the DAG $G$.

2. Every "back-door" path between $X_S$ and $Y$ has to be blocked by $X_C$.



## Scenarios

After these two introductory problems of PDPs, the rest of this chapter will look at PDPs through the causal framework of [@pearl1993]. This means we will look at various causal scenarios visualized through DAGs and compare the PDPs created under this structure with the actual effect of interventions. 

In each scenario nine settings will be simulated for PDP creation, consisting of three standard deviations for the error term (0.1, 0.3 and 0.5) and three magnitudes of observations (100, 1000, 1000). Furthermore, each setting for the PDP was simulated across twenty runs. Each of the nine plots will therefore show twenty PDPs in order to give a solid view of the relationship the PDPs capture for each setting. In addition to the plots of the PDPs, which will be the first three columns in each figure, the actual effect under intervention will be shown in a fourth column as a single yellow line. The interventions were all run with a thousand observations. Initial tests resulted in a large increase in computation time with a higher number of observations, but with results that hardly differed from those obtained with one thousand observations.

The process for obtaining the intervention curve was as follows:
Let $X$ be the predictor variable of interest with possible values $x_{1}, x_{2}, x_{n}$ and $Y$ the response variable of interest.
for each unique $i \in \{1,2,\dots,n\}$ do  
(1) make a copy of the data set  
(2) replace the original values of $X$ with the value $X_{(i)}$ of $X$ under intervention  
(3) recompute all variables dependent on $X$ using the replacement values as input. This includes $Y$, but potentially also other features that rely on $X$ for their value.  Note that only $X$ is replaced with $X_i$ in the existing equations. Both the equations and error terms remain the same as before.  
(4) Compute the average $Y_i$ in dataset $i$ given $X_i$.  
(5) ($X_i$, $Y_i$) are a single point on the intervention curve.  

**Scenario 1:  **

```{r Figure3, echo=FALSE, fig.height = 3, fig.width = 3.5, fig.cap="Chain DAG where X has a direct impact on Y, but is dependent on Z"}
set.seed(18)
dagify(Y ~ X,
       X ~ Z
) %>% 
  ggdag() 
```

In the first scenario, we have a chain DAG, seen in Figure \@ref(fig:Figure3). Our variable $X$ is impacted by $Z$ and has a direct effect on $Y$. $Z$, however, does not. $X_C$ consists of $Z$, which is not a descendant of $X$. There is also no backdoor path between $X$ and $Y$. The backdoor criterion is met. In this scenario the expectation is thus that the PDPs should be overall equal to the true intervention. The initial simulation settings for this scenario are as follows:

$$ Y \leftarrow X + \epsilon_Y  $$
$$ \epsilon_X,\epsilon_Y ~ \sim \mathcal{N}(0, 0.1), \ \ \ \ \ \epsilon_Z \sim \mathcal{U}(-1,1),\ \ \ \ \ Z \leftarrow \epsilon_Z, \ \ \ \ \ X \leftarrow Z + \epsilon_X, \ \ \ \ \ N \leftarrow 100 $$

As will be done in all scenarios, both standard deviaton for $\epsilon$ and $N$ were varied across 3 levels leading to 9 settings.  

```{r  echo=FALSE, Figure4, out.width='100%', fig.align= 'left', fig.cap="Comparison for scenario 1 of PDPs under various settings with the (yellow) intervention curve on the right"}
knitr::include_graphics('images/scenario1_all.jpeg', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```


Overall the PDPs match the intervention curves fairly well. Outside of the extreme regions of $X$, where some curvature is present, the linear quality of the intervention curve is evident in the PDPs. Furthermore, the scale of $\hat{Y}$ is comparable to the scale of $Y_{intervention}$ in most settings.



```{r eval=FALSE, include=FALSE}
# pdp
pdp_simulation <- function(runs, N, error_std){ 
  pdp_simulations <- setNames(data.frame(matrix(ncol = 2, nrow = 0)),  c("X1", "y_hat"))
  
  for(i in 1:runs){
    e <- rnorm(N,0, error_std)
    Z <- runif(N, -1, 1)
    X <- Z + e
    Y <- X + e
    df <- data.frame(X, Z, Y)
    
    rf.basic = randomForest(Y ~ . , data = df, importance = T)
    mod = Predictor$new(rf.basic, data = df)
    pdp.obj = FeatureEffect$new(mod, feature = c("X"), method = "pdp") 
    p_pdp_mod <- plot(pdp.obj)
    
    simulation <- as.data.frame(pdp.obj$results[,1:2])
    simulation$run <- i
    colnames(simulation) <- c("X", "Y_hat", "run_number")
    pdp_simulations <- rbind.data.frame(pdp_simulations, simulation)
  }
  ggplot(data = pdp_simulations) + geom_line(data = pdp_simulations,aes(x = pdp_simulations[,1], y = pdp_simulations[,2], group = pdp_simulations[,3]), alpha = 0.5)
}
# standard deviation eq 0.1
scen1_plot_1 <- pdp_simulation(runs=20, N = 100, error_std=0.1)
scen1_plot_1 <- scen1_plot_1 + labs(x = "X", y = "Y_hat")
scen1_plot_2 <- pdp_simulation(runs=20, N = 1000, error_std=0.1)
scen1_plot_2 <- scen1_plot_2 + labs(x = "X", y = "Y_hat")
scen1_plot_3 <- pdp_simulation(runs=20, N = 10000, error_std=0.1)
scen1_plot_3 <- scen1_plot_3 + labs(x = "X", y = "Y_hat")
# standard deviation eq 0.3
scen1_plot_4 <- pdp_simulation(runs=20, N = 100, error_std=0.3)
scen1_plot_4 <- scen1_plot_4 + labs(x = "X", y = "Y_hat")
scen1_plot_5 <- pdp_simulation(runs=20, N = 1000, error_std=0.3)
scen1_plot_5 <- scen1_plot_5 + labs(x = "X", y = "Y_hat")
scen1_plot_6 <- pdp_simulation(runs=20, N = 10000, error_std=0.3)
scen1_plot_6 <- scen1_plot_6 + labs(x = "X", y = "Y_hat")
# standard deviation eq 0.5
scen1_plot_7 <- pdp_simulation(runs=20, N = 100, error_std=0.5)
scen1_plot_7 <- scen1_plot_7 + labs(x = "X", y = "Y_hat")
scen1_plot_8 <- pdp_simulation(runs=20, N = 1000, error_std=0.5)
scen1_plot_8 <- scen1_plot_8 + labs(x = "X", y = "Y_hat")
scen1_plot_9 <- pdp_simulation(runs=20, N = 10000, error_std=0.5)
scen1_plot_9 <- scen1_plot_9 + labs(x = "X", y = "Y_hat")

# intervention

intervention <- function(runs, N, error_std){ 
  df_intervention_all <- setNames(data.frame(matrix(ncol = 3, nrow = 0)), c("X", "Z", "Y"))
  for(i in 1:runs){
    
    e <- rnorm(N,0, error_std)
    Z <- runif(N, -1, 1)
    X <- Z + e
    Y <- X + e
    df <- data.frame(X, Z, Y)
    for(j in 1:length(unique(df$X))){
      X <- unique(df$X)[j]
      Y <- X + e
      df_intervention <- data.frame(X, Z, Y)
      df_intervention_all <- rbind.data.frame(df_intervention_all, df_intervention)
    }
  }
  
  df_intervention_group <- df_intervention_all %>%
    group_by(X) %>%
    summarise(Y_intervention = mean(Y),
              n = n()) %>%
    
    arrange(X)
  
  ggplot(aes(X, Y_intervention), data = df_intervention_group) + geom_line(colour="gold2", size = 1.5)
}
# standard deviation eq 0.1
scen1_int_plot_2 <- intervention(runs=1, N = 1000, error_std=0.1)
# standard deviation eq 0.3
scen1_int_plot_5 <- intervention(runs=1, N = 1000, error_std=0.3)
# standard deviation eq 0.5
scen1_int_plot_8 <- intervention(runs=1, N = 1000, error_std=0.5)


scenario_1_all <- ggarrange(scen1_plot_1, # N = 100,  std = 0.1
                            scen1_plot_2, # N = 1000, std = 0.1
                            scen1_plot_3, # N = 2000, std = 0.1
                            scen1_int_plot_2,
                            
                            scen1_plot_4, # N = 100,  std = 0.3
                            scen1_plot_5, # N = 1000, std = 0.3
                            scen1_plot_6, # N = 2000, std = 0.3
                            scen1_int_plot_5,
                            
                            scen1_plot_7, # N = 100,  std = 0.5
                            scen1_plot_8, # N = 1000, std = 0.5
                            scen1_plot_9, # N = 2000, std = 0.5
                            scen1_int_plot_8,
                            
                            nrow= 3, ncol = 4)
annotate_figure(scenario_1_all,
                top = text_grob("Increasing number of observations to the right", color = "black"),
                left = text_grob("Increasing standard deviation of error to the bottom", color = "black", rot = 90),
                 fig.lab.face = "bold")

```



**Scenario 2: Chain DAG**

```{r Figure5, echo=FALSE, fig.height = 3, fig.width = 3.5, fig.cap="Chain DAG where X has no a direct impact on Y, but only indirectly through Z"}
set.seed(18)
dagify(Y ~ Z,
       Z ~ X
) %>% 
  ggdag() 
```

In this scenario the DAG again looks like a chain. $X$ has an effect on $Y$ through $Z$, but no direct relationship between $X$ and $Y$ exists. Note that since $Z$ is a descendant of $X$, the PDP and intervention curve should not coincide. The initial simulation settings for this scenario are as follows: 

$$ Y \leftarrow Z + \epsilon_Y  $$
$$ \epsilon_Y,\epsilon_Z ~ \sim \mathcal{N}(0, 0.1), \ \ \ \ \ \epsilon_X \sim \mathcal{U}(-1,1), \ \ \ \ \ X \leftarrow \epsilon_X, \ \ \ \ \ Z \leftarrow X + \epsilon_Z, \ \ \ \ \ N \leftarrow 100 $$


```{r  echo=FALSE, Figure6, out.width='100%', fig.align= 'left', fig.cap="Comparison for scenario 2 of PDPs under various settings with the (yellow) intervention curve on the right"}
knitr::include_graphics('images/scenario2_all.jpeg', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

As can be seen from Figure \@ref(fig:Figure6), the PDP plots do not match the intervention plots well in several cases. In fact, four out of nine settings show a negative slope for the relationship between $X$ and $Y$ in comparison to the overall positive slope for the true intervention. The first row performs best, as expected due to the relatively small error that is used. Interesting is also that in the second and third row, where the error has been increased, the PDP slope goes from positive to negative between the first and second column. PDP accuracy thus suffers in situations where the observation count is low.

```{r eval=FALSE, include=FALSE}
# pdp
pdp_simulation <- function(runs, N, error_std){ 
  pdp_simulations <- setNames(data.frame(matrix(ncol = 2, nrow = 0)),  c("X1", "y_hat"))

  for(i in 1:runs){
    e <- rnorm(N,0, error_std)
    X <- runif(N, -1, 1)
    Z <- X + e
    Y <- Z + e
    df <- data.frame(X, Z, Y)
  
    rf.basic = randomForest(Y ~ . , data = df, importance = T)
    mod = Predictor$new(rf.basic, data = df)
    pdp.obj = FeatureEffect$new(mod, feature = c("X"), method = "pdp") 
    p_pdp_mod <- plot(pdp.obj)
  
    simulation <- as.data.frame(pdp.obj$results[,1:2])
    simulation$run <- i
    colnames(simulation) <- c("X", "Y_hat", "run_number")
    pdp_simulations <- rbind.data.frame(pdp_simulations, simulation)
    }
  ggplot(data = pdp_simulations) + geom_line(data = pdp_simulations,aes(x = pdp_simulations[,1], y = pdp_simulations[,2], group = pdp_simulations[,3]), alpha = 0.5)
}
# standard deviation eq 0.1
scen2_plot_1 <- pdp_simulation(runs=20, N = 100, error_std=0.1)
scen2_plot_1 <- scen2_plot_1 + labs(x = "X", y = "Y_hat")
scen2_plot_2 <- pdp_simulation(runs=20, N = 1000, error_std=0.1)
scen2_plot_2 <- scen2_plot_2 + labs(x = "X", y = "Y_hat")
scen2_plot_3 <- pdp_simulation(runs=20, N = 10000, error_std=0.1)
scen2_plot_3 <- scen2_plot_3 + labs(x = "X", y = "Y_hat")
# standard deviation eq 0.3
scen2_plot_4 <- pdp_simulation(runs=20, N = 100, error_std=0.3)
scen2_plot_4 <- scen2_plot_4 + labs(x = "X", y = "Y_hat")
scen2_plot_5 <- pdp_simulation(runs=20, N = 1000, error_std=0.3)
scen2_plot_5 <- scen2_plot_5 + labs(x = "X", y = "Y_hat")
scen2_plot_6 <- pdp_simulation(runs=20, N = 10000, error_std=0.3)
scen2_plot_6 <- scen2_plot_6 + labs(x = "X", y = "Y_hat")
# standard deviation eq 0.5
scen2_plot_7 <- pdp_simulation(runs=20, N = 100, error_std=0.5)
scen2_plot_7 <- scen2_plot_7 + labs(x = "X", y = "Y_hat")
scen2_plot_8 <- pdp_simulation(runs=20, N = 1000, error_std=0.5)
scen2_plot_8 <- scen2_plot_8 + labs(x = "X", y = "Y_hat")
scen2_plot_9 <- pdp_simulation(runs=20, N = 10000, error_std=0.5)
scen2_plot_9 <- scen2_plot_9 + labs(x = "X", y = "Y_hat")

# intervention

intervention <- function(runs, N, error_std){ 
  df_intervention_all <- setNames(data.frame(matrix(ncol = 3, nrow = 0)), c("X", "Z", "Y"))
    for(i in 1:runs){
  
    e <- rnorm(N,0, error_std)
    X <- runif(N, -1, 1)
    Z <- X + e
    Y <- Z + e
    df <- data.frame(X, Z, Y)
      for(j in 1:length(unique(df$X))){
        X <- unique(df$X)[j]
        Z <- X + e
        Y <- Z + e
        df_intervention <- data.frame(X, Z, Y)
        df_intervention_all <- rbind.data.frame(df_intervention_all, df_intervention)
      }
    }

  df_intervention_group <- df_intervention_all %>%
    group_by(X) %>%
    summarise(Y_intervention = mean(Y),
    n = n()) %>%
    
    arrange(X)

  ggplot(aes(X, Y_intervention), data = df_intervention_group) + geom_line(colour="gold2", size = 1.5)
}
# standard deviation eq 0.1
scen2_int_plot_2 <- intervention(runs=1, N = 1000, error_std=0.1)
# standard deviation eq 0.3
scen2_int_plot_5 <- intervention(runs=1, N = 1000, error_std=0.3)
# standard deviation eq 0.5
scen2_int_plot_8 <- intervention(runs=1, N = 1000, error_std=0.5)


scenario_2_all <- ggarrange(scen2_plot_1, # N = 100,  std = 0.1
                            scen2_plot_2, # N = 1000, std = 0.1
                            scen2_plot_3, # N = 2000, std = 0.1
                            scen2_int_plot_2,
                            
                            scen2_plot_4, # N = 100,  std = 0.3
                            scen2_plot_5, # N = 1000, std = 0.3
                            scen2_plot_6, # N = 2000, std = 0.3
                            scen2_int_plot_5,
                            
                            scen2_plot_7, # N = 100,  std = 0.5
                            scen2_plot_8, # N = 1000, std = 0.5
                            scen2_plot_9, # N = 2000, std = 0.5
                            scen2_int_plot_8,
                            
                        nrow= 3, ncol = 4)
annotate_figure(scenario_2_all,
                top = text_grob("Increasing number of observations to the right", color = "black"),
                left = text_grob("Increasing standard deviation of error to the bottom", color = "black", rot = 90),
                 fig.lab.face = "bold")
```

**Scenario 3**

After two chains, we now have a scenario where $X$ has an influence on both $Z$ and $Y$ directly, as well as $Z$ having an impact on $Y$, as seen in Figure \@ref(fig:Figure7). X confounds $Z \rightarrow Y$ here. An example of a confounding variable in real life might be for instance the relationship between the *level of physical activity* and *weight gain*, which is confounded by *age*. *Age* affects both *weight gain* and the *level of physical activity* (on average), making it similar to the X in our scenario here. This scenario is similar to the previous one with only an edge between $X$ and $Y$ having been added. As can be seen from the DAG in \@ref(fig:Figure7), $Z$ is still a descendant of $X$. The expectation is that the PDPs and the true interventions will not be equal.

```{r Figure7, echo=FALSE, fig.height = 3, fig.width = 3.5, fig.cap="X is a confounding variable impacting both Z and Y"}
dagify(Y ~ X,
       Y ~ Z,
       Z ~ X
) %>% 
  ggdag() 
```

The similarity to the previous scenario can also be noted in the simulation settings, where the only difference is the addition of $X$ to the equation for $Y$.

$$ Y \leftarrow Z + X + \epsilon_Y  $$
$$ \epsilon_Y,\epsilon_Z ~ \sim \mathcal{N}(0, 0.1), \ \ \ \ \ \epsilon_X \sim \mathcal{U}(-1,1), \ \ \ \ \ X \leftarrow \epsilon_X, \ \ \ \ \ Z \leftarrow X + \epsilon_Z, \ \ \ \ \ N \leftarrow 100 $$

```{r  echo=FALSE, Figure8, out.width='100%', fig.align= 'left', fig.cap="Comparison for scenario 3 of PDPs under various settings with the (yellow) intervention curve on the right"}
knitr::include_graphics('images/scenario3_all.jpeg', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
```

The expectation was that the PDP would not show the same results as the true intervention. On first glance in Figure \@ref(fig:Figure8), the PDPs do not seem to be as inaccurate as they were in scenario 2. An overall upward trend seen in the true intervention on the right is also captured by the PDPs in all settings. However, big differences do exist. First of all, the scale of $\hat{Y}$ is off in every setting. This issue gets worse both when the standard deviation of the error increases and when the number of observations is increased. The worst setting is the bottom right, where $N = 10.000$ and the standard deviation of the error is 0.5. The range of $\hat{Y}$ is very small compared to the true intervention next to it and the slope is not steep enough. In fact, the correct slope can only be seen in a very few points: In the top row plot 2 and 3 around $X=0$ and on the second row plot 3, also around $X=0$.
Overall the result is not as poor as in scenario 2, but a causal interpretation of these plots would lead to a severe underestimation of the impact $X$ has on $Y$.

```{r eval=FALSE, include=FALSE}
# pdp
pdp_simulation <- function(runs, N, error_std){ 
  pdp_simulations <- setNames(data.frame(matrix(ncol = 2, nrow = 0)),  c("X1", "y_hat"))
  
  for(i in 1:runs){
    e <- rnorm(N,0, error_std)
    X <- runif(N, -1, 1)
    Z <- X + e
    Y <- X + Z + e
    df <- data.frame(X, Z, Y)
    
    rf.basic = randomForest(Y ~ . , data = df, importance = T)
    mod = Predictor$new(rf.basic, data = df)
    pdp.obj = FeatureEffect$new(mod, feature = c("X"), method = "pdp") 
    p_pdp_mod <- plot(pdp.obj)
    
    simulation <- as.data.frame(pdp.obj$results[,1:2])
    simulation$run <- i
    colnames(simulation) <- c("X", "Y_hat", "run_number")
    pdp_simulations <- rbind.data.frame(pdp_simulations, simulation)
  }
  ggplot(data = pdp_simulations) + geom_line(data = pdp_simulations,aes(x = pdp_simulations[,1], y = pdp_simulations[,2], group = pdp_simulations[,3]), alpha = 0.5)
}
# standard deviation eq 0.1
scen3_plot_1 <- pdp_simulation(runs=20, N = 100, error_std=0.1)
scen3_plot_1 <- scen3_plot_1 + labs(x = "X", y = "Y_hat")
scen3_plot_2 <- pdp_simulation(runs=20, N = 1000, error_std=0.1)
scen3_plot_2 <- scen3_plot_2 + labs(x = "X", y = "Y_hat")
scen3_plot_3 <- pdp_simulation(runs=20, N = 10000, error_std=0.1)
scen3_plot_3 <- scen3_plot_3 + labs(x = "X", y = "Y_hat")
# standard deviation eq 0.3
scen3_plot_4 <- pdp_simulation(runs=20, N = 100, error_std=0.3)
scen3_plot_4 <- scen3_plot_4 + labs(x = "X", y = "Y_hat")
scen3_plot_5 <- pdp_simulation(runs=20, N = 1000, error_std=0.3)
scen3_plot_5 <- scen3_plot_5 + labs(x = "X", y = "Y_hat")
scen3_plot_6 <- pdp_simulation(runs=20, N = 10000, error_std=0.3)
scen3_plot_6 <- scen3_plot_6 + labs(x = "X", y = "Y_hat")
# standard deviation eq 0.5
scen3_plot_7 <- pdp_simulation(runs=20, N = 100, error_std=0.5)
scen3_plot_7 <- scen3_plot_7 + labs(x = "X", y = "Y_hat")
scen3_plot_8 <- pdp_simulation(runs=20, N = 1000, error_std=0.5)
scen3_plot_8 <- scen3_plot_8 + labs(x = "X", y = "Y_hat")
scen3_plot_9 <- pdp_simulation(runs=20, N = 10000, error_std=0.5)
scen3_plot_9 <- scen3_plot_9 + labs(x = "X", y = "Y_hat")

# intervention

intervention <- function(runs, N, error_std){ 
  df_intervention_all <- setNames(data.frame(matrix(ncol = 3, nrow = 0)), c("X", "Z", "Y"))
  for(i in 1:runs){
    
    e <- rnorm(N,0, error_std)
    X <- runif(N, -1, 1)
    Z <- X + e
    Y <- X + Z + e
    df <- data.frame(X, Z, Y)
    for(j in 1:length(unique(df$X))){
      X <- unique(df$X)[j]
      Z <- X + e
      Y <- X + Z + e
      df_intervention <- data.frame(X, Z, Y)
      df_intervention_all <- rbind.data.frame(df_intervention_all, df_intervention)
    }
  }
  
  df_intervention_group <- df_intervention_all %>%
    group_by(X) %>%
    summarise(Y_intervention = mean(Y),
              n = n()) %>%
    
    arrange(X)
  
  ggplot(aes(X, Y_intervention), data = df_intervention_group) + geom_line(colour="gold2", size = 1.5)
}
# standard deviation eq 0.1
scen3_int_plot_2 <- intervention(runs=1, N = 1000, error_std=0.1)
# standard deviation eq 0.3
scen3_int_plot_5 <- intervention(runs=1, N = 1000, error_std=0.3)
# standard deviation eq 0.5
scen3_int_plot_8 <- intervention(runs=1, N = 1000, error_std=0.5)


scenario_3_all <- ggarrange(scen3_plot_1, # N = 100,  std = 0.1
                            scen3_plot_2, # N = 1000, std = 0.1
                            scen3_plot_3, # N = 2000, std = 0.1
                            scen3_int_plot_2,
                            
                            scen3_plot_4, # N = 100,  std = 0.3
                            scen3_plot_5, # N = 1000, std = 0.3
                            scen3_plot_6, # N = 2000, std = 0.3
                            scen3_int_plot_5,
                            
                            scen3_plot_7, # N = 100,  std = 0.5
                            scen3_plot_8, # N = 1000, std = 0.5
                            scen3_plot_9, # N = 2000, std = 0.5
                            scen3_int_plot_8,
                            
                            nrow= 3, ncol = 4)
annotate_figure(scenario_3_all,
                top = text_grob("Increasing number of observations to the right", color = "black"),
                left = text_grob("Increasing standard deviation of error to the bottom", color = "black", rot = 90),
                fig.lab.face = "bold")

```

**Scenario 4**

Scenario 4 consists of a direct effect of $X$ on $Y$. $Z$ meanwhile is unrelated to both $X$ and $Y$. It is however included in the simulation and included in the model that is run to create the PDPs. In a non-simulated setting, $Z$ can be seen as a variable that we assume might be related to $Y$ and therefore include, but in actuality has nothing to do with the causal process and should not be included. We will see now how the PDP deals with this kind of variable in the mix.

```{r  echo=FALSE, Figure9, out.width='100%', fig.align= 'left', fig.cap="X directly impacts Y. Z is included in our model, but has no relationship to X or Y"}
knitr::include_graphics('images/scenario4.jpeg', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
``` 

For the simulation the initial settings look as follows, again increasing both the standard deviation of the error and the magnitude of observations from this starting point. 
$$ Y \leftarrow X + \epsilon_Y  $$
$$ \epsilon_Y ~ \sim \mathcal{N}(0, 0.1), \ \ \ \ \ \epsilon_X, \epsilon_Z \sim \mathcal{U}(-1,1), \ \ \ \ \ X \leftarrow \epsilon_X,\ \ \ \ \ Z \leftarrow \epsilon_Z, \ \ \ \ \ N \leftarrow 100 $$
```{r  echo=FALSE, Figure10causal, out.width='100%', fig.align= 'left', fig.cap="Comparison for scenario 4 of PDPs under various settings with the (yellow) intervention curve on the right"}
knitr::include_graphics('images/scenario4_all.jpeg', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
``` 

The PDPs in Figure \@ref(fig:Figure10causal) are able to capture the intervention curve well. Only in the cases where $N=100$ is there slight curvature at the extreme ends of $X$. With this low number of observations the model is less accurate. In all other settings a consistent slope is present from $X = -1$ to $X = 1$, with the scale of $\hat{Y}$ matching that of $Y_{intervention}$.

```{r eval=FALSE, include=FALSE}
# pdp
pdp_simulation <- function(runs, N, error_std){ 
  pdp_simulations <- setNames(data.frame(matrix(ncol = 2, nrow = 0)),  c("X1", "y_hat"))
  
  for(i in 1:runs){
    e <- rnorm(N,0, error_std)
    X <- runif(N, -1, 1)
    Z <- runif(N, -1, 1)
    Y <- X + e
    df <- data.frame(X, Z, Y)
    
    rf.basic = randomForest(Y ~ . , data = df, importance = T)
    mod = Predictor$new(rf.basic, data = df)
    pdp.obj = FeatureEffect$new(mod, feature = c("X"), method = "pdp") 
    p_pdp_mod <- plot(pdp.obj)
    
    simulation <- as.data.frame(pdp.obj$results[,1:2])
    simulation$run <- i
    colnames(simulation) <- c("X", "Y_hat", "run_number")
    pdp_simulations <- rbind.data.frame(pdp_simulations, simulation)
  }
  ggplot(data = pdp_simulations) + geom_line(data = pdp_simulations,aes(x = pdp_simulations[,1], y = pdp_simulations[,2], group = pdp_simulations[,3]), alpha = 0.5)
}
# standard deviation eq 0.1
scen4_plot_1 <- pdp_simulation(runs=20, N = 100, error_std=0.1)
scen4_plot_1 <- scen4_plot_1 + labs(x = "X", y = "Y_hat")
scen4_plot_2 <- pdp_simulation(runs=20, N = 1000, error_std=0.1)
scen4_plot_2 <- scen4_plot_2 + labs(x = "X", y = "Y_hat")
scen4_plot_3 <- pdp_simulation(runs=20, N = 10000, error_std=0.1)
scen4_plot_3 <- scen4_plot_3 + labs(x = "X", y = "Y_hat")
# standard deviation eq 0.3
scen4_plot_4 <- pdp_simulation(runs=20, N = 100, error_std=0.3)
scen4_plot_4 <- scen4_plot_4 + labs(x = "X", y = "Y_hat")
scen4_plot_5 <- pdp_simulation(runs=20, N = 1000, error_std=0.3)
scen4_plot_5 <- scen4_plot_5 + labs(x = "X", y = "Y_hat")
scen4_plot_6 <- pdp_simulation(runs=20, N = 10000, error_std=0.3)
scen4_plot_6 <- scen4_plot_6 + labs(x = "X", y = "Y_hat")
# standard deviation eq 0.5
scen4_plot_7 <- pdp_simulation(runs=20, N = 100, error_std=0.5)
scen4_plot_7 <- scen4_plot_7 + labs(x = "X", y = "Y_hat")
scen4_plot_8 <- pdp_simulation(runs=20, N = 1000, error_std=0.5)
scen4_plot_8 <- scen4_plot_8 + labs(x = "X", y = "Y_hat")
scen4_plot_9 <- pdp_simulation(runs=20, N = 10000, error_std=0.5)
scen4_plot_9 <- scen4_plot_9 + labs(x = "X", y = "Y_hat")

# intervention

intervention <- function(runs, N, error_std){ 
  df_intervention_all <- setNames(data.frame(matrix(ncol = 3, nrow = 0)), c("X", "Z", "Y"))
  for(i in 1:runs){
    
    e <- rnorm(N,0, error_std)
    X <- runif(N, -1, 1)
    Z <- runif(N, -1, 1)
    Y <- X + e
    df <- data.frame(X, Z, Y)
    for(j in 1:length(unique(df$X))){
      X <- unique(df$X)[j]
      Y <- X + e
      df_intervention <- data.frame(X, Z, Y)
      df_intervention_all <- rbind.data.frame(df_intervention_all, df_intervention)
    }
  }
  
  df_intervention_group <- df_intervention_all %>%
    group_by(X) %>%
    summarise(Y_intervention = mean(Y),
              n = n()) %>%
    
    arrange(X)
  
  ggplot(aes(X, Y_intervention), data = df_intervention_group) + geom_line(colour="gold2", size = 1.5)
}
# standard deviation eq 0.1
scen4_int_plot_2 <- intervention(runs=1, N = 1000, error_std=0.1)
# standard deviation eq 0.3
scen4_int_plot_5 <- intervention(runs=1, N = 1000, error_std=0.3)
# standard deviation eq 0.5
scen4_int_plot_8 <- intervention(runs=1, N = 1000, error_std=0.5)


scenario_4_all <- ggarrange(scen4_plot_1, # N = 100,  std = 0.1
                            scen4_plot_2, # N = 1000, std = 0.1
                            scen4_plot_3, # N = 2000, std = 0.1
                            scen4_int_plot_2,
                            
                            scen4_plot_4, # N = 100,  std = 0.3
                            scen4_plot_5, # N = 1000, std = 0.3
                            scen4_plot_6, # N = 2000, std = 0.3
                            scen4_int_plot_5,
                            
                            scen4_plot_7, # N = 100,  std = 0.5
                            scen4_plot_8, # N = 1000, std = 0.5
                            scen4_plot_9, # N = 2000, std = 0.5
                            scen4_int_plot_8,
                            
                            nrow= 3, ncol = 4)
annotate_figure(scenario_4_all,
                top = text_grob("Increasing number of observations to the right", color = "black"),
                left = text_grob("Increasing standard deviation of error to the bottom", color = "black", rot = 90),
                fig.lab.face = "bold")

```

**Scenario 5**

In this scenario Z is a confounding variable. This is similar to scenario 3 where $X$ was the confounder. Thinking back to the example of *age* being a confounding variable for *level of physical activity* and *weight gain*, in the previous example our $X$ was comparable to the confounder *age*. In this scenario, we can keep the same example, but say our variable $X$ is now comparable to the variable *level of physical activity* that is being confounded. Since $X$ has no descendants and there is no backdoor path, the expectation here is that the PDPs will be similar to the intervention curve. 

```{r Figure11causal, echo=FALSE, fig.height = 3, fig.width = 3.5, fig.cap="Z is a confounding variable impacting both X and Y"}
dagify(Y ~ X,
       Y ~ Z,
       X ~ Z
) %>% 
  ggdag() 
```

The following simulation settings were used:

$$ Y \leftarrow Z + X + \epsilon_Y  $$
$$ \epsilon_Y,\epsilon_X ~ \sim \mathcal{N}(0, 0.1), \ \ \ \ \ \epsilon_Z \sim \mathcal{U}(-1,1), \ \ \ \ \ Z \leftarrow \epsilon_Z, \ \ \ \ \ X \leftarrow Z + \epsilon_X, \ \ \ \ \ N \leftarrow 100 $$

```{r  echo=FALSE, Figure12causal, out.width='100%', fig.align= 'left', fig.cap="Comparison for scenario 5 of PDPs under various settings with the (yellow) intervention curve on the right"}
knitr::include_graphics('images/scenario5_all.jpeg', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
``` 

We can see in Figure \@ref(fig:Figure12causal) that aside from the extreme regions of $X$ where the slope is flatter, the PDPs are fairly similar to the Intervention Curves on the right. We can see that as the standard deviations of the errors increase, so does the range of $Y_intervention$ from (-1, 1) to (-2,2). This same trend can be observed in the PDPs. As could be expected, the PDPs are most accurate with the highest number of observations and lowest standard deviation of errors.

```{r eval=FALSE, include=FALSE}
# pdp
pdp_simulation <- function(runs, N, error_std){ 
  pdp_simulations <- setNames(data.frame(matrix(ncol = 3, nrow = 0)),  c("X", "Y_hat", "run_number"))
  all_dfs <<- setNames(data.frame(matrix(ncol = 3, nrow = 0)),  c("X", "Z", "Y"))
  for(i in 1:runs){
    e <- rnorm(N,0, error_std)
    Z <- runif(N, -1, 1)
    X <- Z + e
    Y <- X + Z + e
    df <- data.frame(X, Z, Y)
    all_dfs <<- rbind.data.frame(all_dfs, df)
    rf.basic <<- randomForest(Y ~ . , data = df, importance = T)
    mod = Predictor$new(rf.basic, data = df)
    pdp.obj = FeatureEffect$new(mod, feature = c("X"), method = "pdp") 
    p_pdp_mod <- plot(pdp.obj)
    
    simulation <- as.data.frame(pdp.obj$results[,1:2])
    simulation$run <- i
    colnames(simulation) <- c("X", "Y_hat", "run_number")
    pdp_simulations <- rbind.data.frame(pdp_simulations, simulation)
  }
  ggplot(data = pdp_simulations) + geom_line(data = pdp_simulations,aes(x = pdp_simulations[,1], y = pdp_simulations[,2], group = pdp_simulations[,3]), alpha = 0.5)
}
# standard deviation eq 0.1
scen5_plot_1 <- pdp_simulation(runs=20, N = 100, error_std=0.1)
scen5_plot_1 <- scen5_plot_1 + labs(x = "X", y = "Y_hat")
scen5_plot_2 <- pdp_simulation(runs=20, N = 1000, error_std=0.1)
scen5_plot_2 <- scen5_plot_2 + labs(x = "X", y = "Y_hat")
scen5_plot_3 <- pdp_simulation(runs=20, N = 10000, error_std=0.1)
scen5_plot_3 <- scen5_plot_3 + labs(x = "X", y = "Y_hat")
# standard deviation eq 0.3
scen5_plot_4 <- pdp_simulation(runs=20, N = 100, error_std=0.3)
scen5_plot_4 <- scen5_plot_4 + labs(x = "X", y = "Y_hat")
scen5_plot_5 <- pdp_simulation(runs=20, N = 1000, error_std=0.3)
scen5_plot_5 <- scen5_plot_5 + labs(x = "X", y = "Y_hat")
scen5_plot_6 <- pdp_simulation(runs=20, N = 10000, error_std=0.3)
scen5_plot_6 <- scen5_plot_6 + labs(x = "X", y = "Y_hat")
# standard deviation eq 0.5
scen5_plot_7 <- pdp_simulation(runs=20, N = 100, error_std=0.5)
scen5_plot_7 <- scen5_plot_7 + labs(x = "X", y = "Y_hat")
scen5_plot_8 <- pdp_simulation(runs=20, N = 1000, error_std=0.5)
scen5_plot_8 <- scen5_plot_8 + labs(x = "X", y = "Y_hat")
scen5_plot_9 <- pdp_simulation(runs=20, N = 10000, error_std=0.5)
scen5_plot_9 <- scen5_plot_9 + labs(x = "X", y = "Y_hat")

# intervention

intervention <- function(runs, N, error_std){ 
  df_intervention_all <- setNames(data.frame(matrix(ncol = 3, nrow = 0)), c("X", "Z", "Y"))
  for(i in 1:runs){
    
    e <- rnorm(N,0, error_std)
    Z <- runif(N, -1, 1)
    X <- Z + e
    Y <- X + Z + e
    df <- data.frame(X, Z, Y)
    for(j in 1:length(unique(df$X))){
      X <- unique(df$X)[j]
      Y <- X + Z + e
      df_intervention <- data.frame(X, Z, Y)
      df_intervention_all <- rbind.data.frame(df_intervention_all, df_intervention)
    }
  }
  
  df_intervention_group <- df_intervention_all %>%
    group_by(X) %>%
    summarise(Y_intervention = mean(Y),
              n = n()) %>%
    
    arrange(X)
  
  ggplot(aes(X, Y_intervention), data = df_intervention_group) + geom_line(colour="gold2", size = 1.5)
}
# standard deviation eq 0.1
scen5_int_plot_2 <- intervention(runs=1, N = 1000, error_std=0.1)
# standard deviation eq 0.3
scen5_int_plot_5 <- intervention(runs=1, N = 1000, error_std=0.3)
# standard deviation eq 0.5
scen5_int_plot_8 <- intervention(runs=1, N = 1000, error_std=0.5)


scenario_5_all <- ggarrange(scen5_plot_1+ coord_cartesian(ylim=(-1:1)), # N = 100,  std = 0.1
                            scen5_plot_2+ coord_cartesian(ylim=(-1:1)), # N = 1000, std = 0.1
                            scen5_plot_3+ coord_cartesian(ylim=(-1:1)), # N = 2000, std = 0.1
                            scen5_int_plot_2+ coord_cartesian(ylim=(-1:1)),
                            
                            scen5_plot_4+ coord_cartesian(ylim=(-2:2)), # N = 100,  std = 0.3
                            scen5_plot_5+ coord_cartesian(ylim=(-2:2)), # N = 1000, std = 0.3
                            scen5_plot_6+ coord_cartesian(ylim=(-2:2)), # N = 2000, std = 0.3
                            scen5_int_plot_5+ coord_cartesian(ylim=(-2:2)),
                            
                            scen5_plot_7+ coord_cartesian(ylim=(-3:3)), # N = 100,  std = 0.5
                            scen5_plot_8 + coord_cartesian(ylim=(-3:3)), # N = 1000, std = 0.5
                            scen5_plot_9+ coord_cartesian(ylim=(-3:3)), # N = 2000, std = 0.5
                            scen5_int_plot_8+ coord_cartesian(ylim=(-3:3)),
                            
                            nrow= 3, ncol = 4)
annotate_figure(scenario_5_all,
                top = text_grob("Increasing number of observations to the right", color = "black"),
                left = text_grob("Increasing standard deviation of error to the bottom", color = "black", rot = 90),
                fig.lab.face = "bold")
```

## Theoretical Comparison

The empirical comparison of the modeled PDP and the intervention curve is of course not ideal. The PDP is dependent on the model fit, meaning a badly fitted model could give unexpected results. Ideally, we want to see what a PDP would look like given a perfect model. One way to approximate this is the following:  
  
Taking the PDP formula:

$$ \hat{f}(x_S) = \frac{1}{n} \sum_{i=1}^n \hat{f} (x_S, x_{C}^{(i)})$$

We can replace $\hat{f} (x_S, x_{C}^{(i)})$ with the conditional expectation $E[Y|X_s = x_s, X_c = x_c]$. This is the only change we make.

For a scenario where $X$ is a confounding variable, as seen in Figure \@ref(fig:Figure7) with scenario 3, we have:  
$$  X \leftarrow  \epsilon_X , \ \ \ \ \ Z \leftarrow X + \epsilon_Z, \ \ \ \ \ Y \leftarrow X + Z + \epsilon_Y, \ \ \ \ \ \epsilon_X \sim \mathcal{N}(0, \sigma^2), \ \ \ \ \ \epsilon_Z \sim \mathcal{N}(0, \sigma^2), \ \ \ \ \ \epsilon_Y \sim \mathcal{N}(0, \sigma^2) $$

Then   

$\begin{aligned} E[Y|X = x, Z = z] = E[X+Z + \epsilon_Y| X = x, Z = z]\\
 =E[x+z+\epsilon_Y] = x + z + E[\epsilon_y]\\
 = x + z
\end{aligned}$

Similarly, for a scenario where $Z$ is a confounding variable, as seen in Figure \@ref(fig:Figure11causal) with scenario 5, the result is:  
$$  Z \leftarrow  \epsilon_z , \ \ \ \ \ X \leftarrow Z + \epsilon_X, \ \ \ \ \ Y \leftarrow X + Z + \epsilon_Y, \ \ \ \ \ \epsilon_X \sim \mathcal{N}(0, \sigma^2), \ \ \ \ \ \epsilon_Z \sim \mathcal{N}(0, \sigma^2), \ \ \ \ \ \epsilon_Y \sim \mathcal{N}(0, \sigma^2) $$

$\begin{aligned} E[Y|X = x, Z = z] = E[X+Z + \epsilon_Y| X = x, Z = z]\\
= x + z
\end{aligned}$


And for a scenario where we have a chain $X \rightarrow Z \rightarrow Y$, as seen in Figure \@ref(fig:Figure5) with scenario 2, this means the following:  

$$  X \leftarrow  \epsilon_X , \ \ \ \ \ Z \leftarrow X + \epsilon_Z, \ \ \ \ \ Y \leftarrow Z + \epsilon_Y, \ \ \ \ \ \epsilon_X \sim \mathcal{N}(0, \sigma^2), \ \ \ \ \ \epsilon_Z \sim \mathcal{N}(0, \sigma^2), \ \ \ \ \ \epsilon_Y \sim \mathcal{N}(0, \sigma^2) $$

$\begin{aligned} E[Y|X = x, Z = z] = E[Z + \epsilon_Y| X = x, Z = z]\\
= z
\end{aligned}$

When implemented, however, we did not get the the expected results. In fact, while the results of the modeled PDP versus the intervention curve seen earlier seemed to give expected results, the theoretical PDP implementation seemed to give the opposite of the expected result. The theoretical PDP was not similar to the intervention curve when the backdoor criterion was met, nor was it dissimilar when the backdoor criterion was not met. 
I assume a mistake in implementation is at fault. In the end, I was unable to figure out where the mistake lies: In the modeling of the PDP, the calculation of the intervention curve, the derivation of the theoretical PDP or the implementation of the theoretical PDP. If no mistake was made, I was unable to come up with a satisfactory explanation of the results. An example of the unexpected results is seen in \@ref(fig:Figure13causal) . 

```{r  echo=FALSE, Figure13causal, out.width='100%', fig.align= 'left', fig.cap="Model PDP versus Theoretical PDP vs Intervention for the scenario where X is a confounder (Scenario 3)"}
knitr::include_graphics('images/theoretical_pdp.jpeg', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
``` 

The expectation for this scenario, as we have seen before when discussing Figure \@ref(fig:Figure8), is that the PDP and the Intervention Curve should not be the same. Now again, as before, we see that this is true for the modeled PDP on the left and the IC on the right. However, the theoretical PDP in the middle is the same as the IC, which expectation is it should not be.

Figure \@ref(fig:Figure14causal) shows the result obtained for the scenario where Z is a confounder. This was our Scenario 5, discussed in Figure \@ref(fig:Figure12causal). We expect the PDP to be the same as the Intervention Curve here, because the backdoor criterion is met. Comparing the modeled PDP on the left to the IC on the right, this seems to be the case. However, as in the previous case, $E[Y]$ ranges from (-2, 2), making it different from the IC and the modeled PDP.


```{r  echo=FALSE, Figure14causal, out.width='100%', fig.align= 'left', fig.cap="Model PDP versus Theoretical PDP vs Intervention for the scenario where Z is a confounder (Scenario 5)"}
knitr::include_graphics('images/theoretical_pdp2.jpeg', auto_pdf = getOption("knitr.graphics.auto_pdf", TRUE))
``` 

## Conclusion

Causal interpretability of a PDP is dependent on several things:
(1) The backdoor criterion being met. We have seen that if the backdoor criterion is met, meaning no variable in the complement set $C$ is a descendant of our variable of interest, the PDP should be the same as the intervention curve. We say should, because in practice it will also depend on:
(2) The model fit. Even when the backdoor criterion is met, the PDP might not fully capture the exact same relationship as the intervention curve. Especially in extreme regions, where data is potentially sparse, the PDP can be deceptive. Same in scenarios with a higher(er) error and low number of observations.

Point (2) can be checked through the standard goodness of fit measures that are pervasive in the statistics literature. Point (1) is more difficult to verify, especially based on only the data. Here a certain amount of domain knowledge is needed to ensure the assumption is met. Still, it is a hard assumption to verify which could limit the confidence people have in a causal interpretation of a PDP.

Additionally, a further limitation is that directionality is not something that can be deduced from a PDP. As we saw with the ice-cream example, sometimes this problem is trivial. In many cases, however, it is not and sufficient domain knowledge is necessary to ensure the correct process is depicted.
